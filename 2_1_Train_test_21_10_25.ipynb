{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "633f7574",
   "metadata": {},
   "source": [
    "# Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94557bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b2bc8d",
   "metadata": {},
   "source": [
    "# Importation des donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8d3111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les donnÃ©es de test\n",
    "df_stationary_test = pd.read_csv(\"df_stationary_test.csv\", index_col=\"date\")\n",
    "df_stationary_test.index = pd.to_datetime(df_stationary_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3a1b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stationary = pd.read_csv(\"df_stationary.csv\", index_col=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd1a4701",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stationary_unrate = df_stationary[\"UNRATE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca1fa0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "045bba6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SÃ©rie prÃªte : 1960-01-01 â†’ 2025-08-01 | n=788 | freq=MS\n"
     ]
    }
   ],
   "source": [
    "# ---------- PrÃ©paration de la sÃ©rie ----------\n",
    "# Si tu as dÃ©jÃ  :\n",
    "# df_stationary_unrate = df_stationary[\"UNRATE\"]\n",
    "\n",
    "y = df_stationary_unrate.copy()\n",
    "\n",
    "# VÃ©rifie que lâ€™index est bien une date (sinon essaie de le convertir)\n",
    "if not isinstance(y.index, (pd.DatetimeIndex, pd.PeriodIndex)):\n",
    "    y.index = pd.to_datetime(y.index, errors=\"coerce\")\n",
    "\n",
    "# Force la frÃ©quence mensuelle (dÃ©but de mois)\n",
    "y.index = y.index.to_period(\"M\").to_timestamp(how=\"start\")\n",
    "y = y.sort_index().asfreq(\"MS\").astype(float).dropna()\n",
    "\n",
    "print(f\"âœ… SÃ©rie prÃªte : {y.index.min().date()} â†’ {y.index.max().date()} | n={len(y)} | freq={y.index.freqstr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b337cf8",
   "metadata": {},
   "source": [
    "# Test AR(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "308add8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] 1983-01-01 â†’ p* = 5\n",
      "[CV] 1986-01-01 â†’ p* = 4\n",
      "[CV] 1989-01-01 â†’ p* = 4\n",
      "[CV] 1992-01-01 â†’ p* = 4\n",
      "[CV] 1995-01-01 â†’ p* = 4\n",
      "[CV] 1998-01-01 â†’ p* = 4\n",
      "[CV] 2001-01-01 â†’ p* = 4\n",
      "[CV] 2004-01-01 â†’ p* = 4\n",
      "[CV] 2007-01-01 â†’ p* = 4\n",
      "[CV] 2010-01-01 â†’ p* = 4\n",
      "[CV] 2013-01-01 â†’ p* = 4\n",
      "[CV] 2016-01-01 â†’ p* = 4\n",
      "[CV] 2019-01-01 â†’ p* = 4\n",
      "[CV] 2022-01-01 â†’ p* = 4\n",
      "\n",
      "âœ… Pseudo-OOS terminÃ© â€” n prÃ©visions = 741\n",
      "               y_hat  y_true  p_used\n",
      "date                                \n",
      "1963-12-01 -0.080890     0.0       1\n",
      "1964-01-01  0.141077    -0.1       1\n",
      "1964-02-01  0.408114    -0.5       1\n",
      "\n",
      "ðŸ“Š Validation 83â€“89 â€” n=84 | MAE=0.891 | RMSE=1.248 | RÂ²=-0.995\n",
      "ðŸ“Š Test 90â€“2025 â€” n=428 | MAE=0.871 | RMSE=1.873 | RÂ²=-0.508\n",
      "ðŸ’¾ ModÃ¨le AR(p) sauvegardÃ© â†’ ARP_last_trained_model.pkl\n",
      "ðŸ’¾ Bundle AR(p) OOS sauvegardÃ© â†’ ARP_h12_oos_bundle.pkl\n",
      "ðŸ’¾ MÃ©ta AR(p) sauvegardÃ©e â†’ ARP_last_trained_model_meta.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# AR(p) â€” Pseudo-OOS continu (h=12) avec re-CV triennale\n",
    "# rÃ©utilise la Series `y` dÃ©jÃ  prÃªte (freq=MS)\n",
    "# ==========================================\n",
    "import os, pickle, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# ---------- ParamÃ¨tres ----------\n",
    "h = 12\n",
    "min_train_n = 36                 # â‰¥ 3 ans de donnÃ©es pour dÃ©marrer\n",
    "trend = \"c\"                      # \"c\" = constante ; \"n\" = sans constante\n",
    "p_grid = range(1, 13)            # p âˆˆ {1,â€¦,12}\n",
    "cv_update_every_months = 36\n",
    "cv_anchor = pd.Timestamp(\"1983-01-01\")\n",
    "\n",
    "# ---------- Utilitaires ----------\n",
    "def months_since(anchor, t):\n",
    "    return (t.year - anchor.year) * 12 + (t.month - anchor.month)\n",
    "\n",
    "def rolling_mae_for_p(y_series, p, h, min_train):\n",
    "    \"\"\"MAE rolling Ã  l'horizon h pour un p donnÃ© (sur y_series, en respectant l'ordre temporel).\"\"\"\n",
    "    rows = []\n",
    "    last_t_end = y_series.index.max() - relativedelta(months=h)\n",
    "    for t_end in y_series.index:\n",
    "        if t_end > last_t_end:\n",
    "            break\n",
    "        y_tr = y_series.loc[:t_end]\n",
    "        if len(y_tr) < max(min_train, p + 1):\n",
    "            continue\n",
    "        model = AutoReg(y_tr, lags=p, old_names=False, trend=trend).fit()\n",
    "        fc = model.predict(start=len(y_tr), end=len(y_tr) + h - 1)\n",
    "        yhat_h = float(fc.iloc[-1])\n",
    "        t_fore = t_end + relativedelta(months=h)\n",
    "        if t_fore in y_series.index:\n",
    "            rows.append((t_fore, yhat_h, float(y_series.loc[t_fore])))\n",
    "    if not rows:\n",
    "        return np.inf\n",
    "    tmp = pd.DataFrame(rows, columns=[\"date\", \"y_hat\", \"y_true\"]).set_index(\"date\")\n",
    "    return float(mean_absolute_error(tmp[\"y_true\"], tmp[\"y_hat\"]))\n",
    "\n",
    "def select_p_by_cv(y_tr, p_grid, h, min_train):\n",
    "    \"\"\"SÃ©lectionne p* minimisant le MAE(h) rolling calculÃ© sur l'Ã©chantillon d'entraÃ®nement courant.\"\"\"\n",
    "    best_p, best_score = None, np.inf\n",
    "    for p in p_grid:\n",
    "        score = rolling_mae_for_p(y_tr, p, h, min_train)\n",
    "        if score < best_score:\n",
    "            best_score, best_p = score, p\n",
    "    return int(best_p if best_p is not None else 1)\n",
    "\n",
    "# ---------- Boucle pseudo-OOS continue ----------\n",
    "rows = []\n",
    "last_model = None\n",
    "last_fit_end = None\n",
    "current_p = None\n",
    "\n",
    "# s'assurer que y est bien MS\n",
    "y = pd.Series(y.astype(float).values, index=pd.to_datetime(y.index)).asfreq(\"MS\").dropna()\n",
    "\n",
    "last_t_end = y.index.max() - relativedelta(months=h)\n",
    "\n",
    "for t_end in y.index:\n",
    "    if t_end > last_t_end:\n",
    "        break\n",
    "\n",
    "    y_tr = y.loc[:t_end]\n",
    "    if len(y_tr) < min_train_n:\n",
    "        continue\n",
    "\n",
    "    # re-CV Ã  partir de 1983-01 tous les 36 mois\n",
    "    if t_end >= cv_anchor:\n",
    "        m = months_since(cv_anchor, t_end)\n",
    "        need_cv = (m % cv_update_every_months == 0)\n",
    "    else:\n",
    "        need_cv = False\n",
    "\n",
    "    if current_p is None and not need_cv:\n",
    "        current_p = 1  # p par dÃ©faut avant la premiÃ¨re re-CV\n",
    "\n",
    "    if need_cv:\n",
    "        current_p = select_p_by_cv(y_tr, p_grid, h, min_train_n)\n",
    "        print(f\"[CV] {t_end.date()} â†’ p* = {current_p}\")\n",
    "\n",
    "    # fit AR(p) avec p courant\n",
    "    arp = AutoReg(y_tr, lags=current_p, old_names=False, trend=trend).fit()\n",
    "    last_model = arp\n",
    "    last_fit_end = t_end\n",
    "\n",
    "    # prÃ©vision Ã  h mois (valeur Ã  l'horizon)\n",
    "    fc = arp.predict(start=len(y_tr), end=len(y_tr) + h - 1)\n",
    "    yhat_h = float(fc.iloc[-1])\n",
    "\n",
    "    t_fore = t_end + relativedelta(months=h)\n",
    "    if t_fore in y.index:\n",
    "        rows.append((t_fore, yhat_h, float(y.loc[t_fore]), int(current_p)))\n",
    "\n",
    "# ---------- DataFrame unique ----------\n",
    "if rows:\n",
    "    df_oos_arp = (\n",
    "        pd.DataFrame(rows, columns=[\"date\", \"y_hat\", \"y_true\", \"p_used\"])\n",
    "          .set_index(\"date\").sort_index()\n",
    "    )\n",
    "else:\n",
    "    df_oos_arp = pd.DataFrame(columns=[\"y_hat\", \"y_true\", \"p_used\"])\n",
    "    df_oos_arp.index = pd.to_datetime(pd.Index([]))\n",
    "\n",
    "print(f\"\\nâœ… Pseudo-OOS terminÃ© â€” n prÃ©visions = {len(df_oos_arp)}\")\n",
    "print(df_oos_arp.head(3))\n",
    "\n",
    "# ---------- (facultatif) Scores par pÃ©riode ----------\n",
    "if len(df_oos_arp):\n",
    "    df_val  = df_oos_arp.loc[\"1983-01-01\":\"1989-12-31\"].copy()\n",
    "    df_test = df_oos_arp.loc[\"1990-01-01\":\"2025-08-31\"].copy()\n",
    "\n",
    "    if len(df_val):\n",
    "        mae  = mean_absolute_error(df_val[\"y_true\"], df_val[\"y_hat\"])\n",
    "        rmse = np.sqrt(mean_squared_error(df_val[\"y_true\"], df_val[\"y_hat\"]))\n",
    "        r2   = r2_score(df_val[\"y_true\"], df_val[\"y_hat\"]) if len(df_val) > 1 else np.nan\n",
    "        print(f\"\\nðŸ“Š Validation 83â€“89 â€” n={len(df_val)} | MAE={mae:.3f} | RMSE={rmse:.3f} | RÂ²={r2:.3f}\")\n",
    "\n",
    "    if len(df_test):\n",
    "        mae  = mean_absolute_error(df_test[\"y_true\"], df_test[\"y_hat\"])\n",
    "        rmse = np.sqrt(mean_squared_error(df_test[\"y_true\"], df_test[\"y_hat\"]))\n",
    "        r2   = r2_score(df_test[\"y_true\"], df_test[\"y_hat\"]) if len(df_test) > 1 else np.nan\n",
    "        print(f\"ðŸ“Š Test 90â€“2025 â€” n={len(df_test)} | MAE={mae:.3f} | RMSE={rmse:.3f} | RÂ²={r2:.3f}\")\n",
    "\n",
    "# ---------- Sauvegardes ----------\n",
    "ARP_LAST_PKL  = \"ARP_last_trained_model.pkl\"\n",
    "ARP_LAST_META = \"ARP_last_trained_model_meta.csv\"\n",
    "ARP_BUNDLE    = \"ARP_h12_oos_bundle.pkl\"\n",
    "\n",
    "# 1) modÃ¨le final\n",
    "if last_model is not None:\n",
    "    try:\n",
    "        joblib.dump(last_model, ARP_LAST_PKL)\n",
    "        print(f\"ðŸ’¾ ModÃ¨le AR(p) sauvegardÃ© â†’ {ARP_LAST_PKL}\")\n",
    "    except Exception:\n",
    "        with open(ARP_LAST_PKL, \"wb\") as f:\n",
    "            pickle.dump(last_model, f)\n",
    "        print(f\"ðŸ’¾ ModÃ¨le AR(p) sauvegardÃ© (pickle) â†’ {ARP_LAST_PKL}\")\n",
    "\n",
    "# 2) bundle des sorties  âœ… correctif: to_timestamp(how=\"start\")\n",
    "bundle = {\n",
    "    \"oos_predictions\": (\n",
    "        df_oos_arp.reset_index()\n",
    "                  .rename(columns={\"y_hat\": \"y_pred\"})\n",
    "                  .assign(\n",
    "                      date=lambda d: pd.to_datetime(d[\"date\"])\n",
    "                                      .dt.to_period(\"M\")\n",
    "                                      .dt.to_timestamp(how=\"start\")\n",
    "                  )\n",
    "    ),\n",
    "    \"params\": {\n",
    "        \"model\": \"AR(p)\",\n",
    "        \"trend\": trend,\n",
    "        \"horizon\": h,\n",
    "        \"cv_update_every_months\": cv_update_every_months,\n",
    "        \"p_grid\": list(p_grid),\n",
    "        \"min_train_n\": min_train_n,\n",
    "        \"anchor\": str(cv_anchor.date())\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"trained_until\": str(last_fit_end.date()) if last_fit_end is not None else None,\n",
    "        \"p_used_last\": int(current_p if current_p is not None else 1),\n",
    "        \"index_freq\": \"MS\",\n",
    "        \"n_obs_y\": int(len(y)),\n",
    "        \"n_forecasts\": int(len(df_oos_arp))\n",
    "    }\n",
    "}\n",
    "with open(ARP_BUNDLE, \"wb\") as f:\n",
    "    pickle.dump(bundle, f)\n",
    "print(f\"ðŸ’¾ Bundle AR(p) OOS sauvegardÃ© â†’ {ARP_BUNDLE}\")\n",
    "\n",
    "# 3) mÃ©ta csv\n",
    "meta_row = {\n",
    "    \"model\": \"AR(p)\",\n",
    "    \"trend\": trend,\n",
    "    \"cv_every_months\": cv_update_every_months,\n",
    "    \"anchor\": cv_anchor.strftime(\"%Y-%m-%d\"),\n",
    "    \"p_used_last\": int(current_p if current_p is not None else 1),\n",
    "    \"trained_until\": str(last_fit_end.date()) if last_fit_end is not None else None,\n",
    "    \"n_obs_y\": int(len(y)),\n",
    "    \"n_forecasts\": int(len(df_oos_arp))\n",
    "}\n",
    "pd.DataFrame([meta_row]).to_csv(ARP_LAST_META)\n",
    "print(f\"ðŸ’¾ MÃ©ta AR(p) sauvegardÃ©e â†’ {ARP_LAST_META}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca0f1b9",
   "metadata": {},
   "source": [
    "# AR(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e50ec315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: 1960-01-01 â†’ 2025-08-01  (n=788) | freq=MS\n",
      "\n",
      "âœ… Pseudo-OOS terminÃ© â€” n prÃ©visions = 741\n",
      "               y_hat  y_true\n",
      "date                        \n",
      "1963-12-01 -0.080890     0.0\n",
      "1964-01-01  0.141077    -0.1\n",
      "1964-02-01  0.408114    -0.5\n",
      "\n",
      "ðŸ“Š Validation 83â€“89 â€” n=84 | MAE=0.970 | RMSE=1.453 | RÂ²=-1.703\n",
      "ðŸ“Š Test 90â€“2025 â€” n=428 | MAE=0.887 | RMSE=1.866 | RÂ²=-0.496\n",
      "ðŸ’¾ ModÃ¨le AR(1) sauvegardÃ© â†’ AR1_last_trained_model.pkl\n",
      "ðŸ’¾ Bundle AR(1) OOS sauvegardÃ© â†’ AR1_h12_oos_bundle.pkl\n",
      "ðŸ’¾ MÃ©ta AR(1) sauvegardÃ©e â†’ AR1_last_trained_model_meta.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# AR(1) â€” Pseudo-OOS continu (h=12), p=1 fixe\n",
    "# rÃ©utilise la Series `y` dÃ©jÃ  prÃªte (freq=MS)\n",
    "# ==========================================\n",
    "import os, pickle, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# ---------- ParamÃ¨tres ----------\n",
    "h = 12\n",
    "min_train_n = 36          # â‰¥ 3 ans de donnÃ©es pour dÃ©marrer\n",
    "trend = \"c\"               # \"c\" = constante ; \"n\" = sans constante\n",
    "p_fixed = 1               # <-- p = 1, constant sur tout l'horizon\n",
    "\n",
    "# ---------- SÃ©curisation de la sÃ©rie y ----------\n",
    "# (S'assure qu'on est bien en MS, float et sans trous fatals)\n",
    "y = pd.Series(y.astype(float).values, index=pd.to_datetime(y.index)).asfreq(\"MS\").dropna()\n",
    "print(f\"y: {y.index.min().date()} â†’ {y.index.max().date()}  (n={len(y)}) | freq={y.index.freqstr}\")\n",
    "\n",
    "# ---------- Boucle pseudo-OOS continue ----------\n",
    "rows = []\n",
    "last_model = None\n",
    "last_fit_end = None\n",
    "\n",
    "last_t_end = y.index.max() - relativedelta(months=h)\n",
    "\n",
    "for t_end in y.index:\n",
    "    if t_end > last_t_end:\n",
    "        break\n",
    "\n",
    "    y_tr = y.loc[:t_end]\n",
    "    if len(y_tr) < max(min_train_n, p_fixed + 1):\n",
    "        continue\n",
    "\n",
    "    # fit AR(1) (p fixe) sur les donnÃ©es disponibles jusqu'Ã  t_end\n",
    "    ar1 = AutoReg(y_tr, lags=p_fixed, old_names=False, trend=trend).fit()\n",
    "    last_model = ar1\n",
    "    last_fit_end = t_end\n",
    "\n",
    "    # prÃ©vision Ã  h mois (prendre la valeur Ã  l'horizon)\n",
    "    fc = ar1.predict(start=len(y_tr), end=len(y_tr) + h - 1)\n",
    "    yhat_h = float(fc.iloc[-1])\n",
    "\n",
    "    t_fore = t_end + relativedelta(months=h)\n",
    "    if t_fore in y.index:\n",
    "        rows.append((t_fore, yhat_h, float(y.loc[t_fore])))\n",
    "\n",
    "# ---------- DataFrame OOS ----------\n",
    "if rows:\n",
    "    df_oos_ar1 = (\n",
    "        pd.DataFrame(rows, columns=[\"date\", \"y_hat\", \"y_true\"])\n",
    "          .set_index(\"date\").sort_index()\n",
    "    )\n",
    "else:\n",
    "    df_oos_ar1 = pd.DataFrame(columns=[\"y_hat\", \"y_true\"])\n",
    "    df_oos_ar1.index = pd.to_datetime(pd.Index([]))\n",
    "\n",
    "print(f\"\\nâœ… Pseudo-OOS terminÃ© â€” n prÃ©visions = {len(df_oos_ar1)}\")\n",
    "print(df_oos_ar1.head(3))\n",
    "\n",
    "# ---------- (facultatif) Scores par pÃ©riode ----------\n",
    "if len(df_oos_ar1):\n",
    "    df_val  = df_oos_ar1.loc[\"1983-01-01\":\"1989-12-31\"].copy()\n",
    "    df_test = df_oos_ar1.loc[\"1990-01-01\":\"2025-08-31\"].copy()\n",
    "\n",
    "    if len(df_val):\n",
    "        mae  = mean_absolute_error(df_val[\"y_true\"], df_val[\"y_hat\"])\n",
    "        rmse = np.sqrt(mean_squared_error(df_val[\"y_true\"], df_val[\"y_hat\"]))\n",
    "        r2   = r2_score(df_val[\"y_true\"], df_val[\"y_hat\"]) if len(df_val) > 1 else np.nan\n",
    "        print(f\"\\nðŸ“Š Validation 83â€“89 â€” n={len(df_val)} | MAE={mae:.3f} | RMSE={rmse:.3f} | RÂ²={r2:.3f}\")\n",
    "\n",
    "    if len(df_test):\n",
    "        mae  = mean_absolute_error(df_test[\"y_true\"], df_test[\"y_hat\"])\n",
    "        rmse = np.sqrt(mean_squared_error(df_test[\"y_true\"], df_test[\"y_hat\"]))\n",
    "        r2   = r2_score(df_test[\"y_true\"], df_test[\"y_hat\"]) if len(df_test) > 1 else np.nan\n",
    "        print(f\"ðŸ“Š Test 90â€“2025 â€” n={len(df_test)} | MAE={mae:.3f} | RMSE={rmse:.3f} | RÂ²={r2:.3f}\")\n",
    "\n",
    "# ---------- Sauvegardes ----------\n",
    "AR1_LAST_PKL  = \"AR1_last_trained_model.pkl\"\n",
    "AR1_LAST_META = \"AR1_last_trained_model_meta.csv\"\n",
    "AR1_BUNDLE    = \"AR1_h12_oos_bundle.pkl\"\n",
    "\n",
    "# 1) modÃ¨le final\n",
    "if last_model is not None:\n",
    "    try:\n",
    "        joblib.dump(last_model, AR1_LAST_PKL)\n",
    "        print(f\"ðŸ’¾ ModÃ¨le AR(1) sauvegardÃ© â†’ {AR1_LAST_PKL}\")\n",
    "    except Exception:\n",
    "        with open(AR1_LAST_PKL, \"wb\") as f:\n",
    "            pickle.dump(last_model, f)\n",
    "        print(f\"ðŸ’¾ ModÃ¨le AR(1) sauvegardÃ© (pickle) â†’ {AR1_LAST_PKL}\")\n",
    "\n",
    "# 2) bundle des sorties (dates normalisÃ©es en dÃ©but de mois)\n",
    "bundle = {\n",
    "    \"oos_predictions\": (\n",
    "        df_oos_ar1.reset_index()\n",
    "                  .rename(columns={\"y_hat\": \"y_pred\"})\n",
    "                  .assign(date=lambda d: pd.to_datetime(d[\"date\"]).dt.to_period(\"M\").dt.to_timestamp(how=\"start\"))\n",
    "    ),\n",
    "    \"params\": {\n",
    "        \"model\": \"AR(1)\",\n",
    "        \"trend\": trend,\n",
    "        \"horizon\": h,\n",
    "        \"lag\": 1,\n",
    "        \"min_train_n\": min_train_n\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"trained_until\": str(last_fit_end.date()) if last_fit_end is not None else None,\n",
    "        \"index_freq\": \"MS\",\n",
    "        \"n_obs_y\": int(len(y)),\n",
    "        \"n_forecasts\": int(len(df_oos_ar1))\n",
    "    }\n",
    "}\n",
    "with open(AR1_BUNDLE, \"wb\") as f:\n",
    "    pickle.dump(bundle, f)\n",
    "print(f\"ðŸ’¾ Bundle AR(1) OOS sauvegardÃ© â†’ {AR1_BUNDLE}\")\n",
    "\n",
    "# 3) mÃ©ta csv\n",
    "meta_row = {\n",
    "    \"model\": \"AR(1)\",\n",
    "    \"trend\": trend,\n",
    "    \"lag\": 1,\n",
    "    \"trained_until\": str(last_fit_end.date()) if last_fit_end is not None else None,\n",
    "    \"n_obs_y\": int(len(y)),\n",
    "    \"n_forecasts\": int(len(df_oos_ar1))\n",
    "}\n",
    "pd.DataFrame([meta_row]).to_csv(AR1_LAST_META)\n",
    "print(f\"ðŸ’¾ MÃ©ta AR(1) sauvegardÃ©e â†’ {AR1_LAST_META}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851d33ba",
   "metadata": {},
   "source": [
    "# Test RÃ©gression linÃ©aire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b058321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b3aed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029126f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DonnÃ©es prÃªtes : 1960-01-01 â†’ 2025-08-01 | n=788 | freq=MS\n",
      "Features (10): ['TB3MS', 'RPI', 'INDPRO', 'DPCERA3M086SBEA', 'S&P 500', 'BUSLOANS'] ...\n",
      "\n",
      "âœ… Pseudo-OOS terminÃ© â€” n prÃ©visions = 741\n",
      "              y_pred  y_true\n",
      "date                        \n",
      "1963-12-01 -0.354113     0.0\n",
      "1964-01-01 -0.282896    -0.1\n",
      "1964-02-01  1.105841    -0.5\n",
      "\n",
      "ðŸ“Š Validation 83â€“89 â€” n=84 | MAE=0.810 | RMSE=1.018 | RÂ²=-0.328\n",
      "ðŸ“Š Test 90â€“2025 â€” n=428 | MAE=0.827 | RMSE=1.460 | RÂ²=0.084\n",
      "\n",
      "ðŸ’¾ Bundle sauvegardÃ© â†’ linear_regression.pkl\n",
      "ðŸ’¾ MÃ©ta sauvegardÃ©e â†’ linear_regression_meta.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# LinearRegression â€” Pseudo-OOS continu (h=12) comme AR(1)\n",
    "# - Initialisation (1960â†’), Validation (1983â€“1989), Test (1990â€“2025)\n",
    "# - Expanding window mensuelle (refit chaque mois)\n",
    "# - Winsorisation + normalisation apprises sur TRAIN courant\n",
    "# - Sauvegardes: bundle + mÃ©ta\n",
    "# =========================================================\n",
    "import os, pickle, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# ---------- ParamÃ¨tres ----------\n",
    "h = 12\n",
    "min_train_n = 36           # â‰¥ 3 ans avant de commencer Ã  prÃ©voir\n",
    "winsor_level = 0.01        # 1er / 99e percentiles\n",
    "norm_var = True            # normaliser ou non\n",
    "target_col = \"UNRATE\"      # cible dans df_stationary\n",
    "\n",
    "# ---------- Bornes des fenÃªtres ----------\n",
    "eval_start = pd.Timestamp(\"1983-01-01\")\n",
    "eval_end   = pd.Timestamp(\"1989-12-31\")\n",
    "test_start = pd.Timestamp(\"1990-01-01\")\n",
    "test_end   = pd.Timestamp(\"2025-12-31\")   # ajuste si besoin (ex: 2025-08-31)\n",
    "\n",
    "# ---------- Fichiers de sortie ----------\n",
    "LINREG_PKL  = \"linear_regression.pkl\"        # bundle (dict) avec oos_predictions, params, etc.\n",
    "LINREG_META = \"linear_regression_meta.csv\"   # mÃ©ta rÃ©sumÃ©\n",
    "# ---------- PrÃ©paration df_stationary ----------\n",
    "def _ensure_ms_index(df):\n",
    "    \"\"\"Met l'index au dÃ©but de mois (MS). Si 'date' existe, on l'utilise comme index.\"\"\"\n",
    "    if \"date\" in df.columns:\n",
    "        df = df.set_index(\"date\")\n",
    "    idx = pd.to_datetime(df.index)\n",
    "    df = df.copy()\n",
    "    df.index = idx.to_period(\"M\").to_timestamp(how=\"start\")\n",
    "    return df.asfreq(\"MS\")\n",
    "\n",
    "# On part de df_stationary (toutes donnÃ©es : 1960â†’2025)\n",
    "df_all = _ensure_ms_index(df_stationary).sort_index()\n",
    "\n",
    "# VÃ©rifs rapides\n",
    "if target_col not in df_all.columns:\n",
    "    raise ValueError(f\"La colonne cible '{target_col}' est absente de df_stationary.\")\n",
    "\n",
    "# X et y\n",
    "y_all = df_all[target_col].astype(float)\n",
    "X_all = df_all.drop(columns=[target_col]).astype(float)\n",
    "features = list(X_all.columns)\n",
    "\n",
    "print(f\"âœ… DonnÃ©es prÃªtes : {df_all.index.min().date()} â†’ {df_all.index.max().date()} | n={len(df_all)} | freq=MS\")\n",
    "print(f\"Features ({len(features)}): {features[:6]}{' ...' if len(features)>6 else ''}\")\n",
    "\n",
    "# ---------- PrÃ©proc ----------\n",
    "def fit_preproc(X, wins=0.01, do_norm=True):\n",
    "    lower = X.quantile(wins)\n",
    "    upper = X.quantile(1 - wins)\n",
    "    Xw = X.clip(lower=lower, upper=upper, axis=1)\n",
    "    if do_norm:\n",
    "        mean = Xw.mean()\n",
    "        std  = Xw.std().replace(0, 1)\n",
    "        Xn   = (Xw - mean) / std\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": mean, \"std\": std, \"norm\": True}\n",
    "        return Xn, prep\n",
    "    else:\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": None, \"std\": None, \"norm\": False}\n",
    "        return Xw, prep\n",
    "\n",
    "def apply_preproc(X, prep):\n",
    "    Xp = X.clip(lower=prep[\"lower\"], upper=prep[\"upper\"], axis=1)\n",
    "    if prep[\"norm\"]:\n",
    "        Xp = (Xp - prep[\"mean\"]) / prep[\"std\"].replace(0, 1)\n",
    "    return Xp\n",
    "\n",
    "# ---------- Boucle pseudo-OOS (expanding, refit mensuel) ----------\n",
    "rows = []                 # (date_forecast, y_hat, y_true)\n",
    "models = []               # modÃ¨les entraÃ®nÃ©s (pour inspection)\n",
    "preprocs = []             # objets prÃ©proc par fit\n",
    "train_ends = []           # derniÃ¨re date de train pour chaque fit\n",
    "\n",
    "last_t_end = y_all.index.max() - relativedelta(months=h)\n",
    "last_model = None\n",
    "last_fit_end = None\n",
    "\n",
    "for t_end in y_all.index:\n",
    "    if t_end > last_t_end:\n",
    "        break\n",
    "\n",
    "    # TRAIN jusqu'Ã  t_end\n",
    "    y_tr = y_all.loc[:t_end]\n",
    "    X_tr = X_all.loc[:t_end]\n",
    "\n",
    "    if len(y_tr) < min_train_n:\n",
    "        continue\n",
    "\n",
    "    # Fit prÃ©proc sur TRAIN courant\n",
    "    X_tr_p, prep = fit_preproc(X_tr, wins=winsor_level, do_norm=norm_var)\n",
    "\n",
    "    # Fit OLS\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_tr_p, y_tr.values)\n",
    "\n",
    "    # PrÃ©vision Ã  l'horizon h => on utilise les features Ã  t_fore\n",
    "    t_fore = t_end + relativedelta(months=h)\n",
    "    if t_fore in y_all.index:\n",
    "        x_fore_raw = X_all.loc[[t_fore]]\n",
    "        x_fore_p = apply_preproc(x_fore_raw, prep)\n",
    "        yhat_h = float(model.predict(x_fore_p)[0])\n",
    "\n",
    "        rows.append((t_fore, yhat_h, float(y_all.loc[t_fore])))\n",
    "\n",
    "    # trace / stockage\n",
    "    last_model = model\n",
    "    last_fit_end = t_end\n",
    "    models.append(model)\n",
    "    preprocs.append(prep)\n",
    "    train_ends.append(t_end)\n",
    "\n",
    "# ---------- DataFrame OOS ----------\n",
    "if rows:\n",
    "    df_oos = (pd.DataFrame(rows, columns=[\"date\", \"y_pred\", \"y_true\"])\n",
    "                .assign(date=lambda d: pd.to_datetime(d[\"date\"]).dt.to_period(\"M\").dt.to_timestamp(how=\"start\"))\n",
    "                .set_index(\"date\").sort_index())\n",
    "else:\n",
    "    df_oos = pd.DataFrame(columns=[\"y_pred\", \"y_true\"])\n",
    "    df_oos.index = pd.to_datetime(pd.Index([]))\n",
    "\n",
    "print(f\"\\nâœ… Pseudo-OOS terminÃ© â€” n prÃ©visions = {len(df_oos)}\")\n",
    "print(df_oos.head(3))\n",
    "\n",
    "# ---------- Scores Validation & Test ----------\n",
    "def _scores(df):\n",
    "    if len(df) == 0:\n",
    "        return {\"MAE\": np.nan, \"RMSE\": np.nan, \"R2\": np.nan}\n",
    "    e = df[\"y_true\"] - df[\"y_pred\"]\n",
    "    mae  = mean_absolute_error(df[\"y_true\"], df[\"y_pred\"])\n",
    "    rmse = np.sqrt(mean_squared_error(df[\"y_true\"], df[\"y_pred\"]))\n",
    "    r2   = r2_score(df[\"y_true\"], df[\"y_pred\"]) if len(df) > 1 else np.nan\n",
    "    return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2)}\n",
    "\n",
    "df_val  = df_oos.loc[eval_start:eval_end].copy()\n",
    "df_test = df_oos.loc[test_start:test_end].copy()\n",
    "\n",
    "sc_val  = _scores(df_val)\n",
    "sc_test = _scores(df_test)\n",
    "\n",
    "print(f\"\\nðŸ“Š Validation 83â€“89 â€” n={len(df_val)} | MAE={sc_val['MAE']:.3f} | RMSE={sc_val['RMSE']:.3f} | RÂ²={sc_val['R2']:.3f}\")\n",
    "print(f\"ðŸ“Š Test 90â€“2025 â€” n={len(df_test)} | MAE={sc_test['MAE']:.3f} | RMSE={sc_test['RMSE']:.3f} | RÂ²={sc_test['R2']:.3f}\")\n",
    "\n",
    "# ---------- Sauvegardes ----------\n",
    "bundle = {\n",
    "    \"oos_predictions\": df_oos.reset_index(),     # (date, y_pred, y_true)\n",
    "    \"params\": {\n",
    "        \"model\": \"LinearRegression\",\n",
    "        \"horizon\": h,\n",
    "        \"min_train_n\": min_train_n,\n",
    "        \"winsor_level\": winsor_level,\n",
    "        \"norm_var\": norm_var,\n",
    "        \"features\": features,\n",
    "        \"eval_window\": (str(eval_start.date()), str(eval_end.date())),\n",
    "        \"test_window\": (str(test_start.date()), str(test_end.date()))\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"trained_until\": str(last_fit_end.date()) if last_fit_end is not None else None,\n",
    "        \"index_freq\": \"MS\",\n",
    "        \"n_obs_all\": int(len(df_all)),\n",
    "        \"n_forecasts\": int(len(df_oos))\n",
    "    },\n",
    "    \"train_fit_dates\": pd.to_datetime(pd.Index(train_ends))  # pour inspection\n",
    "}\n",
    "\n",
    "with open(LINREG_PKL, \"wb\") as f:\n",
    "    pickle.dump(bundle, f)\n",
    "pd.DataFrame([{\n",
    "    \"model\": \"LinearRegression\",\n",
    "    \"horizon\": h,\n",
    "    \"min_train_n\": min_train_n,\n",
    "    \"winsor_level\": winsor_level,\n",
    "    \"norm_var\": norm_var,\n",
    "    \"trained_until\": bundle[\"meta\"][\"trained_until\"],\n",
    "    \"n_forecasts\": bundle[\"meta\"][\"n_forecasts\"]\n",
    "}]).to_csv(LINREG_META)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Bundle sauvegardÃ© â†’ {LINREG_PKL}\")\n",
    "print(f\"ðŸ’¾ MÃ©ta sauvegardÃ©e â†’ {LINREG_META}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2dcd38",
   "metadata": {},
   "source": [
    "# Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ffe4aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# RIDGE â€” Pseudo-OOS (h=12) + Bagging (Ã  la BoE)\n",
    "# - Refit annuel, retune alpha tous les 36 mois (dÃ¨s 1983), scoring=MAE (hv-block CV, gap=12)\n",
    "# - Winsorisation 1%/99% + normalisation apprises sur TRAIN (et par bootstrap pour chaque modÃ¨le)\n",
    "# - Sauvegardes: bundle OOS + artefact \"dernier modÃ¨le\"\n",
    "# ===============================================\n",
    "import os, pickle, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf108f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Fichiers ----------\n",
    "\n",
    "# ---------- ParamÃ¨tres ----------\n",
    "h = 12\n",
    "min_train_n = 36                 # â‰¥ 3 ans avant de commencer\n",
    "winsor_level = 0.01\n",
    "norm_var = True\n",
    "target_col = \"UNRATE\"\n",
    "\n",
    "# Bagging (comme article : 30 modÃ¨les bootstrap)\n",
    "n_boot = 30\n",
    "bootstrap_proportion = 1.0       # (proportion d'Ã©chantillon bootstrap)\n",
    "seed0 = 12345\n",
    "\n",
    "# FenÃªtres\n",
    "eval_start = pd.Timestamp(\"1983-01-01\")\n",
    "eval_end   = pd.Timestamp(\"1989-12-31\")\n",
    "test_start = pd.Timestamp(\"1990-01-01\")\n",
    "test_end   = pd.Timestamp(\"2025-12-31\")\n",
    "\n",
    "# Cadences (article : refit=12 mois, retune=36 mois)\n",
    "refit_every_months = 12\n",
    "retune_every_months = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abadd2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DonnÃ©es prÃªtes : 1960-01-01 â†’ 2025-08-01 | n=788 | freq=MS\n",
      "Features (10): ['TB3MS', 'RPI', 'INDPRO', 'DPCERA3M086SBEA', 'S&P 500', 'BUSLOANS'] ...\n",
      "\n",
      "âœ… Pseudo-OOS Ridge+Bagging (h=12) terminÃ© â€” n prÃ©visions = 741\n",
      "              y_pred  y_true  y_pred_std\n",
      "date                                    \n",
      "1963-12-01 -0.254922     0.0    0.320833\n",
      "1964-01-01 -0.038495    -0.1    0.357014\n",
      "1964-02-01  1.302823    -0.5    0.675225\n",
      "\n",
      "ðŸ“Š Validation 83â€“89 â€” n=84 | MAE=0.802 | RMSE=1.001 | RÂ²=-0.282\n",
      "ðŸ“Š Test 90â€“2025 â€” n=428 | MAE=0.825 | RMSE=1.471 | RÂ²=0.071\n",
      "\n",
      "ðŸ’¾ Bundle OOS sauvegardÃ© â†’ ridge_regression.pkl\n",
      "ðŸ’¾ MÃ©ta bundle       â†’ ridge_regression_meta.csv\n",
      "ðŸ’¾ Dernier modÃ¨le    â†’ RIDGE_last_trained_model.pkl\n",
      "ðŸ’¾ MÃ©ta dernier fit  â†’ RIDGE_last_trained_model_meta.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------- PrÃ©paration df_stationary ----------\n",
    "def _ensure_ms_index(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Index mensuel (MS). Si 'date' existe, on l'utilise comme index.\"\"\"\n",
    "    df = df.copy()\n",
    "    if \"date\" in df.columns:\n",
    "        df = df.set_index(\"date\")\n",
    "    idx = pd.to_datetime(df.index)\n",
    "    df.index = idx.to_period(\"M\").to_timestamp(how=\"start\")\n",
    "    return df.asfreq(\"MS\")\n",
    "\n",
    "# âš ï¸ On suppose que df_stationary est dÃ©jÃ  en mÃ©moire (toutes sÃ©ries transformÃ©es)\n",
    "df_all = _ensure_ms_index(df_stationary).sort_index()\n",
    "\n",
    "if target_col not in df_all.columns:\n",
    "    raise ValueError(f\"La colonne cible '{target_col}' est absente de df_stationary.\")\n",
    "\n",
    "y_all = df_all[target_col].astype(float)\n",
    "X_all = df_all.drop(columns=[target_col]).astype(float)\n",
    "features = list(X_all.columns)\n",
    "\n",
    "print(f\"âœ… DonnÃ©es prÃªtes : {df_all.index.min().date()} â†’ {df_all.index.max().date()} | n={len(df_all)} | freq=MS\")\n",
    "print(f\"Features ({len(features)}): {features[:6]}{' ...' if len(features)>6 else ''}\")\n",
    "\n",
    "# ---------- PrÃ©proc ----------\n",
    "def fit_preproc(X: pd.DataFrame, wins=0.01, do_norm=True):\n",
    "    lower = X.quantile(wins)\n",
    "    upper = X.quantile(1 - wins)\n",
    "    Xw = X.clip(lower=lower, upper=upper, axis=1)\n",
    "    if do_norm:\n",
    "        mean = Xw.mean()\n",
    "        std  = Xw.std().replace(0, 1)\n",
    "        Xn   = (Xw - mean) / std\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": mean, \"std\": std, \"norm\": True}\n",
    "        return Xn, prep\n",
    "    else:\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": None, \"std\": None, \"norm\": False}\n",
    "        return Xw, prep\n",
    "\n",
    "def apply_preproc(X: pd.DataFrame, prep: dict):\n",
    "    Xp = X.clip(lower=prep[\"lower\"], upper=prep[\"upper\"], axis=1)\n",
    "    if prep[\"norm\"]:\n",
    "        Xp = (Xp - prep[\"mean\"]) / prep[\"std\"].replace(0, 1)\n",
    "    return Xp\n",
    "\n",
    "# ---------- hv-block CV (5 blocs consÃ©cutifs, gap=12 autour du test) ----------\n",
    "class HVBlockCV:\n",
    "    def __init__(self, n_splits=5, gap=12):\n",
    "        self.n_splits = n_splits\n",
    "        self.gap = gap\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n = len(X)\n",
    "        fold_sizes = np.full(self.n_splits, n // self.n_splits, dtype=int)\n",
    "        fold_sizes[: n % self.n_splits] += 1\n",
    "        idx = np.arange(n)\n",
    "        cur = 0\n",
    "        for fs in fold_sizes:\n",
    "            start, stop = cur, cur + fs\n",
    "            test_idx = idx[start:stop]\n",
    "            train_mask = np.ones(n, dtype=bool)\n",
    "            left = max(0, start - self.gap)\n",
    "            right = min(n, stop + self.gap)\n",
    "            train_mask[left:right] = False\n",
    "            train_idx = idx[train_mask]\n",
    "            cur = stop\n",
    "            if len(train_idx) == 0 or len(test_idx) == 0:\n",
    "                continue\n",
    "            yield train_idx, test_idx\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "# ---------- Grille d'alpha ----------\n",
    "alpha_grid = np.logspace(-5, 4, 100)\n",
    "\n",
    "def tune_ridge_alpha(X_tr_p: pd.DataFrame, y_tr: np.ndarray):\n",
    "    \"\"\"Tuning d'alpha par hv-block CV (gap=12), scoring=MAE. Retourne (best_alpha, best_mae, best_estimator).\"\"\"\n",
    "    cv = HVBlockCV(n_splits=5, gap=12)\n",
    "    gs = GridSearchCV(\n",
    "        Ridge(fit_intercept=True),\n",
    "        param_grid={\"alpha\": alpha_grid},\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    gs.fit(X_tr_p, y_tr)\n",
    "    return float(gs.best_params_[\"alpha\"]), float(-gs.best_score_), gs.best_estimator_\n",
    "\n",
    "# ---------- Bootstrap util ----------\n",
    "def bootstrap_indices(n, proportion=1.0, seed=None):\n",
    "    m = int(round(n * proportion))\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    return rng_local.integers(0, n, size=m, endpoint=False)\n",
    "\n",
    "# ---------- Boucle pseudo-OOS ----------\n",
    "rows = []                    # (date_forecast, y_pred_mean, y_true, y_pred_std)\n",
    "train_ends = []              # dates de refit\n",
    "alpha_history = []           # Î± utilisÃ© Ã  chaque refit\n",
    "cv_mae_history = []          # MAE CV Ã  chaque retune\n",
    "\n",
    "# Artefact \"dernier modÃ¨le\" (ensemble bootstrap du dernier refit)\n",
    "last_boot_models = []        # liste de dicts: {\"prep\": prep_dict, \"alpha\": float, \"coef\": np.ndarray, \"intercept\": float}\n",
    "last_fit_end = None\n",
    "last_best_alpha = None\n",
    "\n",
    "last_t_end = y_all.index.max() - relativedelta(months=h)\n",
    "last_refit_t = None\n",
    "last_tune_t  = None\n",
    "best_alpha   = 1.0\n",
    "boot_seed    = seed0\n",
    "\n",
    "boot_models = []  # liste courante [(prep_b, mdl_b)]\n",
    "\n",
    "for t_end in y_all.index:\n",
    "    if t_end > last_t_end:\n",
    "        break\n",
    "\n",
    "    y_tr = y_all.loc[:t_end]\n",
    "    X_tr = X_all.loc[:t_end]\n",
    "    n_tr = len(y_tr)\n",
    "    if n_tr < min_train_n:\n",
    "        continue\n",
    "\n",
    "    # cadence refit (tous les 12 mois)\n",
    "    if last_refit_t is None:\n",
    "        need_refit = True\n",
    "    else:\n",
    "        months_since_refit = (t_end.year - last_refit_t.year)*12 + (t_end.month - last_refit_t.month)\n",
    "        need_refit = months_since_refit >= refit_every_months\n",
    "\n",
    "    # cadence retune (â‰¥ 1983 puis tous les 36 mois)\n",
    "    need_tune = False\n",
    "    if t_end >= eval_start:\n",
    "        if last_tune_t is None:\n",
    "            need_tune = True\n",
    "        else:\n",
    "            months_since_tune = (t_end.year - last_tune_t.year)*12 + (t_end.month - last_tune_t.month)\n",
    "            need_tune = months_since_tune >= retune_every_months\n",
    "\n",
    "    if need_refit:\n",
    "        # prÃ©proc global (pour tuning)\n",
    "        X_tr_p_global, _ = fit_preproc(X_tr, wins=winsor_level, do_norm=norm_var)\n",
    "\n",
    "        # tuning alpha si demandÃ©\n",
    "        if need_tune:\n",
    "            best_alpha, best_cv_mae, _ = tune_ridge_alpha(X_tr_p_global, y_tr.values)\n",
    "            last_tune_t = t_end\n",
    "            cv_mae_history.append(best_cv_mae)\n",
    "        else:\n",
    "            cv_mae_history.append(np.nan)\n",
    "\n",
    "        # bagging : n_boot modÃ¨les bootstrap (prÃ©proc appris par bootstrap)\n",
    "        boot_models = []\n",
    "        for b in range(n_boot):\n",
    "            b_seed = boot_seed + b\n",
    "            ix = bootstrap_indices(n_tr, proportion=bootstrap_proportion, seed=b_seed)\n",
    "            Xb_raw = X_tr.iloc[ix]\n",
    "            yb = y_tr.iloc[ix].values\n",
    "\n",
    "            Xb_p, prep_b = fit_preproc(Xb_raw, wins=winsor_level, do_norm=norm_var)\n",
    "            mdl_b = Ridge(alpha=best_alpha, fit_intercept=True)\n",
    "            mdl_b.fit(Xb_p, yb)\n",
    "\n",
    "            boot_models.append((prep_b, mdl_b))\n",
    "\n",
    "        # mÃ©moires pour OOS + artefact\n",
    "        last_refit_t = t_end\n",
    "        train_ends.append(t_end)\n",
    "        alpha_history.append(best_alpha)\n",
    "        boot_seed += 9973\n",
    "\n",
    "        # artefact \"dernier modÃ¨le\"\n",
    "        last_boot_models = []\n",
    "        for prep_b, mdl_b in boot_models:\n",
    "            last_boot_models.append({\n",
    "                \"prep\": {\n",
    "                    \"lower\": prep_b[\"lower\"].to_dict(),\n",
    "                    \"upper\": prep_b[\"upper\"].to_dict(),\n",
    "                    \"mean\":  prep_b[\"mean\"].to_dict() if prep_b[\"norm\"] else None,\n",
    "                    \"std\":   prep_b[\"std\"].to_dict() if prep_b[\"norm\"] else None,\n",
    "                    \"norm\":  prep_b[\"norm\"]\n",
    "                },\n",
    "                \"alpha\": float(mdl_b.alpha),\n",
    "                \"coef\":  mdl_b.coef_.tolist(),\n",
    "                \"intercept\": float(mdl_b.intercept_)\n",
    "            })\n",
    "        last_fit_end = t_end\n",
    "        last_best_alpha = best_alpha\n",
    "\n",
    "    # prÃ©vision h=12 si un ensemble est dispo\n",
    "    if not boot_models:\n",
    "        continue\n",
    "\n",
    "    t_fore = t_end + relativedelta(months=h)\n",
    "    if t_fore in y_all.index:\n",
    "        x_fore_raw = X_all.loc[[t_fore]]\n",
    "        preds_b = []\n",
    "        for prep_b, mdl_b in boot_models:\n",
    "            x_fore_p = apply_preproc(x_fore_raw, prep_b)\n",
    "            preds_b.append(float(mdl_b.predict(x_fore_p)[0]))\n",
    "        yhat_mean = float(np.mean(preds_b))\n",
    "        yhat_std  = float(np.std(preds_b, ddof=1)) if len(preds_b) > 1 else 0.0\n",
    "        rows.append((t_fore, yhat_mean, float(y_all.loc[t_fore]), yhat_std))\n",
    "\n",
    "# ---------- DataFrame OOS ----------\n",
    "if rows:\n",
    "    df_oos = (pd.DataFrame(rows, columns=[\"date\",\"y_pred\",\"y_true\",\"y_pred_std\"])\n",
    "              .assign(date=lambda d: pd.to_datetime(d[\"date\"]).dt.to_period(\"M\").dt.to_timestamp(how=\"start\"))\n",
    "              .set_index(\"date\").sort_index())\n",
    "else:\n",
    "    df_oos = pd.DataFrame(columns=[\"y_pred\",\"y_true\",\"y_pred_std\"])\n",
    "    df_oos.index = pd.to_datetime(pd.Index([]))\n",
    "\n",
    "print(f\"\\nâœ… Pseudo-OOS Ridge+Bagging (h=12) terminÃ© â€” n prÃ©visions = {len(df_oos)}\")\n",
    "print(df_oos.head(3))\n",
    "\n",
    "# ---------- Scores ----------\n",
    "def _scores(df):\n",
    "    if len(df) == 0:\n",
    "        return {\"MAE\": np.nan, \"RMSE\": np.nan, \"R2\": np.nan}\n",
    "    mae  = mean_absolute_error(df[\"y_true\"], df[\"y_pred\"])\n",
    "    rmse = np.sqrt(mean_squared_error(df[\"y_true\"], df[\"y_pred\"]))\n",
    "    r2   = r2_score(df[\"y_true\"], df[\"y_pred\"]) if len(df) > 1 else np.nan\n",
    "    return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2)}\n",
    "\n",
    "df_val  = df_oos.loc[eval_start:eval_end].copy()\n",
    "df_test = df_oos.loc[test_start:test_end].copy()\n",
    "\n",
    "sc_val  = _scores(df_val)\n",
    "sc_test = _scores(df_test)\n",
    "\n",
    "print(f\"\\nðŸ“Š Validation 83â€“89 â€” n={len(df_val)} | MAE={sc_val['MAE']:.3f} | RMSE={sc_val['RMSE']:.3f} | RÂ²={sc_val['R2']:.3f}\")\n",
    "print(f\"ðŸ“Š Test 90â€“2025 â€” n={len(df_test)} | MAE={sc_test['MAE']:.3f} | RMSE={sc_test['RMSE']:.3f} | RÂ²={sc_test['R2']:.3f}\")\n",
    "\n",
    "# ---------- Sauvegardes ----------\n",
    "# 1) Bundle OOS\n",
    "bundle = {\n",
    "    \"oos_predictions\": df_oos.reset_index(),\n",
    "    \"params\": {\n",
    "        \"model\": \"Ridge + Bagging\",\n",
    "        \"n_boot\": n_boot,\n",
    "        \"bootstrap_proportion\": bootstrap_proportion,\n",
    "        \"alpha_schedule\": \"hv-block CV (5 folds, gap=12), MAE, every 36 months since 1983\",\n",
    "        \"horizon\": h,\n",
    "        \"min_train_n\": min_train_n,\n",
    "        \"winsor_level\": winsor_level,\n",
    "        \"norm_var\": norm_var,\n",
    "        \"features\": features,\n",
    "        \"eval_window\": (str(eval_start.date()), str(eval_end.date())),\n",
    "        \"test_window\": (str(test_start.date()), str(test_end.date())),\n",
    "        \"refit_every_months\": refit_every_months,\n",
    "        \"retune_every_months\": retune_every_months\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"trained_until\": str(train_ends[-1].date()) if len(train_ends) else None,\n",
    "        \"index_freq\": \"MS\",\n",
    "        \"n_obs_all\": int(len(df_all)),\n",
    "        \"n_forecasts\": int(len(df_oos)),\n",
    "        \"alpha_history\": alpha_history,\n",
    "        \"cv_mae_history\": cv_mae_history\n",
    "    },\n",
    "    \"train_fit_dates\": pd.to_datetime(pd.Index(train_ends))\n",
    "}\n",
    "with open(RIDGE_BUNDLE, \"wb\") as f:\n",
    "    pickle.dump(bundle, f)\n",
    "pd.DataFrame([{\n",
    "    \"model\": \"Ridge+Bagging\",\n",
    "    \"n_boot\": n_boot,\n",
    "    \"horizon\": h,\n",
    "    \"min_train_n\": min_train_n,\n",
    "    \"winsor_level\": winsor_level,\n",
    "    \"norm_var\": norm_var,\n",
    "    \"refit_every_months\": refit_every_months,\n",
    "    \"retune_every_months\": retune_every_months,\n",
    "    \"trained_until\": bundle[\"meta\"][\"trained_until\"],\n",
    "    \"n_forecasts\": bundle[\"meta\"][\"n_forecasts\"]\n",
    "}]).to_csv(RIDGE_META, index=False)\n",
    "\n",
    "# 2) Artefact dernier modÃ¨le (ensemble bootstrap)\n",
    "ridge_artifact = {\n",
    "    \"trained_until\": str(last_fit_end.date()) if last_fit_end is not None else None,\n",
    "    \"horizon\": h,\n",
    "    \"best_alpha\": float(last_best_alpha) if last_best_alpha is not None else None,\n",
    "    \"features\": features,\n",
    "    \"n_boot\": n_boot,\n",
    "    \"models\": last_boot_models  # liste de {prep, alpha, coef, intercept}\n",
    "}\n",
    "with open(RIDGE_LAST_PKL, \"wb\") as f:\n",
    "    pickle.dump(ridge_artifact, f)\n",
    "pd.DataFrame([{\n",
    "    \"trained_until\": ridge_artifact[\"trained_until\"],\n",
    "    \"best_alpha\": ridge_artifact[\"best_alpha\"],\n",
    "    \"n_features\": len(features),\n",
    "    \"horizon\": h,\n",
    "    \"n_boot\": n_boot\n",
    "}]).to_csv(RIDGE_LAST_META, index=False)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Bundle OOS sauvegardÃ© â†’ {RIDGE_BUNDLE}\")\n",
    "print(f\"ðŸ’¾ MÃ©ta bundle       â†’ {RIDGE_META}\")\n",
    "print(f\"ðŸ’¾ Dernier modÃ¨le    â†’ {RIDGE_LAST_PKL}\")\n",
    "print(f\"ðŸ’¾ MÃ©ta dernier fit  â†’ {RIDGE_LAST_META}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23013bbd",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2911616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DonnÃ©es prÃªtes : 1960-01-01 â†’ 2025-08-01 | n=788 | freq=MS\n",
      "Features (10): ['TB3MS', 'RPI', 'INDPRO', 'DPCERA3M086SBEA', 'S&P_500', 'BUSLOANS'] ...\n",
      "\n",
      "âœ… Pseudo-OOS LightGBM+Bagging (h=12) terminÃ© â€” n prÃ©visions = 741\n",
      "              y_pred  y_true  y_pred_std\n",
      "date                                    \n",
      "1963-12-01  0.097778     0.0     0.20658\n",
      "1964-01-01  0.097778    -0.1     0.20658\n",
      "1964-02-01  0.097778    -0.5     0.20658\n",
      "\n",
      "ðŸ“Š Validation 83â€“89 â€” n=84 | MAE=0.798 | RMSE=1.021 | RÂ²=-0.336\n",
      "ðŸ“Š Test 90â€“2025 â€” n=428 | MAE=0.782 | RMSE=1.385 | RÂ²=0.176\n",
      "\n",
      "ðŸ’¾ Bundle OOS sauvegardÃ© â†’ lightgbm_regression.pkl\n",
      "ðŸ’¾ MÃ©ta bundle       â†’ lightgbm_regression_meta.csv\n",
      "ðŸ’¾ Dernier modÃ¨le    â†’ LGBM_last_trained_model.pkl\n",
      "ðŸ’¾ MÃ©ta dernier fit  â†’ LGBM_last_trained_model_meta.csv\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# LIGHTGBM â€” Pseudo-OOS (h=12) + Bagging (Ã  la BoE)\n",
    "# - Refit annuel, retune hyperparams tous les 36 mois (dÃ¨s 1983), scoring=MAE (hv-block CV, gap=12)\n",
    "# - Winsorisation 1%/99% + normalisation apprises sur TRAIN (et par bootstrap pour chaque modÃ¨le)\n",
    "# - Sauvegardes: bundle OOS + artefact \"dernier modÃ¨le\"\n",
    "# ===============================================\n",
    "import os, pickle, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# ---------- Fichiers ----------\n",
    "LGBM_BUNDLE     = \"lightgbm_regression.pkl\"              # bundle OOS\n",
    "LGBM_META       = \"lightgbm_regression_meta.csv\"         # mÃ©ta du bundle\n",
    "LGBM_LAST_PKL   = \"LGBM_last_trained_model.pkl\"          # artefact dernier ensemble (liste de (prep, model))\n",
    "LGBM_LAST_META  = \"LGBM_last_trained_model_meta.csv\"     # mÃ©ta dernier fit\n",
    "\n",
    "# ---------- ParamÃ¨tres ----------\n",
    "h = 12\n",
    "min_train_n = 36                 # â‰¥ 3 ans avant de commencer\n",
    "winsor_level = 0.01\n",
    "norm_var = True\n",
    "target_col = \"UNRATE\"\n",
    "\n",
    "# Bagging (comme article : 30 modÃ¨les bootstrap)\n",
    "n_boot = 30\n",
    "bootstrap_proportion = 1.0\n",
    "seed0 = 12345\n",
    "\n",
    "# FenÃªtres\n",
    "eval_start = pd.Timestamp(\"1983-01-01\")\n",
    "eval_end   = pd.Timestamp(\"1989-12-31\")\n",
    "test_start = pd.Timestamp(\"1990-01-01\")\n",
    "# ajuste selon ton jeu (2019-11-30 si tu veux coller au papier)\n",
    "test_end   = pd.Timestamp(\"2025-12-31\")\n",
    "\n",
    "# Cadences\n",
    "refit_every_months = 12\n",
    "retune_every_months = 36\n",
    "\n",
    "# ---------- PrÃ©paration df_stationary ----------\n",
    "def _ensure_ms_index(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Index mensuel (MS). Si 'date' existe, on l'utilise comme index.\"\"\"\n",
    "    df = df.copy()\n",
    "    if \"date\" in df.columns:\n",
    "        df = df.set_index(\"date\")\n",
    "    idx = pd.to_datetime(df.index)\n",
    "    df.index = idx.to_period(\"M\").to_timestamp(how=\"start\")\n",
    "    return df.asfreq(\"MS\")\n",
    "\n",
    "# âš ï¸ On suppose que df_stationary est dÃ©jÃ  en mÃ©moire (toutes sÃ©ries transformÃ©es)\n",
    "df_all = _ensure_ms_index(df_stationary).sort_index()\n",
    "\n",
    "if target_col not in df_all.columns:\n",
    "    raise ValueError(f\"La colonne cible '{target_col}' est absente de df_stationary.\")\n",
    "\n",
    "y_all = df_all[target_col].astype(float)\n",
    "X_all = df_all.drop(columns=[target_col]).astype(float)\n",
    "# noms de features sans espaces (LightGBM warning â†’ underscores)\n",
    "X_all.columns = [str(c).replace(\" \", \"_\") for c in X_all.columns]\n",
    "features = list(X_all.columns)\n",
    "\n",
    "print(f\"âœ… DonnÃ©es prÃªtes : {df_all.index.min().date()} â†’ {df_all.index.max().date()} | n={len(df_all)} | freq=MS\")\n",
    "print(f\"Features ({len(features)}): {features[:6]}{' ...' if len(features)>6 else ''}\")\n",
    "\n",
    "# ---------- PrÃ©proc ----------\n",
    "def fit_preproc(X: pd.DataFrame, wins=0.01, do_norm=True):\n",
    "    lower = X.quantile(wins)\n",
    "    upper = X.quantile(1 - wins)\n",
    "    Xw = X.clip(lower=lower, upper=upper, axis=1)\n",
    "    if do_norm:\n",
    "        mean = Xw.mean()\n",
    "        std  = Xw.std().replace(0, 1)\n",
    "        Xn   = (Xw - mean) / std\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": mean, \"std\": std, \"norm\": True}\n",
    "        return Xn, prep\n",
    "    else:\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": None, \"std\": None, \"norm\": False}\n",
    "        return Xw, prep\n",
    "\n",
    "def apply_preproc(X: pd.DataFrame, prep: dict):\n",
    "    Xp = X.clip(lower=prep[\"lower\"], upper=prep[\"upper\"], axis=1)\n",
    "    if prep[\"norm\"]:\n",
    "        Xp = (Xp - prep[\"mean\"]) / prep[\"std\"].replace(0, 1)\n",
    "    return Xp\n",
    "\n",
    "# ---------- hv-block CV (5 blocs consÃ©cutifs, gap=12 autour du test) ----------\n",
    "class HVBlockCV:\n",
    "    def __init__(self, n_splits=5, gap=12):\n",
    "        self.n_splits = n_splits\n",
    "        self.gap = gap\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n = len(X)\n",
    "        fold_sizes = np.full(self.n_splits, n // self.n_splits, dtype=int)\n",
    "        fold_sizes[: n % self.n_splits] += 1\n",
    "        idx = np.arange(n)\n",
    "        cur = 0\n",
    "        for fs in fold_sizes:\n",
    "            start, stop = cur, cur + fs\n",
    "            test_idx = idx[start:stop]\n",
    "            train_mask = np.ones(n, dtype=bool)\n",
    "            left = max(0, start - self.gap)\n",
    "            right = min(n, stop + self.gap)\n",
    "            train_mask[left:right] = False\n",
    "            train_idx = idx[train_mask]\n",
    "            cur = stop\n",
    "            if len(train_idx) == 0 or len(test_idx) == 0:\n",
    "                continue\n",
    "            yield train_idx, test_idx\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "# ---------- Grille LightGBM (proche repo) ----------\n",
    "# Ajout de garde-fous pour Ã©viter \"no leaves meet split requirements\"\n",
    "param_dist = {\n",
    "    \"subsample\":        [0.05,.1,.2,.3,.4,.5,.6,.7,.8,.9,1.0],\n",
    "    \"colsample_bytree\": [.2,.3,.4,.5,.6,.7,1.0],\n",
    "    \"num_leaves\":       [2,3,4,5,8,10,20,40,70,100],\n",
    "    \"n_estimators\":     [5,10,20,30,40,50,75,100],  # Ã©viter 1/3 qui dÃ©gÃ©nÃ¨rent souvent\n",
    "    \"max_depth\":        [1,2,3,5,8,15,-1],          # -1 = illimitÃ©\n",
    "    \"reg_alpha\":        [0, .1, 1, 2, 7, 10, 50, 100],\n",
    "    \"reg_lambda\":       [0, .1, 1, 10, 20, 50, 100],\n",
    "    \"min_child_samples\":[5,10,15],                  # NEW: stabilitÃ©\n",
    "    \"min_split_gain\":   [0.0, 0.01, 0.05],          # NEW: stabilitÃ©\n",
    "}\n",
    "\n",
    "def tune_lgbm(X_tr_p: pd.DataFrame, y_tr: np.ndarray, seed=None):\n",
    "    cv = HVBlockCV(n_splits=5, gap=12)\n",
    "    base = LGBMRegressor(\n",
    "        boosting_type=\"gbdt\",\n",
    "        n_jobs=1,\n",
    "        random_state=seed,\n",
    "        objective=\"regression\",\n",
    "        # paramÃ¨tres fixes sÃ»rs\n",
    "        importance_type=\"gain\",\n",
    "        verbose=-1\n",
    "    )\n",
    "    rs = RandomizedSearchCV(\n",
    "        estimator=base,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=100,\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=seed,\n",
    "        refit=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    rs.fit(X_tr_p, y_tr)\n",
    "    return rs.best_params_, float(-rs.best_score_), rs.best_estimator_\n",
    "\n",
    "# ---------- Bootstrap util ----------\n",
    "def bootstrap_indices(n, proportion=1.0, seed=None):\n",
    "    m = int(round(n * proportion))\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    return rng_local.integers(0, n, size=m, endpoint=False)\n",
    "\n",
    "def _to_month_start(ts):\n",
    "    return pd.Timestamp(ts).to_period(\"M\").to_timestamp(how=\"start\") if ts is not None else None\n",
    "\n",
    "# ---------- Boucle pseudo-OOS ----------\n",
    "rows = []                    # (date_forecast, y_pred_mean, y_true, y_pred_std)\n",
    "train_ends = []              # dates de refit\n",
    "cv_mae_history = []          # MAE CV Ã  chaque retune\n",
    "best_params_hist = []        # pour info\n",
    "\n",
    "# Artefact \"dernier modÃ¨le\"\n",
    "last_boot_models = []        # liste de dicts: {\"prep\": prep_dict, \"params\": dict}\n",
    "last_fit_end = None\n",
    "last_best_params = None\n",
    "\n",
    "last_t_end = y_all.index.max() - relativedelta(months=h)\n",
    "last_refit_t = None\n",
    "last_tune_t  = None\n",
    "best_params  = {}            # params LGBM courants\n",
    "boot_seed    = seed0\n",
    "\n",
    "boot_models = []  # liste courante [(prep_b, mdl_b)]\n",
    "\n",
    "for t_end in y_all.index:\n",
    "    if t_end > last_t_end:\n",
    "        break\n",
    "\n",
    "    y_tr = y_all.loc[:t_end]\n",
    "    X_tr = X_all.loc[:t_end]\n",
    "    n_tr = len(y_tr)\n",
    "    if n_tr < min_train_n:\n",
    "        continue\n",
    "\n",
    "    # cadence refit (tous les 12 mois)\n",
    "    if last_refit_t is None:\n",
    "        need_refit = True\n",
    "    else:\n",
    "        months_since_refit = (t_end.year - last_refit_t.year)*12 + (t_end.month - last_refit_t.month)\n",
    "        need_refit = months_since_refit >= refit_every_months\n",
    "\n",
    "    # cadence retune (â‰¥ 1983 puis tous les 36 mois)\n",
    "    need_tune = False\n",
    "    if t_end >= eval_start:\n",
    "        if last_tune_t is None:\n",
    "            need_tune = True\n",
    "        else:\n",
    "            months_since_tune = (t_end.year - last_tune_t.year)*12 + (t_end.month - last_tune_t.month)\n",
    "            need_tune = months_since_tune >= retune_every_months\n",
    "\n",
    "    if need_refit:\n",
    "        # prÃ©proc global (pour tuning)\n",
    "        X_tr_p_global, _ = fit_preproc(X_tr, wins=winsor_level, do_norm=norm_var)\n",
    "\n",
    "        # tuning hyperparams si demandÃ©\n",
    "        if need_tune:\n",
    "            best_params, best_cv_mae, _ = tune_lgbm(X_tr_p_global, y_tr.values, seed=seed0)\n",
    "            last_tune_t = t_end\n",
    "            cv_mae_history.append(best_cv_mae)\n",
    "            best_params_hist.append(best_params.copy())\n",
    "        else:\n",
    "            cv_mae_history.append(np.nan)\n",
    "            best_params_hist.append(best_params.copy() if best_params else {})\n",
    "\n",
    "        # bagging : n_boot modÃ¨les bootstrap (prÃ©proc appris par bootstrap)\n",
    "        boot_models = []\n",
    "        for b in range(n_boot):\n",
    "            b_seed = boot_seed + b\n",
    "            ix = bootstrap_indices(n_tr, proportion=bootstrap_proportion, seed=b_seed)\n",
    "            Xb_raw = X_tr.iloc[ix]\n",
    "            yb = y_tr.iloc[ix].values\n",
    "\n",
    "            Xb_p, prep_b = fit_preproc(Xb_raw, wins=winsor_level, do_norm=norm_var)\n",
    "\n",
    "            mdl_b = LGBMRegressor(\n",
    "                boosting_type=\"gbdt\",\n",
    "                n_jobs=1,\n",
    "                random_state=b_seed,\n",
    "                objective=\"regression\",\n",
    "                importance_type=\"gain\",\n",
    "                verbose=-1,\n",
    "                **best_params\n",
    "            )\n",
    "            mdl_b.fit(Xb_p, yb)\n",
    "            boot_models.append((prep_b, mdl_b))\n",
    "\n",
    "        # mÃ©moires pour OOS + artefact\n",
    "        last_refit_t = t_end\n",
    "        train_ends.append(t_end)\n",
    "        boot_seed += 9973\n",
    "\n",
    "        # artefact \"dernier modÃ¨le\"\n",
    "        last_boot_models = []\n",
    "        for prep_b, mdl_b in boot_models:\n",
    "            last_boot_models.append({\n",
    "                \"prep\": {\n",
    "                    \"lower\": prep_b[\"lower\"].to_dict(),\n",
    "                    \"upper\": prep_b[\"upper\"].to_dict(),\n",
    "                    \"mean\":  prep_b[\"mean\"].to_dict() if prep_b[\"norm\"] else None,\n",
    "                    \"std\":   prep_b[\"std\"].to_dict() if prep_b[\"norm\"] else None,\n",
    "                    \"norm\":  prep_b[\"norm\"]\n",
    "                },\n",
    "                \"params\": mdl_b.get_params()  # hyperparams retenus\n",
    "            })\n",
    "        last_fit_end = t_end\n",
    "        last_best_params = best_params.copy()\n",
    "\n",
    "    # prÃ©vision h=12 si un ensemble est dispo\n",
    "    if not boot_models:\n",
    "        continue\n",
    "\n",
    "    t_fore = t_end + relativedelta(months=h)\n",
    "    if t_fore in y_all.index:\n",
    "        x_fore_raw = X_all.loc[[t_fore]]\n",
    "        preds_b = []\n",
    "        for prep_b, mdl_b in boot_models:\n",
    "            x_fore_p = apply_preproc(x_fore_raw, prep_b)\n",
    "            preds_b.append(float(mdl_b.predict(x_fore_p)[0]))\n",
    "        yhat_mean = float(np.mean(preds_b))\n",
    "        yhat_std  = float(np.std(preds_b, ddof=1)) if len(preds_b) > 1 else 0.0\n",
    "        rows.append((t_fore, yhat_mean, float(y_all.loc[t_fore]), yhat_std))\n",
    "\n",
    "# ---------- DataFrame OOS ----------\n",
    "if rows:\n",
    "    df_oos = (pd.DataFrame(rows, columns=[\"date\",\"y_pred\",\"y_true\",\"y_pred_std\"])\n",
    "              .assign(date=lambda d: pd.to_datetime(d[\"date\"]).dt.to_period(\"M\").dt.to_timestamp(how=\"start\"))\n",
    "              .set_index(\"date\").sort_index())\n",
    "else:\n",
    "    df_oos = pd.DataFrame(columns=[\"y_pred\",\"y_true\",\"y_pred_std\"])\n",
    "    df_oos.index = pd.to_datetime(pd.Index([]))\n",
    "\n",
    "print(f\"\\nâœ… Pseudo-OOS LightGBM+Bagging (h=12) terminÃ© â€” n prÃ©visions = {len(df_oos)}\")\n",
    "print(df_oos.head(3))\n",
    "\n",
    "# ---------- Scores ----------\n",
    "def _scores(df):\n",
    "    if len(df) == 0:\n",
    "        return {\"MAE\": np.nan, \"RMSE\": np.nan, \"R2\": np.nan}\n",
    "    mae  = mean_absolute_error(df[\"y_true\"], df[\"y_pred\"])\n",
    "    rmse = np.sqrt(mean_squared_error(df[\"y_true\"], df[\"y_pred\"]))\n",
    "    ssr  = np.sum((df[\"y_true\"]-df[\"y_pred\"])**2)\n",
    "    sst  = np.sum((df[\"y_true\"]-df[\"y_true\"].mean())**2)\n",
    "    r2   = 1 - ssr/sst if sst>0 else np.nan\n",
    "    return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2)}\n",
    "\n",
    "df_val  = df_oos.loc[eval_start:eval_end].copy()\n",
    "df_test = df_oos.loc[test_start:test_end].copy()\n",
    "\n",
    "sc_val  = _scores(df_val)\n",
    "sc_test = _scores(df_test)\n",
    "\n",
    "print(f\"\\nðŸ“Š Validation 83â€“89 â€” n={len(df_val)} | MAE={sc_val['MAE']:.3f} | RMSE={sc_val['RMSE']:.3f} | RÂ²={sc_val['R2']:.3f}\")\n",
    "print(f\"ðŸ“Š Test 90â€“{test_end.year} â€” n={len(df_test)} | MAE={sc_test['MAE']:.3f} | RMSE={sc_test['RMSE']:.3f} | RÂ²={sc_test['R2']:.3f}\")\n",
    "\n",
    "# ---------- Sauvegardes ----------\n",
    "bundle = {\n",
    "    \"oos_predictions\": df_oos.reset_index(),\n",
    "    \"params\": {\n",
    "        \"model\": \"LightGBM + Bagging\",\n",
    "        \"n_boot\": n_boot,\n",
    "        \"bootstrap_proportion\": bootstrap_proportion,\n",
    "        \"hyper_search\": \"hv-block CV (5 folds, gap=12), MAE, every 36 months since 1983, RandomizedSearch 100 iters\",\n",
    "        \"horizon\": h,\n",
    "        \"min_train_n\": min_train_n,\n",
    "        \"winsor_level\": winsor_level,\n",
    "        \"norm_var\": norm_var,\n",
    "        \"features\": features,\n",
    "        \"eval_window\": (str(eval_start.date()), str(eval_end.date())),\n",
    "        \"test_window\": (str(test_start.date()), str(test_end.date())),\n",
    "        \"refit_every_months\": refit_every_months,\n",
    "        \"retune_every_months\": retune_every_months,\n",
    "        \"best_params_last\": last_best_params\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"trained_until\": str(_to_month_start(last_refit_t)) if last_refit_t is not None else None,\n",
    "        \"last_tune_time\": str(_to_month_start(last_tune_t)) if last_tune_t is not None else None,\n",
    "        \"index_freq\": \"MS\",\n",
    "        \"n_obs_all\": int(len(df_all)),\n",
    "        \"n_forecasts\": int(len(df_oos)),\n",
    "        \"cv_mae_history\": cv_mae_history,\n",
    "        \"best_params_history\": best_params_hist\n",
    "    },\n",
    "    \"train_fit_dates\": pd.to_datetime(pd.Index(train_ends))\n",
    "}\n",
    "with open(LGBM_BUNDLE, \"wb\") as f:\n",
    "    pickle.dump(bundle, f)\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"model\": \"LightGBM+Bagging\",\n",
    "    \"horizon\": h,\n",
    "    \"min_train_n\": min_train_n,\n",
    "    \"winsor_level\": winsor_level,\n",
    "    \"norm_var\": norm_var,\n",
    "    \"refit_every_months\": refit_every_months,\n",
    "    \"retune_every_months\": retune_every_months,\n",
    "    \"n_boot\": n_boot,\n",
    "    \"bootstrap_proportion\": bootstrap_proportion,\n",
    "    \"trained_until\": bundle[\"meta\"][\"trained_until\"],\n",
    "    \"n_forecasts\": bundle[\"meta\"][\"n_forecasts\"]\n",
    "}]).to_csv(LGBM_META, index=False)\n",
    "\n",
    "# Artefact dernier modÃ¨le (ensemble bootstrap)\n",
    "lgbm_artifact = {\n",
    "    \"trained_until\": str(_to_month_start(last_fit_end)) if last_fit_end is not None else None,\n",
    "    \"horizon\": h,\n",
    "    \"features\": features,\n",
    "    \"n_boot\": n_boot,\n",
    "    \"best_params\": last_best_params,\n",
    "    \"models\": last_boot_models  # liste de {prep, params}\n",
    "}\n",
    "with open(LGBM_LAST_PKL, \"wb\") as f:\n",
    "    pickle.dump(lgbm_artifact, f)\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"trained_until\": lgbm_artifact[\"trained_until\"],\n",
    "    \"n_features\": len(features),\n",
    "    \"horizon\": h,\n",
    "    \"n_boot\": n_boot\n",
    "}]).to_csv(LGBM_LAST_META, index=False)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Bundle OOS sauvegardÃ© â†’ {LGBM_BUNDLE}\")\n",
    "print(f\"ðŸ’¾ MÃ©ta bundle       â†’ {LGBM_META}\")\n",
    "print(f\"ðŸ’¾ Dernier modÃ¨le    â†’ {LGBM_LAST_PKL}\")\n",
    "print(f\"ðŸ’¾ MÃ©ta dernier fit  â†’ {LGBM_LAST_META}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e53a2f",
   "metadata": {},
   "source": [
    "# VECM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8352d81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DonnÃ©es prÃªtes : 1959-01-01 â†’ 2025-08-01 | n=800 | freq=MS\n",
      "Variables (11): ['UNRATE', 'TB3MS', 'RPI', 'INDPRO', 'DPCERA3M086SBEA', 'S&P 500', 'BUSLOANS', 'CPIAUCSL'] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\vector_ar\\vecm.py:1710: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  A[0] = pi + np.identity(K)\n",
      "c:\\Users\\Mita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\vector_ar\\vecm.py:1710: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  A[0] = pi + np.identity(K)\n",
      "c:\\Users\\Mita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\vector_ar\\vecm.py:1710: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  A[0] = pi + np.identity(K)\n",
      "c:\\Users\\Mita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\vector_ar\\vecm.py:1710: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  A[0] = pi + np.identity(K)\n",
      "c:\\Users\\Mita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\vector_ar\\vecm.py:1710: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  A[0] = pi + np.identity(K)\n",
      "c:\\Users\\Mita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\vector_ar\\vecm.py:1710: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  A[0] = pi + np.identity(K)\n",
      "c:\\Users\\Mita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\vector_ar\\vecm.py:1710: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  A[0] = pi + np.identity(K)\n",
      "c:\\Users\\Mita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\vector_ar\\vecm.py:1710: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  A[0] = pi + np.identity(K)\n",
      "c:\\Users\\Mita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\vector_ar\\vecm.py:1710: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  A[0] = pi + np.identity(K)\n",
      "c:\\Users\\Mita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\vector_ar\\vecm.py:1710: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  A[0] = pi + np.identity(K)\n",
      "c:\\Users\\Mita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\vector_ar\\vecm.py:1710: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  A[0] = pi + np.identity(K)\n",
      "c:\\Users\\Mita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\vector_ar\\vecm.py:1710: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  A[0] = pi + np.identity(K)\n",
      "c:\\Users\\Mita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\vector_ar\\vecm.py:1710: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  A[0] = pi + np.identity(K)\n",
      "c:\\Users\\Mita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\vector_ar\\vecm.py:1710: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  A[0] = pi + np.identity(K)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Pseudo-OOS VECM (h=12, tuning=MAE) â€” n prÃ©visions = 729\n",
      "              y_pred  y_true\n",
      "date                        \n",
      "1964-12-01  5.679635     5.0\n",
      "1965-01-01  5.679635     4.9\n",
      "1965-02-01  5.679635     5.1\n",
      "\n",
      "ðŸ“Š Validation 83â€“89 â€” n=84 | MAE=0.872 | RMSE=1.180 | RÂ²=0.290\n",
      "ðŸ“Š Test 90â€“2025 â€” n=428 | MAE=0.947 | RMSE=1.483 | RÂ²=0.276\n",
      "\n",
      "ðŸ’¾ Bundle OOS sauvegardÃ© â†’ vecm_oos.pkl\n",
      "ðŸ’¾ MÃ©ta bundle       â†’ vecm_oos_meta.csv\n",
      "ðŸ’¾ Dernier modÃ¨le    â†’ VECM_last_trained_model.pkl\n",
      "ðŸ’¾ MÃ©ta dernier fit  â†’ VECM_last_trained_model_meta.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# VECM â€” Pseudo-OOS (h=12), sÃ©lection hyperparams par MAE\n",
    "# - Expanding, refit annuel, retune tous les 36 mois (â‰¥ 1983)\n",
    "# - Hyperparams = (k_ar_diff, coint_rank, deterministic) optimisÃ©s\n",
    "#   par MAE Ã  l'horizon h via rolling-origin sur l'Ã©chantillon train\n",
    "# - Winsorisation optionnelle (conserve les niveaux/cointÃ©grations)\n",
    "# - Sauvegardes: bundle OOS + artefact \"dernier modÃ¨le\"\n",
    "# Requis: df (niveaux, freq=MS) avec la colonne cible 'UNRATE'\n",
    "# =========================================================\n",
    "import os, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from statsmodels.tsa.vector_ar.vecm import VECM\n",
    "\n",
    "# ---------- Fichiers ----------\n",
    "VECM_BUNDLE     = \"vecm_oos.pkl\"\n",
    "VECM_META       = \"vecm_oos_meta.csv\"\n",
    "VECM_LAST_PKL   = \"VECM_last_trained_model.pkl\"\n",
    "VECM_LAST_META  = \"VECM_last_trained_model_meta.csv\"\n",
    "\n",
    "# ---------- ParamÃ¨tres globaux ----------\n",
    "target_col = \"UNRATE\"\n",
    "h = 12\n",
    "min_train_n = 60                 # â‰¥5 ans conseillÃ© pour VECM\n",
    "winsor_level = 0.01              # 0.0 pour dÃ©sactiver\n",
    "seed0 = 12345\n",
    "\n",
    "# FenÃªtres\n",
    "eval_start = pd.Timestamp(\"1983-01-01\")\n",
    "eval_end   = pd.Timestamp(\"1989-12-31\")\n",
    "test_start = pd.Timestamp(\"1990-01-01\")\n",
    "test_end   = pd.Timestamp(\"2025-12-31\")\n",
    "\n",
    "# Cadences\n",
    "refit_every_months   = 12\n",
    "retune_every_months  = 36\n",
    "\n",
    "# Recherche hyperparams (bornes)\n",
    "P_MAX   = 12                         # k_ar_diff âˆˆ {1..P_MAX}\n",
    "R_MAX   = None                      # rank âˆˆ {0..min(R_MAX, n_vars-1)} ; None => n_vars-1\n",
    "DET_SET = [\"ci\", \"co\"]              # constantes dans/sur la relation de cointÃ©gration\n",
    "\n",
    "# Rolling-origin pour le tuning MAE (contrÃ´le coÃ»t)\n",
    "ORIGIN_MIN_YEARS = 5                # taille min de la fenÃªtre d'apprentissage interne (annÃ©es)\n",
    "ORIGIN_STEP      = 6                # mois entre deux origines (rÃ©duit le coÃ»t)\n",
    "MAX_FAILS_PER_COMBO = 8             # tolÃ©rance Ã©checs par combo\n",
    "\n",
    "# ---------- Utils ----------\n",
    "def _ensure_ms_index(df_):\n",
    "    df_ = df_.copy()\n",
    "    if \"date\" in df_.columns:\n",
    "        df_ = df_.set_index(\"date\")\n",
    "    idx = pd.to_datetime(df_.index)\n",
    "    df_.index = idx.to_period(\"M\").to_timestamp(how=\"start\")\n",
    "    return df_.asfreq(\"MS\")\n",
    "\n",
    "def _winsorize_df(X, p=0.01):\n",
    "    if p is None or p <= 0.0:\n",
    "        return X.copy()\n",
    "    lower = X.quantile(p)\n",
    "    upper = X.quantile(1-p)\n",
    "    return X.clip(lower=lower, upper=upper, axis=1)\n",
    "\n",
    "def _to_month_start(ts):\n",
    "    return pd.Timestamp(ts).to_period(\"M\").to_timestamp(how=\"start\") if ts is not None else None\n",
    "\n",
    "def _scores(df):\n",
    "    if len(df) == 0:\n",
    "        return {\"MAE\": np.nan, \"RMSE\": np.nan, \"R2\": np.nan}\n",
    "    mae  = mean_absolute_error(df[\"y_true\"], df[\"y_pred\"])\n",
    "    rmse = np.sqrt(mean_squared_error(df[\"y_true\"], df[\"y_pred\"]))\n",
    "    ssr  = np.sum((df[\"y_true\"]-df[\"y_pred\"])**2)\n",
    "    sst  = np.sum((df[\"y_true\"]-df[\"y_true\"].mean())**2)\n",
    "    r2   = 1 - ssr/sst if sst>0 else np.nan\n",
    "    return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2)}\n",
    "\n",
    "def _extract_vecm_params(res):\n",
    "    k_ar_diff = getattr(res, \"k_ar_diff\", None)\n",
    "    rank      = getattr(res, \"rank\", None)\n",
    "    det       = None\n",
    "    if k_ar_diff is None and hasattr(res, \"model\"):\n",
    "        k_ar_diff = getattr(res.model, \"k_ar_diff\", None)\n",
    "    if rank is None and hasattr(res, \"model\"):\n",
    "        rank = getattr(res.model, \"rank\", None)\n",
    "    if hasattr(res, \"model\") and hasattr(res.model, \"deterministic\"):\n",
    "        det = res.model.deterministic\n",
    "    return {\n",
    "        \"k_ar_diff\": None if k_ar_diff is None else int(k_ar_diff),\n",
    "        \"coint_rank\": None if rank is None else int(rank),\n",
    "        \"deterministic\": det\n",
    "    }\n",
    "\n",
    "def _predict_h_steps(res, endog_train, steps=12):\n",
    "    # Essayez predict(steps=h) (statsmodels rÃ©cents)\n",
    "    try:\n",
    "        path = res.predict(steps=steps)  # (h, n_vars)\n",
    "        return path\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Fallback: forecast avec derniers Ã©tats\n",
    "    y = np.asarray(endog_train, float)\n",
    "    k_ar = getattr(res, \"k_ar\", None)\n",
    "    if k_ar is None and hasattr(res, \"model\"):\n",
    "        k_ar = getattr(res.model, \"k_ar\", None)\n",
    "    if k_ar is None:\n",
    "        k_ar = min(len(y), steps)\n",
    "    y_last = y[-k_ar:]\n",
    "    try:\n",
    "        return res.forecast(y_last, steps=steps)\n",
    "    except Exception:\n",
    "        return np.repeat(y_last[-1][None, :], steps, axis=0)\n",
    "\n",
    "def _rolling_mae_for_combo(endog, p, r, det, h, min_win, step=6, target_ix=0):\n",
    "    \"\"\"\n",
    "    MAE rolling-origin pour un triplet (p,r,deterministic).\n",
    "    endog: DataFrame niveaux (winsorisÃ©), colonnes: [target, ...]\n",
    "    On crÃ©e des origines t0, fit VECM(endog[:t0]), prÃ©dit h, compare Ã  endog[t0+h, target].\n",
    "    \"\"\"\n",
    "    n = len(endog)\n",
    "    if n < (min_win + h + 1):\n",
    "        return np.inf, 0\n",
    "\n",
    "    # indices des origines\n",
    "    origins = []\n",
    "    t0 = min_win\n",
    "    while (t0 + h) < n:\n",
    "        origins.append(t0)\n",
    "        t0 += step\n",
    "    if len(origins) == 0:\n",
    "        return np.inf, 0\n",
    "\n",
    "    err = []\n",
    "    fails = 0\n",
    "    for t0 in origins:\n",
    "        try:\n",
    "            tr = endog.iloc[:t0]\n",
    "            model = VECM(tr, k_ar_diff=p, coint_rank=r, deterministic=det)\n",
    "            res = model.fit()\n",
    "            path = _predict_h_steps(res, tr, steps=h)\n",
    "            yhat_h = float(path[-1, target_ix])  # cible = premiÃ¨re colonne\n",
    "            y_true = float(endog.iloc[t0 + h, target_ix])\n",
    "            err.append(abs(y_true - yhat_h))\n",
    "        except Exception:\n",
    "            fails += 1\n",
    "            if fails > MAX_FAILS_PER_COMBO:\n",
    "                return np.inf, 0\n",
    "            continue\n",
    "\n",
    "    if len(err) == 0:\n",
    "        return np.inf, 0\n",
    "    return float(np.mean(err)), len(err)\n",
    "\n",
    "def tune_vecm_by_mae(endog_tr, h, p_max=6, r_max=None, det_set=None,\n",
    "                     origin_min_years=5, origin_step=6, target_ix=0):\n",
    "    \"\"\"\n",
    "    SÃ©lectionne (p, r, det) minimisant la MAE(h) via rolling-origin dans endog_tr.\n",
    "    \"\"\"\n",
    "    if det_set is None:\n",
    "        det_set = [\"ci\", \"co\"]\n",
    "    n_vars = endog_tr.shape[1]\n",
    "    r_max_eff = (n_vars - 1) if r_max is None else min(r_max, n_vars - 1)\n",
    "    r_max_eff = max(0, r_max_eff)\n",
    "\n",
    "    min_win = max(int(origin_min_years * 12), 36)  # min 3 ans absolu\n",
    "    best = {\"mae\": np.inf, \"n_eval\": 0, \"p\": None, \"r\": None, \"det\": None}\n",
    "\n",
    "    for det in det_set:\n",
    "        for p in range(1, p_max + 1):\n",
    "            for r in range(0, r_max_eff + 1):\n",
    "                mae, n_eval = _rolling_mae_for_combo(\n",
    "                    endog=endog_tr,\n",
    "                    p=p, r=r, det=det,\n",
    "                    h=h, min_win=min_win, step=origin_step,\n",
    "                    target_ix=target_ix\n",
    "                )\n",
    "                if np.isfinite(mae) and (mae < best[\"mae\"]):\n",
    "                    best.update({\"mae\": mae, \"n_eval\": n_eval, \"p\": p, \"r\": r, \"det\": det})\n",
    "    # garde-fou\n",
    "    if best[\"p\"] is None:\n",
    "        best = {\"mae\": np.inf, \"n_eval\": 0, \"p\": 1, \"r\": min(1, r_max_eff), \"det\": det_set[0]}\n",
    "    return best\n",
    "\n",
    "# ---------- DonnÃ©es ----------\n",
    "# On part de \"df\" dÃ©jÃ  en mÃ©moire\n",
    "df_all = _ensure_ms_index(df).sort_index()\n",
    "\n",
    "if target_col not in df_all.columns:\n",
    "    raise ValueError(f\"La colonne cible '{target_col}' est absente de df.\")\n",
    "\n",
    "# mettre la cible en 1re colonne\n",
    "cols = [target_col] + [c for c in df_all.columns if c != target_col]\n",
    "df_all = df_all[cols].astype(float)\n",
    "\n",
    "print(f\"âœ… DonnÃ©es prÃªtes : {df_all.index.min().date()} â†’ {df_all.index.max().date()} | n={len(df_all)} | freq=MS\")\n",
    "print(f\"Variables ({len(df_all.columns)}): {df_all.columns.tolist()[:8]}{' ...' if df_all.shape[1]>8 else ''}\")\n",
    "\n",
    "# ---------- Boucle pseudo-OOS ----------\n",
    "rows = []\n",
    "train_ends = []\n",
    "hp_history = []\n",
    "\n",
    "last_t_end = df_all.index.max() - relativedelta(months=h)\n",
    "last_refit_t = None\n",
    "last_tune_t  = None\n",
    "\n",
    "last_fitted_res = None\n",
    "last_fit_end    = None\n",
    "\n",
    "for t_end in df_all.index:\n",
    "    if t_end > last_t_end:\n",
    "        break\n",
    "\n",
    "    endog_tr_raw = df_all.loc[:t_end]\n",
    "    n_tr = len(endog_tr_raw)\n",
    "    if n_tr < min_train_n:\n",
    "        continue\n",
    "\n",
    "    # refit / retune cadence\n",
    "    need_refit = (last_refit_t is None) or (\n",
    "        (t_end.year - (last_refit_t.year if last_refit_t else t_end.year))*12\n",
    "        + (t_end.month - (last_refit_t.month if last_refit_t else t_end.month)) >= refit_every_months\n",
    "    )\n",
    "    need_tune = False\n",
    "    if t_end >= eval_start:\n",
    "        if last_tune_t is None:\n",
    "            need_tune = True\n",
    "        else:\n",
    "            months_since_tune = (t_end.year - last_tune_t.year)*12 + (t_end.month - last_tune_t.month)\n",
    "            need_tune = months_since_tune >= retune_every_months\n",
    "\n",
    "    # winsor lÃ©ger\n",
    "    endog_tr = _winsorize_df(endog_tr_raw, p=winsor_level)\n",
    "\n",
    "    if need_refit:\n",
    "        # --------- Tuning MAE(h) ----------\n",
    "        if need_tune:\n",
    "            best = tune_vecm_by_mae(\n",
    "                endog_tr=endog_tr, h=h,\n",
    "                p_max=P_MAX, r_max=R_MAX, det_set=DET_SET,\n",
    "                origin_min_years=ORIGIN_MIN_YEARS, origin_step=ORIGIN_STEP,\n",
    "                target_ix=0\n",
    "            )\n",
    "            hp_history.append({\n",
    "                \"t_end\": str(t_end.date()),\n",
    "                \"k_ar_diff\": int(best[\"p\"]),\n",
    "                \"coint_rank\": int(best[\"r\"]),\n",
    "                \"deterministic\": best[\"det\"],\n",
    "                \"mae_cv_like\": float(best[\"mae\"]),\n",
    "                \"n_eval_points\": int(best[\"n_eval\"])\n",
    "            })\n",
    "            last_tune_t = t_end\n",
    "            k_best, r_best, det_best = best[\"p\"], best[\"r\"], best[\"det\"]\n",
    "        else:\n",
    "            # s'il n'y a jamais eu de tuning auparavant, on en fait un ici\n",
    "            if len(hp_history) == 0:\n",
    "                best = tune_vecm_by_mae(\n",
    "                    endog_tr=endog_tr, h=h,\n",
    "                    p_max=P_MAX, r_max=R_MAX, det_set=DET_SET,\n",
    "                    origin_min_years=ORIGIN_MIN_YEARS, origin_step=ORIGIN_STEP,\n",
    "                    target_ix=0\n",
    "                )\n",
    "                hp_history.append({\n",
    "                    \"t_end\": str(t_end.date()),\n",
    "                    \"k_ar_diff\": int(best[\"p\"]),\n",
    "                    \"coint_rank\": int(best[\"r\"]),\n",
    "                    \"deterministic\": best[\"det\"],\n",
    "                    \"mae_cv_like\": float(best[\"mae\"]),\n",
    "                    \"n_eval_points\": int(best[\"n_eval\"])\n",
    "                })\n",
    "                last_tune_t = t_end\n",
    "                k_best, r_best, det_best = best[\"p\"], best[\"r\"], best[\"det\"]\n",
    "            else:\n",
    "                # rÃ©utiliser les derniers hyperparams\n",
    "                last_hp = hp_history[-1]\n",
    "                k_best, r_best, det_best = int(last_hp[\"k_ar_diff\"]), int(last_hp[\"coint_rank\"]), last_hp[\"deterministic\"]\n",
    "\n",
    "        # --------- Fit final ----------\n",
    "        model = VECM(endog_tr, k_ar_diff=k_best, coint_rank=r_best, deterministic=det_best)\n",
    "        res   = model.fit()\n",
    "\n",
    "        last_fitted_res = res\n",
    "        last_fit_end    = t_end\n",
    "        last_refit_t    = t_end\n",
    "        train_ends.append(t_end)\n",
    "\n",
    "    # PrÃ©vision h=12 si modÃ¨le dispo\n",
    "    if last_fitted_res is None:\n",
    "        continue\n",
    "\n",
    "    t_fore = t_end + relativedelta(months=h)\n",
    "    if t_fore in df_all.index:\n",
    "        path = _predict_h_steps(last_fitted_res, df_all.loc[:t_end], steps=h)\n",
    "        yhat_h = float(path[-1, 0])  # cible = 1re colonne\n",
    "        y_true = float(df_all.loc[t_fore, target_col])\n",
    "        rows.append((t_fore, yhat_h, y_true))\n",
    "\n",
    "# ---------- DataFrame OOS ----------\n",
    "if rows:\n",
    "    df_oos = (pd.DataFrame(rows, columns=[\"date\",\"y_pred\",\"y_true\"])\n",
    "              .assign(date=lambda d: pd.to_datetime(d[\"date\"]).dt.to_period(\"M\").dt.to_timestamp(how=\"start\"))\n",
    "              .set_index(\"date\").sort_index())\n",
    "else:\n",
    "    df_oos = pd.DataFrame(columns=[\"y_pred\",\"y_true\"])\n",
    "    df_oos.index = pd.to_datetime(pd.Index([]))\n",
    "\n",
    "print(f\"\\nâœ… Pseudo-OOS VECM (h={h}, tuning=MAE) â€” n prÃ©visions = {len(df_oos)}\")\n",
    "print(df_oos.head(3))\n",
    "\n",
    "# ---------- Scores ----------\n",
    "df_val  = df_oos.loc[eval_start:eval_end].copy()\n",
    "df_test = df_oos.loc[test_start:test_end].copy()\n",
    "\n",
    "sc_val  = _scores(df_val)\n",
    "sc_test = _scores(df_test)\n",
    "\n",
    "print(f\"\\nðŸ“Š Validation 83â€“89 â€” n={len(df_val)} | MAE={sc_val['MAE']:.3f} | RMSE={sc_val['RMSE']:.3f} | RÂ²={sc_val['R2']:.3f}\")\n",
    "print(f\"ðŸ“Š Test 90â€“{test_end.year} â€” n={len(df_test)} | MAE={sc_test['MAE']:.3f} | RMSE={sc_test['RMSE']:.3f} | RÂ²={sc_test['R2']:.3f}\")\n",
    "\n",
    "# ---------- Sauvegardes ----------\n",
    "bundle = {\n",
    "    \"oos_predictions\": df_oos.reset_index(),\n",
    "    \"params\": {\n",
    "        \"model\": \"VECM (MAE-tuned)\",\n",
    "        \"horizon\": h,\n",
    "        \"min_train_n\": min_train_n,\n",
    "        \"winsor_level\": winsor_level,\n",
    "        \"refit_every_months\": refit_every_months,\n",
    "        \"retune_every_months\": retune_every_months,\n",
    "        \"p_max\": P_MAX,\n",
    "        \"rank_max\": R_MAX,\n",
    "        \"det_set\": DET_SET,\n",
    "        \"origin_min_years\": ORIGIN_MIN_YEARS,\n",
    "        \"origin_step_months\": ORIGIN_STEP,\n",
    "        \"features\": df_all.columns.tolist()\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"trained_until\": str(_to_month_start(last_refit_t)) if last_refit_t is not None else None,\n",
    "        \"last_tune_time\": str(_to_month_start(last_tune_t)) if last_tune_t is not None else None,\n",
    "        \"index_freq\": \"MS\",\n",
    "        \"n_obs_all\": int(len(df_all)),\n",
    "        \"n_forecasts\": int(len(df_oos)),\n",
    "        \"hp_history\": hp_history\n",
    "    },\n",
    "    \"train_fit_dates\": pd.to_datetime(pd.Index(train_ends))\n",
    "}\n",
    "with open(VECM_BUNDLE, \"wb\") as f:\n",
    "    pickle.dump(bundle, f)\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"model\": \"VECM (MAE-tuned)\",\n",
    "    \"horizon\": h,\n",
    "    \"min_train_n\": min_train_n,\n",
    "    \"winsor_level\": winsor_level,\n",
    "    \"refit_every_months\": refit_every_months,\n",
    "    \"retune_every_months\": retune_every_months,\n",
    "    \"p_max\": P_MAX,\n",
    "    \"rank_max\": R_MAX,\n",
    "    \"det_set\": \",\".join(DET_SET),\n",
    "    \"origin_min_years\": ORIGIN_MIN_YEARS,\n",
    "    \"origin_step_months\": ORIGIN_STEP,\n",
    "    \"trained_until\": bundle[\"meta\"][\"trained_until\"],\n",
    "    \"n_forecasts\": bundle[\"meta\"][\"n_forecasts\"]\n",
    "}]).to_csv(VECM_META, index=False)\n",
    "\n",
    "# ---------- Artefact \"dernier modÃ¨le\" ----------\n",
    "if last_fitted_res is not None:\n",
    "    last_params_extracted = _extract_vecm_params(last_fitted_res)\n",
    "else:\n",
    "    last_params_extracted = None\n",
    "\n",
    "vecm_artifact = {\n",
    "    \"trained_until\": str(_to_month_start(last_fit_end)) if last_fit_end is not None else None,\n",
    "    \"horizon\": h,\n",
    "    \"features\": df_all.columns.tolist(),\n",
    "    \"last_params\": last_params_extracted\n",
    "}\n",
    "with open(VECM_LAST_PKL, \"wb\") as f:\n",
    "    pickle.dump(vecm_artifact, f)\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"trained_until\": vecm_artifact[\"trained_until\"],\n",
    "    \"n_features\": len(df_all.columns),\n",
    "    \"horizon\": h,\n",
    "    **(last_params_extracted if last_params_extracted is not None else {})\n",
    "}]).to_csv(VECM_LAST_META, index=False)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Bundle OOS sauvegardÃ© â†’ {VECM_BUNDLE}\")\n",
    "print(f\"ðŸ’¾ MÃ©ta bundle       â†’ {VECM_META}\")\n",
    "print(f\"ðŸ’¾ Dernier modÃ¨le    â†’ {VECM_LAST_PKL}\")\n",
    "print(f\"ðŸ’¾ MÃ©ta dernier fit  â†’ {VECM_LAST_META}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
