{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2478ba5b",
   "metadata": {},
   "source": [
    "# Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "41027a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import shap\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.utils import shuffle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22973a6a",
   "metadata": {},
   "source": [
    "# Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dedcdda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les donnÃ©es\n",
    "df_stationary_test = pd.read_csv(\"df_stationary_test.csv\", index_col=\"date\")\n",
    "df_stationary_test.index = pd.to_datetime(df_stationary_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "38404298",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stationary_train = pd.read_csv(\"df_stationary_train.csv\", index_col=\"date\")\n",
    "df_stationary_train.index = pd.to_datetime(df_stationary_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c1c02",
   "metadata": {},
   "source": [
    "# 1) ParamÃ¨tres & sous-ensemble TRAIN global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "090941b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "081d728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- ParamÃ¨tres ----------\n",
    "window_len = 12      # longueur minimale de la fenÃªtre (non utilisÃ© si expanding)\n",
    "step_size = 12       # refit annuel (adapter : 1 = mensuel, 12 = annuel)\n",
    "winsor_level = 0.01\n",
    "# === Bloc 1bis â€” ParamÃ¨tre de dÃ©calage (12 mois) ===\n",
    "SHIFT_LAG = 12  # dÃ©calage des features : X_{t-12} -> y_t\n",
    "h = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "afc97582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN global\n",
    "df_train_global = df_stationary_train.sort_index().copy()\n",
    "\n",
    "# âœ… Inclure toutes les features (aucune exclusion sauf la cible)\n",
    "features = [c for c in df_stationary_train.columns if c != \"UNRATE\"]\n",
    "cols_tx = features.copy()\n",
    "\n",
    "# ---------- Bornes de validation ----------\n",
    "valid_start = pd.Timestamp(\"1983-01-01\")\n",
    "valid_end   = pd.Timestamp(\"1989-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "29b6e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Bloc 3bis â€” PrÃ©-calcul des features dÃ©calÃ©es ===\n",
    "# (Optionnel mais plus propre/rapide que de refaire .shift() Ã  chaque itÃ©ration)\n",
    "df_tx_shifted = df_train_global[cols_tx].shift(SHIFT_LAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e3e66c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Bloc 2 â€” Helpers de prÃ©traitement (USREC exemptÃ©e), + mÃ©triques ===\n",
    "\n",
    "def fit_preproc(X, winsor_level=0.01, do_norm=True, exempt_cols=None):\n",
    "    \"\"\"\n",
    "    Apprend les bornes de winsorisation et les stats de normalisation sur X,\n",
    "    mais n'applique rien aux colonnes 'exempt_cols' (ex: ['USREC']).\n",
    "    \"\"\"\n",
    "    exempt_cols = list(exempt_cols or [])\n",
    "    cols_w = [c for c in X.columns if c not in exempt_cols]\n",
    "\n",
    "    Xw = X.copy()\n",
    "\n",
    "    # Winsorisation uniquement sur les colonnes non-exemptÃ©es\n",
    "    if len(cols_w):\n",
    "        lower_wins = X[cols_w].quantile(winsor_level)\n",
    "        upper_wins = X[cols_w].quantile(1 - winsor_level)\n",
    "        Xw[cols_w] = Xw[cols_w].clip(lower=lower_wins, upper=upper_wins, axis=1)\n",
    "    else:\n",
    "        lower_wins = pd.Series(dtype=float)\n",
    "        upper_wins = pd.Series(dtype=float)\n",
    "\n",
    "    # Normalisation uniquement sur les colonnes non-exemptÃ©es\n",
    "    if do_norm and len(cols_w):\n",
    "        mean = Xw[cols_w].mean()\n",
    "        std  = Xw[cols_w].std().replace(0, 1)\n",
    "        Xn = Xw.copy()\n",
    "        Xn[cols_w] = (Xw[cols_w] - mean) / std\n",
    "    else:\n",
    "        mean, std = None, None\n",
    "        Xn = Xw\n",
    "\n",
    "    return Xn, {\n",
    "        \"lower_wins\": lower_wins,\n",
    "        \"upper_wins\": upper_wins,\n",
    "        \"mean\": mean,\n",
    "        \"std\": std,\n",
    "        \"norm\": do_norm,\n",
    "        \"exempt_cols\": exempt_cols\n",
    "    }\n",
    "\n",
    "\n",
    "def apply_preproc(X, prep):\n",
    "    \"\"\"\n",
    "    Applique les bornes/stats apprises sur les colonnes non-exemptÃ©es.\n",
    "    Les colonnes exemptÃ©es (ex: USREC) restent brutes.\n",
    "    \"\"\"\n",
    "    exempt_cols = list(prep.get(\"exempt_cols\", []))\n",
    "    cols_w = [c for c in X.columns if c not in exempt_cols]\n",
    "\n",
    "    Xp = X.copy()\n",
    "    if len(cols_w):\n",
    "        # Appliquer les mÃªmes bornes/paramÃ¨tres sur les colonnes non-exemptÃ©es\n",
    "        lw = prep[\"lower_wins\"].reindex(cols_w)\n",
    "        uw = prep[\"upper_wins\"].reindex(cols_w)\n",
    "        Xp[cols_w] = Xp[cols_w].clip(lower=lw, upper=uw, axis=1)\n",
    "\n",
    "        if prep[\"norm\"]:\n",
    "            mean = prep[\"mean\"].reindex(cols_w)\n",
    "            std  = prep[\"std\"].reindex(cols_w).replace(0, 1)\n",
    "            Xp[cols_w] = (Xp[cols_w] - mean) / std\n",
    "\n",
    "    # Les colonnes exemptÃ©es (ex. USREC) restent brutes\n",
    "    return Xp\n",
    "\n",
    "\n",
    "def perf_report(y_true, y_pred):\n",
    "    # RMSE = racine du MSE manuellement pour compatibilitÃ© avec toutes les versions\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    return {\"RMSE\": float(rmse), \"MAE\": float(mae), \"R2\": float(r2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2964722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Bloc 3bis â€” PrÃ©-calcul des features dÃ©calÃ©es ===\n",
    "# (Optionnel mais plus propre/rapide que de refaire .shift() Ã  chaque itÃ©ration)\n",
    "df_tx_shifted = df_train_global[cols_tx].shift(SHIFT_LAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0a476149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00] Skip (pas assez d'obs aprÃ¨s dÃ©calage).\n",
      "[01] Fin 1961-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 12 obs.\n",
      "[02] Fin 1962-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 24 obs.\n",
      "[03] Fin 1963-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 36 obs.\n",
      "[04] Fin 1964-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 48 obs.\n",
      "[05] Fin 1965-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 60 obs.\n",
      "[06] Fin 1966-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 72 obs.\n",
      "[07] Fin 1967-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 84 obs.\n",
      "[08] Fin 1968-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 96 obs.\n",
      "[09] Fin 1969-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 108 obs.\n",
      "[10] Fin 1970-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 120 obs.\n",
      "[11] Fin 1971-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 132 obs.\n",
      "[12] Fin 1972-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 144 obs.\n",
      "[13] Fin 1973-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 156 obs.\n",
      "[14] Fin 1974-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 168 obs.\n",
      "[15] Fin 1975-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 180 obs.\n",
      "[16] Fin 1976-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 192 obs.\n",
      "[17] Fin 1977-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 204 obs.\n",
      "[18] Fin 1978-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 216 obs.\n",
      "[19] Fin 1979-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 228 obs.\n",
      "[20] Fin 1980-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 240 obs.\n",
      "[21] Fin 1981-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 252 obs.\n",
      "[22] Fin 1982-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 264 obs.\n",
      "[23] Fin 1983-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 276 obs.\n",
      "[24] Fin 1984-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 288 obs.\n",
      "[25] Fin 1985-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 300 obs.\n",
      "[26] Fin 1986-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 312 obs.\n",
      "[27] Fin 1987-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 324 obs.\n",
      "[28] Fin 1988-12 â€” shift=12m â€” winsor(1.00%) + norm â€” 336 obs.\n"
     ]
    }
   ],
   "source": [
    "# === Bloc 3 â€” Boucle d'entraÃ®nement (expanding) avec prÃ©dicteurs dÃ©calÃ©s de 12 mois ===\n",
    "\n",
    "# ---------- Containers ----------\n",
    "models = []\n",
    "coefs = []\n",
    "preprocs = []\n",
    "train_periods = []\n",
    "preds = []\n",
    "norm_var = True  # True = normalisation des colonnes non-exemptÃ©es (USREC reste brute)\n",
    "\n",
    "# ---------- Boucle EXPANDING WINDOW SUR LE TRAIN ----------\n",
    "for t, end in enumerate(range(step_size, len(df_train_global), step_size)):\n",
    "    df_train_local = df_train_global.iloc[:end].copy()\n",
    "\n",
    "    # 0) Construire X_train dÃ©calÃ© (X_{t-12}) et aligner y_t\n",
    "    X_train_raw = df_tx_shifted.loc[df_train_local.index].copy()   # prÃ©dicteurs dÃ©calÃ©s\n",
    "    y_train_full = df_train_local[\"UNRATE\"].copy()\n",
    "\n",
    "    # supprimer les premiÃ¨res lignes qui deviennent NaN Ã  cause du dÃ©calage\n",
    "    mask_valid = ~X_train_raw.isna().any(axis=1)\n",
    "    X_train_raw = X_train_raw.loc[mask_valid]\n",
    "    y_train = y_train_full.loc[X_train_raw.index]\n",
    "\n",
    "    # guardrail : si pas assez d'observations, on saute cette itÃ©ration\n",
    "    if len(X_train_raw) < 2:\n",
    "        print(f\"[{t:02d}] Skip (pas assez d'obs aprÃ¨s dÃ©calage).\")\n",
    "        continue\n",
    "\n",
    "    # 1ï¸âƒ£ Winsorisation/normalisation apprises sur TRAIN â€” USREC exclue\n",
    "    X_train, prep = fit_preproc(\n",
    "        X_train_raw,\n",
    "        winsor_level=winsor_level,\n",
    "        do_norm=norm_var,\n",
    "        exempt_cols=[\"USREC\"]  # â¬…ï¸ USREC pas de winsor ni normalisation\n",
    "    )\n",
    "\n",
    "    # 2ï¸âƒ£ EntraÃ®nement OLS\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 3ï¸âƒ£ FenÃªtre de prÃ©diction (annÃ©e suivante) + intersection 1983â€“1989\n",
    "    start_pred = (df_train_local.index[-1] + relativedelta(months=1)).replace(day=1)\n",
    "    end_pred   = start_pred + relativedelta(months=12, days=-1)\n",
    "\n",
    "    start_use = max(start_pred, valid_start)\n",
    "    end_use   = min(end_pred, valid_end)\n",
    "\n",
    "    if start_use <= end_use:\n",
    "        # X_valid doit aussi Ãªtre dÃ©calÃ© de 12 mois\n",
    "        X_valid_raw = df_tx_shifted.loc[start_use:end_use].copy()\n",
    "        # enlever les Ã©ventuels NaN rÃ©siduels\n",
    "        X_valid_raw = X_valid_raw.dropna(how=\"any\")\n",
    "\n",
    "        if not X_valid_raw.empty:\n",
    "            # y_valid = y_t alignÃ© au mÃªme index\n",
    "            y_valid = df_train_global.loc[X_valid_raw.index, \"UNRATE\"].values\n",
    "\n",
    "            # appliquer le prÃ©traitement appris sur TRAIN\n",
    "            X_valid = apply_preproc(X_valid_raw, prep)\n",
    "\n",
    "            # prÃ©dire\n",
    "            yhat = model.predict(X_valid)\n",
    "\n",
    "            # stocker\n",
    "            df_out = pd.DataFrame({\"y_true\": y_valid, \"y_pred\": yhat}, index=X_valid_raw.index)\n",
    "            df_out[\"model_trained_until\"] = df_train_local.index[-1]\n",
    "            preds.append(df_out)\n",
    "\n",
    "    # 4ï¸âƒ£ Sauvegarde des Ã©lÃ©ments d'entraÃ®nement\n",
    "    models.append(model)\n",
    "    coefs.append(pd.Series(model.coef_, index=cols_tx))\n",
    "    preprocs.append(prep)\n",
    "    train_periods.append(df_train_local.index[-1])\n",
    "\n",
    "    print(f\"[{t:02d}] Fin {df_train_local.index[-1].strftime('%Y-%m')} â€” \"\n",
    "          f\"shift={SHIFT_LAG}m â€” winsor({winsor_level:.2%}) + \"\n",
    "          f\"{'norm' if norm_var else 'no-norm'} â€” {len(X_train)} obs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "32b96255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "078318d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… EntraÃ®nement + validation 1983â€“1989 terminÃ©s (prÃ©dicteurs dÃ©calÃ©s de 12 mois).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mita\\AppData\\Local\\Temp\\ipykernel_19192\\3596606838.py:16: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series(perf_report(g[\"y_true\"], g[\"y_pred\"])))\n"
     ]
    }
   ],
   "source": [
    "# === Bloc 4 â€” (inchangÃ©) AgrÃ©gation OOS + mÃ©triques + sauvegarde d'expÃ©rience ===\n",
    "# (utilise 'preds' rempli ci-dessus ; USREC reste exclue du prÃ©traitement)\n",
    "df_oos = (pd.concat(preds).sort_index() if len(preds) else\n",
    "          pd.DataFrame(columns=[\"y_true\",\"y_pred\"]))\n",
    "\n",
    "m83 = (df_oos.index >= \"1983-01-01\") & (df_oos.index <= \"1983-12-31\")\n",
    "perf_1983 = perf_report(df_oos.loc[m83, \"y_true\"], df_oos.loc[m83, \"y_pred\"]) if m83.any() else None\n",
    "\n",
    "m8389 = (df_oos.index >= valid_start) & (df_oos.index <= valid_end)\n",
    "perf_83_89 = perf_report(df_oos.loc[m8389, \"y_true\"], df_oos.loc[m8389, \"y_pred\"]) if m8389.any() else None\n",
    "\n",
    "if not df_oos.empty:\n",
    "    df_oos[\"year\"] = df_oos.index.year\n",
    "    annual_perf = (df_oos.loc[m8389]\n",
    "                   .groupby(\"year\")\n",
    "                   .apply(lambda g: pd.Series(perf_report(g[\"y_true\"], g[\"y_pred\"])))\n",
    "                   .to_dict(orient=\"index\"))\n",
    "else:\n",
    "    annual_perf = {}\n",
    "\n",
    "exp_results = {\n",
    "    \"models\": models,\n",
    "    \"coefs\": coefs,\n",
    "    \"train_periods\": train_periods,\n",
    "    \"features\": cols_tx,\n",
    "    \"preprocs\": preprocs,\n",
    "    \"oos_predictions\": df_oos,\n",
    "    \"annual_perf\": annual_perf,\n",
    "    \"perf_1983\": perf_1983,\n",
    "    \"perf_83_89\": perf_83_89,\n",
    "    \"params\": {\n",
    "        \"step_size\": step_size,\n",
    "        \"winsor_level\": winsor_level,\n",
    "        \"norm_var\": norm_var,\n",
    "        \"valid_window\": (\"1983-01\", \"1989-12\"),\n",
    "        \"window_len\": window_len,\n",
    "        \"exempt_cols\": [\"USREC\"],\n",
    "        \"shift_lag\": SHIFT_LAG\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nâœ… EntraÃ®nement + validation 1983â€“1989 terminÃ©s (prÃ©dicteurs dÃ©calÃ©s de 12 mois).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "771eb12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fichier 'linear_regression.pkl' sauvegardÃ©.\n",
      "âœ… Fichier 'linear_regression_meta.csv' sauvegardÃ© avec les mÃ©tadonnÃ©es du modÃ¨le linÃ©aire.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Sauvegarde du dictionnaire de rÃ©sultats\n",
    "joblib.dump(exp_results, \"linear_regression.pkl\")\n",
    "print(\"âœ… Fichier 'linear_regression.pkl' sauvegardÃ©.\")\n",
    "\n",
    "# =========================\n",
    "# ðŸ”¹ Sauvegarde des mÃ©tadonnÃ©es du modÃ¨le linÃ©aire\n",
    "# =========================\n",
    "\n",
    "# On extrait les infos clÃ©s pour documenter le modÃ¨le\n",
    "meta_info = {\n",
    "    \"trained_until\": str(exp_results[\"train_periods\"][-1]) if exp_results.get(\"train_periods\") else None,\n",
    "    \"n_models\": len(exp_results.get(\"models\", [])),\n",
    "    \"features\": len(exp_results.get(\"features\", [])),\n",
    "    \"shift_lag\": exp_results[\"params\"].get(\"shift_lag\"),\n",
    "    \"winsor_level\": exp_results[\"params\"].get(\"winsor_level\"),\n",
    "    \"norm_var\": exp_results[\"params\"].get(\"norm_var\"),\n",
    "    \"valid_window_start\": exp_results[\"params\"][\"valid_window\"][0],\n",
    "    \"valid_window_end\": exp_results[\"params\"][\"valid_window\"][1],\n",
    "    \"mae_83_89\": exp_results[\"perf_83_89\"][\"MAE\"] if exp_results.get(\"perf_83_89\") else None,\n",
    "    \"rmse_83_89\": exp_results[\"perf_83_89\"][\"RMSE\"] if exp_results.get(\"perf_83_89\") else None,\n",
    "    \"r2_83_89\": exp_results[\"perf_83_89\"][\"R2\"] if exp_results.get(\"perf_83_89\") else None,\n",
    "}\n",
    "\n",
    "# Conversion en DataFrame pour export\n",
    "meta_df = pd.DataFrame.from_dict(meta_info, orient=\"index\", columns=[\"value\"])\n",
    "\n",
    "# Sauvegarde au format CSV\n",
    "meta_df.to_csv(\"linear_regression_meta.csv\")\n",
    "print(\"âœ… Fichier 'linear_regression_meta.csv' sauvegardÃ© avec les mÃ©tadonnÃ©es du modÃ¨le linÃ©aire.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ebefe84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ã‰VALUATION PSEUDOâ€“OOS (1983â€“1989) ===\n",
      "RÂ²            : -0.300\n",
      "RÂ² (origin)   : -0.243\n",
      "MAE           : 0.737\n",
      "RMSE          : 1.009\n",
      "Corr(y, yÌ‚)   : 0.424  (p=6.53e-05)\n",
      "Hit rate sign : 0.711\n",
      "AMD (|bias|)  : 0.534\n",
      "\n",
      "--- Benchmark naÃ¯f (zÃ©ro-changement) ---\n",
      "MAE_naÃ¯f0     : 0.813\n",
      "RMSE_naÃ¯f0    : 1.095\n",
      "\n",
      "--- MAE/RMSE par annÃ©e (1983â€“1989) ---\n",
      " year    n      MAE     RMSE\n",
      " 1983 12.0 1.213098 1.425113\n",
      " 1984 12.0 1.760668 1.865827\n",
      " 1985 12.0 0.437982 0.546664\n",
      " 1986 12.0 0.310524 0.364724\n",
      " 1987 12.0 0.686419 0.763033\n",
      " 1988 12.0 0.544958 0.691979\n",
      " 1989 11.0 0.157393 0.206414\n",
      "\n",
      "âœ… Ã‰valuation 1983â€“1989 terminÃ©e. RÃ©sultats dans eval_results_83_89.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mita\\AppData\\Local\\Temp\\ipykernel_19192\\1257724684.py:77: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "# === Bloc 5 â€” Ã‰valuation pseudoâ€“OOS dÃ©taillÃ©e (1983â€“1989) ===\n",
    "# (USREC est dÃ©jÃ  traitÃ©e correctement via les blocs prÃ©cÃ©dents)\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# 0ï¸âƒ£ VÃ©rifs de base\n",
    "if \"df_oos\" not in globals():\n",
    "    raise RuntimeError(\"df_oos est introuvable. Assure-toi d'avoir exÃ©cutÃ© le bloc d'entraÃ®nement/prÃ©diction.\")\n",
    "\n",
    "# 1ï¸âƒ£ Construire forecast_df Ã  partir de df_oos, restreint Ã  1983â€“1989\n",
    "forecast_slice = df_oos.loc[\"1983-01-01\":\"1989-12-31\"].copy()\n",
    "\n",
    "# On crÃ©e explicitement 'target' depuis l'index (quel que soit son nom), et on renomme y_pred -> y_hat\n",
    "forecast_df = (\n",
    "    forecast_slice.rename(columns={\"y_pred\": \"y_hat\"})[[\"y_true\", \"y_hat\"]]\n",
    "                  .assign(target=forecast_slice.index)\n",
    "                  .dropna(subset=[\"y_true\", \"y_hat\"])\n",
    ")\n",
    "forecast_df[\"target\"] = pd.to_datetime(forecast_df[\"target\"])\n",
    "forecast_df = forecast_df.sort_values(\"target\").reset_index(drop=True)\n",
    "\n",
    "# 2ï¸âƒ£ Fonctions auxiliaires\n",
    "def r2_origin_reg(y, yhat):\n",
    "    \"\"\"RÂ² d'une rÃ©gression Ã  lâ€™origine: y â‰ˆ b * yhat (sans intercept).\"\"\"\n",
    "    y = np.asarray(y); yhat = np.asarray(yhat)\n",
    "    denom = np.dot(yhat, yhat)\n",
    "    if denom == 0:\n",
    "        return np.nan\n",
    "    b = np.dot(yhat, y) / denom\n",
    "    sse = np.sum((y - b * yhat) ** 2)\n",
    "    sst = np.sum((y - y.mean()) ** 2)\n",
    "    return 1 - sse / sst if sst > 0 else np.nan\n",
    "\n",
    "def corr_pvalue(y, yhat):\n",
    "    y = np.asarray(y); yhat = np.asarray(yhat)\n",
    "    if len(y) < 3:\n",
    "        return np.nan, np.nan\n",
    "    r, p = pearsonr(y, yhat)\n",
    "    return float(r), float(p)\n",
    "\n",
    "# 3ï¸âƒ£ Ã‰valuation\n",
    "if forecast_df.empty:\n",
    "    print(\"\\n[Ã‰VALUATION 83â€“89] Aucune prÃ©vision disponible (forecast_df est vide).\")\n",
    "    eval_results_83_89 = None\n",
    "else:\n",
    "    y    = forecast_df[\"y_true\"].values.astype(float)\n",
    "    yhat = forecast_df[\"y_hat\"].values.astype(float)\n",
    "\n",
    "    # MÃ©triques principales\n",
    "    r2   = r2_score(y, yhat)\n",
    "    mae  = mean_absolute_error(y, yhat)\n",
    "    rmse = np.sqrt(mean_squared_error(y, yhat))   # âœ… compatible toutes versions\n",
    "    r2_o = r2_origin_reg(y, yhat)\n",
    "    corr, pval = corr_pvalue(y, yhat)\n",
    "    amd  = float(abs(np.mean(y - yhat)))          # biais absolu moyen\n",
    "\n",
    "    # Benchmark naÃ¯f (zÃ©ro-changement)\n",
    "    yhat_naive0 = np.zeros_like(y)\n",
    "    mae_naive0  = mean_absolute_error(y, yhat_naive0)\n",
    "    rmse_naive0 = np.sqrt(mean_squared_error(y, yhat_naive0))\n",
    "\n",
    "    # PrÃ©cision directionnelle\n",
    "    hit_rate = float(np.mean(np.sign(y) == np.sign(yhat))) if len(y) > 0 else np.nan\n",
    "\n",
    "    # Calibration (Mincerâ€“Zarnowitz): y = a + b * yhat\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(yhat.reshape(-1, 1), y)\n",
    "    a_calib = float(reg.intercept_)\n",
    "    b_calib = float(reg.coef_[0])\n",
    "\n",
    "    # RÃ©sumÃ© par annÃ©e 1983â€¦1989\n",
    "    by_year = (\n",
    "        forecast_df.assign(year=forecast_df[\"target\"].dt.year)\n",
    "                   .groupby(\"year\")\n",
    "                   .apply(lambda g: pd.Series({\n",
    "                       \"n\": len(g),\n",
    "                       \"MAE\": mean_absolute_error(g[\"y_true\"], g[\"y_hat\"]),\n",
    "                       \"RMSE\": np.sqrt(mean_squared_error(g[\"y_true\"], g[\"y_hat\"]))\n",
    "                   }))\n",
    "                   .reset_index()\n",
    "    )\n",
    "\n",
    "    # Impression des rÃ©sultats\n",
    "    print(\"\\n=== Ã‰VALUATION PSEUDOâ€“OOS (1983â€“1989) ===\")\n",
    "    print(f\"RÂ²            : {r2:.3f}\")\n",
    "    print(f\"RÂ² (origin)   : {r2_o:.3f}\")\n",
    "    print(f\"MAE           : {mae:.3f}\")\n",
    "    print(f\"RMSE          : {rmse:.3f}\")\n",
    "    print(f\"Corr(y, yÌ‚)   : {corr:.3f}  (p={pval:.3g})\")\n",
    "    print(f\"Hit rate sign : {hit_rate:.3f}\")\n",
    "    print(f\"AMD (|bias|)  : {amd:.3f}\")\n",
    "\n",
    "    print(\"\\n--- Benchmark naÃ¯f (zÃ©ro-changement) ---\")\n",
    "    print(f\"MAE_naÃ¯f0     : {mae_naive0:.3f}\")\n",
    "    print(f\"RMSE_naÃ¯f0    : {rmse_naive0:.3f}\")\n",
    "\n",
    "    if not by_year.empty:\n",
    "        print(\"\\n--- MAE/RMSE par annÃ©e (1983â€“1989) ---\")\n",
    "        print(by_year.to_string(index=False))\n",
    "\n",
    "    # Sauvegarde des rÃ©sultats\n",
    "    eval_results_83_89 = {\n",
    "        \"overall\": {\n",
    "            \"r2\": float(r2), \"r2_origin\": float(r2_o), \"mae\": float(mae), \"rmse\": float(rmse),\n",
    "            \"corr\": float(corr), \"pval\": float(pval), \"hit_rate\": float(hit_rate), \"amd\": float(amd)\n",
    "        },\n",
    "        \"benchmark_naive0\": {\n",
    "            \"mae\": float(mae_naive0), \"rmse\": float(rmse_naive0)\n",
    "        },\n",
    "        \"calibration\": {\n",
    "            \"intercept\": a_calib, \"slope\": b_calib\n",
    "        },\n",
    "        \"by_year\": by_year,\n",
    "        \"forecast_df\": forecast_df\n",
    "    }\n",
    "    \n",
    "    print(\"\\nâœ… Ã‰valuation 1983â€“1989 terminÃ©e. RÃ©sultats dans eval_results_83_89.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7821abed",
   "metadata": {},
   "source": [
    "Le modÃ¨le linÃ©aire Ã  12 mois dâ€™horizon prÃ©voit la direction du chÃ´mage mieux quâ€™un modÃ¨le naÃ¯f, mais Ã©choue Ã  reproduire lâ€™ampleur des variations, surtout lors des retournements conjoncturels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ad858c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# ðŸ” IMPORTANCE PAR PERMUTATION â€” PSEUDO-OOS\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def permutation_importance_pseudo_oos(models, df_train_global, cols_tx, h=12, n_repeats=20, metric=mean_absolute_error):\n",
    "    \"\"\"\n",
    "    Importance par permutation pour une sÃ©rie de modÃ¨les OLS entraÃ®nÃ©s\n",
    "    selon une logique expanding window pseudoâ€“OOS.\n",
    "    -> aucune rÃ©estimation\n",
    "    -> mesure la dÃ©gradation moyenne de la performance aprÃ¨s permutation de chaque variable\n",
    "    \"\"\"\n",
    "    var_imp = {col: [] for col in cols_tx}\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        # Reconstituer la fenÃªtre utilisÃ©e par le modÃ¨le i\n",
    "        end_idx = (i + 1) * 12  # correspond Ã  ton step_size = 12\n",
    "        df_win = df_train_global.iloc[:end_idx].copy()\n",
    "\n",
    "        if len(df_win) <= h:\n",
    "            continue\n",
    "\n",
    "        # PrÃ©paration des donnÃ©es (alignement X_t avec Y_{t+h})\n",
    "        X = df_win[cols_tx].iloc[:-h].copy()\n",
    "        y = df_win[\"UNRATE\"].shift(-h).iloc[:-h].copy()\n",
    "        valid = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "        X, y = X.loc[valid], y.loc[valid]\n",
    "\n",
    "        # Score de base (MAE sur donnÃ©es d'origine)\n",
    "        base_score = metric(y, model.predict(X))\n",
    "\n",
    "        # Boucle sur chaque variable\n",
    "        for col in cols_tx:\n",
    "            perm_scores = []\n",
    "            for _ in range(n_repeats):\n",
    "                X_perm = X.copy()\n",
    "                X_perm[col] = np.random.permutation(X_perm[col])\n",
    "                perm_scores.append(metric(y, model.predict(X_perm)))\n",
    "            perm_scores = np.array(perm_scores)\n",
    "            var_imp[col].append(np.mean(perm_scores) / base_score)\n",
    "\n",
    "    # AgrÃ©gation moyenne sur toutes les fenÃªtres\n",
    "    results = []\n",
    "    for col, ratios in var_imp.items():\n",
    "        if len(ratios) > 0:\n",
    "            results.append({\n",
    "                \"variable\": col,\n",
    "                \"perm_mae_ratio_mean\": np.mean(ratios),\n",
    "                \"perm_mae_ratio_std\": np.std(ratios),\n",
    "                \"n_windows\": len(ratios)\n",
    "            })\n",
    "\n",
    "    imp_df = pd.DataFrame(results).sort_values(\"perm_mae_ratio_mean\", ascending=False).reset_index(drop=True)\n",
    "    return imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa54405b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ðŸ” Importance par permutation (pseudoâ€“OOS) ===\n",
      "       variable  perm_mae_ratio_mean  perm_mae_ratio_std  n_windows\n",
      "          USREC             1.253742            0.135382         27\n",
      "          TB3MS             1.034833            0.029633         27\n",
      "        S&P 500             1.012959            0.004565         27\n",
      "DPCERA3M086SBEA             1.000301            0.000418         27\n",
      "         INDPRO             1.000260            0.001776         27\n",
      "      OILPRICEx             1.000161            0.000390         27\n",
      "            RPI             1.000154            0.000394         27\n",
      "       BUSLOANS             1.000140            0.000181         27\n",
      "           M2SL             1.000133            0.000219         27\n",
      "       CPIAUCSL             1.000030            0.000138         27\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# ðŸ§­ Appel de la fonction sur ton jeu de modÃ¨les OLS\n",
    "# ----------------------------------------------------------\n",
    "perm_df = permutation_importance_pseudo_oos(\n",
    "    models=models,\n",
    "    df_train_global=df_train_global,\n",
    "    cols_tx=cols_tx,\n",
    "    h=h,\n",
    "    n_repeats=20  # augmente Ã  50 pour des rÃ©sultats plus stables\n",
    ")\n",
    "\n",
    "print(\"\\n=== ðŸ” Importance par permutation (pseudoâ€“OOS) ===\")\n",
    "print(perm_df.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9a06ae",
   "metadata": {},
   "source": [
    "ðŸ‘‰ RÃ¨gle simple :\n",
    "- Ratio â‰ˆ 1 â†’ la variable nâ€™apporte pratiquement rien.\n",
    "- Ratio > 1.05 â†’ variable rÃ©ellement utile (la perte dâ€™information est notable).\n",
    "- Ratio > 1.20 â†’ forte importance.\n",
    "\n",
    "Ton modÃ¨le OLS prÃ©dit principalement le chÃ´mage via le cycle Ã©conomique :\n",
    "- USREC (rÃ©cession ou non) est la variable clÃ© â€” sa permutation dÃ©grade la performance de ~26 %.\n",
    "- TB3MS (taux court) les taux dâ€™intÃ©rÃªt Ã  court terme apportent une petite amÃ©lioration (signal de politique monÃ©taire).\n",
    "- Les autres variables ont un effet nÃ©gligeable.\n",
    "\n",
    "Conclusion : Ã€ horizon 12 mois, le modÃ¨le OLS base sa prÃ©vision du chÃ´mage principalement sur la prÃ©sence ou non de rÃ©cession.\n",
    "Les indicateurs financiers jouent un rÃ´le secondaire, tandis que les autres variables macro (activitÃ©, prix, monnaie) ont une contribution nÃ©gligeable dans les performances pseudoâ€“OOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3cf89f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ðŸ’¡ Importance SHAP (shares) ===\n",
      "       variable  shap_mean_abs  shap_share\n",
      "          USREC       0.342433    0.644562\n",
      "          TB3MS       0.167207    0.314733\n",
      "        S&P 500       0.017503    0.032945\n",
      "      OILPRICEx       0.001360    0.002560\n",
      "DPCERA3M086SBEA       0.001169    0.002200\n",
      "         INDPRO       0.000662    0.001246\n",
      "            RPI       0.000381    0.000717\n",
      "       BUSLOANS       0.000257    0.000484\n",
      "           M2SL       0.000188    0.000353\n",
      "       CPIAUCSL       0.000105    0.000198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\shap\\explainers\\_linear.py:99: FutureWarning: The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, maskers.Partition or maskers.Impute).\n",
      "  warnings.warn(wmsg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# ðŸ’¡ IMPORTANCE SHAPLEY (pour modÃ¨le OLS)\n",
    "# ==========================================================\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1ï¸âƒ£ On utilise le dernier modÃ¨le entraÃ®nÃ©\n",
    "model_final = models[-1]\n",
    "\n",
    "# 2ï¸âƒ£ On reconstitue ses donnÃ©es finales (derniÃ¨re fenÃªtre du train)\n",
    "df_final = df_train_global.copy()\n",
    "X_full = df_final[cols_tx].iloc[:-h].copy()\n",
    "Y_full = df_final[\"UNRATE\"].shift(-h).iloc[:-h].copy()\n",
    "valid = ~(X_full.isnull().any(axis=1) | Y_full.isnull())\n",
    "X_full = X_full.loc[valid]\n",
    "Y_full = Y_full.loc[valid]\n",
    "\n",
    "# 3ï¸âƒ£ Calcul des valeurs SHAP\n",
    "# Pour les modÃ¨les linÃ©aires, on peut utiliser shap.LinearExplainer (plus stable)\n",
    "explainer = shap.LinearExplainer(model_final, X_full, feature_perturbation=\"interventional\")\n",
    "shap_values = explainer(X_full)\n",
    "\n",
    "# 4ï¸âƒ£ Importance moyenne absolue\n",
    "shap_df = pd.DataFrame({\n",
    "    \"variable\": X_full.columns,\n",
    "    \"shap_mean_abs\": np.abs(shap_values.values).mean(axis=0),\n",
    "})\n",
    "shap_df[\"shap_share\"] = shap_df[\"shap_mean_abs\"] / shap_df[\"shap_mean_abs\"].sum()\n",
    "shap_df = shap_df.sort_values(\"shap_mean_abs\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# 5ï¸âƒ£ Affichage\n",
    "print(\"\\n=== ðŸ’¡ Importance SHAP (shares) ===\")\n",
    "print(shap_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d47ec9c",
   "metadata": {},
   "source": [
    "- USREC (0.64)\tðŸ’¥ Variable dominante (64 %) â€” la moitiÃ© des variations prÃ©dites du chÃ´mage viennent du seul indicateur de rÃ©cession. Quand USREC = 1, le modÃ¨le anticipe une hausse significative du chÃ´mage.\t\n",
    "\n",
    "- TB3MS (0.31)\tâš™ï¸ DeuxiÃ¨me facteur clÃ© (31 %) â€” les taux courts jouent un rÃ´le important : ils influencent fortement les prÃ©visions Ã  12 mois, probablement comme proxy de la politique monÃ©taire restrictive.\n",
    "\n",
    "Les valeurs SHAP confirment que la rÃ©cession (USREC) et les taux dâ€™intÃ©rÃªt Ã  court terme (TB3MS) sont les deux principaux moteurs des prÃ©visions du modÃ¨le linÃ©aire.\n",
    "Ensemble, ils expliquent prÃ¨s de 95 % de la contribution totale aux variations prÃ©dites du chÃ´mage.\n",
    "\n",
    "Les autres variables macroÃ©conomiques ont un poids nÃ©gligeable, ce qui souligne la simplicitÃ© et la nature cyclique du lien entre conjoncture et chÃ´mage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db7b28b",
   "metadata": {},
   "source": [
    "# ðŸ“Š MÃ©triques utilisÃ©es\n",
    "\n",
    "## 1) Performance globale\n",
    "\n",
    "**Coefficient de dÃ©termination (RÂ²)**  \n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (y_i - \\bar{y})^2}\n",
    "$$  \n",
    "âž¡ï¸ Part de la variance expliquÃ©e par le modÃ¨le (0 = pas mieux que la moyenne, 1 = parfait).\n",
    "\n",
    "**Erreur absolue moyenne (MAE)**  \n",
    "$$\n",
    "MAE = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i|\n",
    "$$  \n",
    "âž¡ï¸ Ã‰cart absolu moyen entre valeurs rÃ©elles et prÃ©dites, robuste aux outliers.\n",
    "\n",
    "**Erreur quadratique moyenne (RMSE)**  \n",
    "$$\n",
    "RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n",
    "$$  \n",
    "âž¡ï¸ Similaire au MAE mais pÃ©nalise davantage les grosses erreurs.\n",
    "\n",
    "**CorrÃ©lation de Pearson**  \n",
    "$$\n",
    "\\rho(y, \\hat{y}) = \\frac{\\text{Cov}(y, \\hat{y})}{\\sigma_y \\cdot \\sigma_{\\hat{y}}}\n",
    "$$  \n",
    "âž¡ï¸ Mesure le degrÃ© de lien linÃ©aire entre les prÃ©dictions et les observations.\n",
    "\n",
    "**Abs Mean Deviance (AMD)**  \n",
    "$$\n",
    "AMD = \\frac{1}{n}\\sum_{i=1}^n |\\hat{y}_i - \\bar{\\hat{y}}|\n",
    "$$  \n",
    "âž¡ï¸ Ã‰cart moyen des prÃ©dictions par rapport Ã  leur moyenne ; sert de rÃ©fÃ©rence pour la permutation prÃ©diction-basÃ©e.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Importance par permutation\n",
    "La relation entre Y et X dÃ©pend du temps. Quand on perturbe la sÃ©rie X (en la mÃ©langeant), on casse ce lien, et si lâ€™erreur augmente, cela montre que X est une variable clÃ© pour expliquer Y.\n",
    "\n",
    "**Ratio MAE**  \n",
    "$$\n",
    "PI^{MAE}_j = \\frac{MAE^{(perm)}_j}{MAE^{(base)}}\n",
    "$$  \n",
    "âž¡ï¸ Si > 1, la variable est utile pour rÃ©duire lâ€™erreur absolue.\n",
    "\n",
    "**Ratio RMSE**  \n",
    "$$\n",
    "PI^{RMSE}_j = \\frac{RMSE^{(perm)}_j}{RMSE^{(base)}}\n",
    "$$  \n",
    "âž¡ï¸ Si > 1, la variable aide Ã  limiter les grosses erreurs.\n",
    "\n",
    "**DÃ©viance de prÃ©diction**  \n",
    "$$\n",
    "PI^{dev}_j = \\frac{1}{n}\\sum_{i=1}^n \\big|\\hat{y}_i - \\hat{y}^{(perm)}_{i,j}\\big|\n",
    "$$  \n",
    "âž¡ï¸ Mesure combien les prÃ©dictions changent quand on brouille une variable.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Importance Shapley\n",
    "\n",
    "**DÃ©composition des prÃ©dictions**  \n",
    "$$\n",
    "\\hat{y}_i = \\phi_0 + \\sum_{j=1}^p \\phi_{ij}\n",
    "$$  \n",
    "âž¡ï¸ Chaque prÃ©diction est expliquÃ©e par une contribution \\(\\phi_{ij}\\) par variable.\n",
    "\n",
    "**Importance absolue moyenne**  \n",
    "$$\n",
    "\\text{Mean}(|\\phi_j|) = \\frac{1}{n}\\sum_{i=1}^n |\\phi_{ij}|\n",
    "$$  \n",
    "âž¡ï¸ Contribution moyenne (absolue) dâ€™une variable sur toutes les prÃ©dictions.\n",
    "\n",
    "**Shapley share**  \n",
    "$$\n",
    "\\Gamma_j = \\frac{\\text{Mean}(|\\phi_j|)}{\\sum_{k=1}^p \\text{Mean}(|\\phi_k|)}\n",
    "$$  \n",
    "âž¡ï¸ Part relative de la variable dans lâ€™explication totale (somme des parts = 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a6535a",
   "metadata": {},
   "source": [
    "## InterprÃ©tation des rÃ©sultats \n",
    "\n",
    "### Performance globale \n",
    "- RÂ² = 0.2266 â†’ modÃ¨le OLS explique ~23 % de la variance du chÃ´mage US.\n",
    "- MAE = 0.6774 â†’ en moyenne, lâ€™erreur absolue est de 0.68 points (dans lâ€™unitÃ© de la variable cible).\n",
    "- RMSE = 0.8750 â†’ un peu plus Ã©levÃ© que le MAE, ce qui indique la prÃ©sence de grosses erreurs ponctuelles.\n",
    "- CorrÃ©lation = 0.4760 (p â‰ˆ 10â»Â³âµ) â†’ lien positif et significatif entre prÃ©dictions et observations, mais seulement modÃ©rÃ©. Ce qui peut expliquer la prÃ©sence d'une relation \n",
    "- Abs Mean Deviance = 0.3370 â†’ sert ici de rÃ©fÃ©rence pour lâ€™importance prÃ©diction-basÃ©e : les prÃ©dictions sâ€™Ã©cartent en moyenne de 0.34 de leur propre moyenne.\n",
    "\n",
    "Lecture : le modÃ¨le OLS capte une partie utile du signal, mais laisse beaucoup de variance inexpliquÃ©e. La corrÃ©lation faible illustre Ã©ventuellement la prÃ©sence des relations non-linÃ©aire, et non captÃ©es par OLS.\n",
    "\n",
    "### ðŸ”¹ 2. Importance par permutation\n",
    "- INDPRO (Industrial Production) : la plus influente. Sa permutation augmente MAE de +19 % et RMSE de +20 %, avec une forte dÃ©viance de prÃ©diction (0.41).\n",
    "- TB3MS (Taux dâ€™intÃ©rÃªt Ã  3 mois) : impact non nÃ©gligeable, ratios ~1.02 et dÃ©viance ~0.10.\n",
    "- BUSLOANS (PrÃªts commerciaux) : rÃ´le similaire (MAE ratio 1.017, dÃ©viance ~0.09).\n",
    "- S&P 500 : contribution modÃ©rÃ©e, ratios lÃ©gÃ¨rement > 1.\n",
    "- RPI, M2SL : influence plus faible mais perceptible.\n",
    "- CPIAUCSL, OILPRICEx, DPCERA3M086SBEA : quasi neutres (ratios â‰ˆ 1, dÃ©viance trÃ¨s faible).\n",
    "\n",
    "Lecture : INDPRO domine largement la performance, les autres apportent des complÃ©ments mais plus modestes.\n",
    "\n",
    "### ðŸ”¹ 3. Importance Shapley (shares)\n",
    "- INDPRO : ~52 % de lâ€™explication totale des prÃ©dictions â†’ cohÃ©rence parfaite avec la permutation.\n",
    "- TB3MS (12 %) + BUSLOANS (12 %) : deux autres piliers importants.\n",
    "- S&P 500 (9,7 %) : contribue de faÃ§on notable.\n",
    "- M2SL (5 %), RPI (3 %), CPIAUCSL (3,5 %) : apports plus secondaires.\n",
    "- OILPRICEx et DPCERA3M086SBEA (<2 %) : quasi nÃ©gligeables dans ce modÃ¨le.\n",
    "\n",
    "Lecture : INDPRO est la variable macroÃ©conomique centrale, suivie par des indicateurs financiers (taux courts, prÃªts bancaires, marchÃ© actions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3b489",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662bb79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "saved_model = joblib.load(\"models/model_final.pkl\")\n",
    "model_final = saved_model[\"model\"]\n",
    "features = saved_model[\"features\"]\n",
    "winsor_level = saved_model[\"winsor_level\"]\n",
    "norm_var = saved_model[\"norm_var\"]\n",
    "mean_full = saved_model[\"mean_full\"]\n",
    "std_full = saved_model[\"std_full\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
