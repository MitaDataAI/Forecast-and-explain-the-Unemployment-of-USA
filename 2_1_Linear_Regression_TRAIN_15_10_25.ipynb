{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2478ba5b",
   "metadata": {},
   "source": [
    "# Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "41027a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import shap\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.utils import shuffle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22973a6a",
   "metadata": {},
   "source": [
    "# Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dedcdda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les données\n",
    "df_stationary_test = pd.read_csv(\"df_stationary_test.csv\", index_col=\"date\")\n",
    "df_stationary_test.index = pd.to_datetime(df_stationary_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "38404298",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stationary_train = pd.read_csv(\"df_stationary_train.csv\", index_col=\"date\")\n",
    "df_stationary_train.index = pd.to_datetime(df_stationary_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c1c02",
   "metadata": {},
   "source": [
    "# 1) Paramètres & sous-ensemble TRAIN global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "090941b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "081d728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Paramètres ----------\n",
    "window_len = 12      # longueur minimale de la fenêtre (non utilisé si expanding)\n",
    "step_size = 12       # refit annuel (adapter : 1 = mensuel, 12 = annuel)\n",
    "winsor_level = 0.01\n",
    "# === Bloc 1bis — Paramètre de décalage (12 mois) ===\n",
    "SHIFT_LAG = 12  # décalage des features : X_{t-12} -> y_t\n",
    "h = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "afc97582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN global\n",
    "df_train_global = df_stationary_train.sort_index().copy()\n",
    "\n",
    "# ✅ Inclure toutes les features (aucune exclusion sauf la cible)\n",
    "features = [c for c in df_stationary_train.columns if c != \"UNRATE\"]\n",
    "cols_tx = features.copy()\n",
    "\n",
    "# ---------- Bornes de validation ----------\n",
    "valid_start = pd.Timestamp(\"1983-01-01\")\n",
    "valid_end   = pd.Timestamp(\"1989-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "29b6e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Bloc 3bis — Pré-calcul des features décalées ===\n",
    "# (Optionnel mais plus propre/rapide que de refaire .shift() à chaque itération)\n",
    "df_tx_shifted = df_train_global[cols_tx].shift(SHIFT_LAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e3e66c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Bloc 2 — Helpers de prétraitement (USREC exemptée), + métriques ===\n",
    "\n",
    "def fit_preproc(X, winsor_level=0.01, do_norm=True, exempt_cols=None):\n",
    "    \"\"\"\n",
    "    Apprend les bornes de winsorisation et les stats de normalisation sur X,\n",
    "    mais n'applique rien aux colonnes 'exempt_cols' (ex: ['USREC']).\n",
    "    \"\"\"\n",
    "    exempt_cols = list(exempt_cols or [])\n",
    "    cols_w = [c for c in X.columns if c not in exempt_cols]\n",
    "\n",
    "    Xw = X.copy()\n",
    "\n",
    "    # Winsorisation uniquement sur les colonnes non-exemptées\n",
    "    if len(cols_w):\n",
    "        lower_wins = X[cols_w].quantile(winsor_level)\n",
    "        upper_wins = X[cols_w].quantile(1 - winsor_level)\n",
    "        Xw[cols_w] = Xw[cols_w].clip(lower=lower_wins, upper=upper_wins, axis=1)\n",
    "    else:\n",
    "        lower_wins = pd.Series(dtype=float)\n",
    "        upper_wins = pd.Series(dtype=float)\n",
    "\n",
    "    # Normalisation uniquement sur les colonnes non-exemptées\n",
    "    if do_norm and len(cols_w):\n",
    "        mean = Xw[cols_w].mean()\n",
    "        std  = Xw[cols_w].std().replace(0, 1)\n",
    "        Xn = Xw.copy()\n",
    "        Xn[cols_w] = (Xw[cols_w] - mean) / std\n",
    "    else:\n",
    "        mean, std = None, None\n",
    "        Xn = Xw\n",
    "\n",
    "    return Xn, {\n",
    "        \"lower_wins\": lower_wins,\n",
    "        \"upper_wins\": upper_wins,\n",
    "        \"mean\": mean,\n",
    "        \"std\": std,\n",
    "        \"norm\": do_norm,\n",
    "        \"exempt_cols\": exempt_cols\n",
    "    }\n",
    "\n",
    "\n",
    "def apply_preproc(X, prep):\n",
    "    \"\"\"\n",
    "    Applique les bornes/stats apprises sur les colonnes non-exemptées.\n",
    "    Les colonnes exemptées (ex: USREC) restent brutes.\n",
    "    \"\"\"\n",
    "    exempt_cols = list(prep.get(\"exempt_cols\", []))\n",
    "    cols_w = [c for c in X.columns if c not in exempt_cols]\n",
    "\n",
    "    Xp = X.copy()\n",
    "    if len(cols_w):\n",
    "        # Appliquer les mêmes bornes/paramètres sur les colonnes non-exemptées\n",
    "        lw = prep[\"lower_wins\"].reindex(cols_w)\n",
    "        uw = prep[\"upper_wins\"].reindex(cols_w)\n",
    "        Xp[cols_w] = Xp[cols_w].clip(lower=lw, upper=uw, axis=1)\n",
    "\n",
    "        if prep[\"norm\"]:\n",
    "            mean = prep[\"mean\"].reindex(cols_w)\n",
    "            std  = prep[\"std\"].reindex(cols_w).replace(0, 1)\n",
    "            Xp[cols_w] = (Xp[cols_w] - mean) / std\n",
    "\n",
    "    # Les colonnes exemptées (ex. USREC) restent brutes\n",
    "    return Xp\n",
    "\n",
    "\n",
    "def perf_report(y_true, y_pred):\n",
    "    # RMSE = racine du MSE manuellement pour compatibilité avec toutes les versions\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    return {\"RMSE\": float(rmse), \"MAE\": float(mae), \"R2\": float(r2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2964722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Bloc 3bis — Pré-calcul des features décalées ===\n",
    "# (Optionnel mais plus propre/rapide que de refaire .shift() à chaque itération)\n",
    "df_tx_shifted = df_train_global[cols_tx].shift(SHIFT_LAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0a476149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00] Skip (pas assez d'obs après décalage).\n",
      "[01] Fin 1961-12 — shift=12m — winsor(1.00%) + norm — 12 obs.\n",
      "[02] Fin 1962-12 — shift=12m — winsor(1.00%) + norm — 24 obs.\n",
      "[03] Fin 1963-12 — shift=12m — winsor(1.00%) + norm — 36 obs.\n",
      "[04] Fin 1964-12 — shift=12m — winsor(1.00%) + norm — 48 obs.\n",
      "[05] Fin 1965-12 — shift=12m — winsor(1.00%) + norm — 60 obs.\n",
      "[06] Fin 1966-12 — shift=12m — winsor(1.00%) + norm — 72 obs.\n",
      "[07] Fin 1967-12 — shift=12m — winsor(1.00%) + norm — 84 obs.\n",
      "[08] Fin 1968-12 — shift=12m — winsor(1.00%) + norm — 96 obs.\n",
      "[09] Fin 1969-12 — shift=12m — winsor(1.00%) + norm — 108 obs.\n",
      "[10] Fin 1970-12 — shift=12m — winsor(1.00%) + norm — 120 obs.\n",
      "[11] Fin 1971-12 — shift=12m — winsor(1.00%) + norm — 132 obs.\n",
      "[12] Fin 1972-12 — shift=12m — winsor(1.00%) + norm — 144 obs.\n",
      "[13] Fin 1973-12 — shift=12m — winsor(1.00%) + norm — 156 obs.\n",
      "[14] Fin 1974-12 — shift=12m — winsor(1.00%) + norm — 168 obs.\n",
      "[15] Fin 1975-12 — shift=12m — winsor(1.00%) + norm — 180 obs.\n",
      "[16] Fin 1976-12 — shift=12m — winsor(1.00%) + norm — 192 obs.\n",
      "[17] Fin 1977-12 — shift=12m — winsor(1.00%) + norm — 204 obs.\n",
      "[18] Fin 1978-12 — shift=12m — winsor(1.00%) + norm — 216 obs.\n",
      "[19] Fin 1979-12 — shift=12m — winsor(1.00%) + norm — 228 obs.\n",
      "[20] Fin 1980-12 — shift=12m — winsor(1.00%) + norm — 240 obs.\n",
      "[21] Fin 1981-12 — shift=12m — winsor(1.00%) + norm — 252 obs.\n",
      "[22] Fin 1982-12 — shift=12m — winsor(1.00%) + norm — 264 obs.\n",
      "[23] Fin 1983-12 — shift=12m — winsor(1.00%) + norm — 276 obs.\n",
      "[24] Fin 1984-12 — shift=12m — winsor(1.00%) + norm — 288 obs.\n",
      "[25] Fin 1985-12 — shift=12m — winsor(1.00%) + norm — 300 obs.\n",
      "[26] Fin 1986-12 — shift=12m — winsor(1.00%) + norm — 312 obs.\n",
      "[27] Fin 1987-12 — shift=12m — winsor(1.00%) + norm — 324 obs.\n",
      "[28] Fin 1988-12 — shift=12m — winsor(1.00%) + norm — 336 obs.\n"
     ]
    }
   ],
   "source": [
    "# === Bloc 3 — Boucle d'entraînement (expanding) avec prédicteurs décalés de 12 mois ===\n",
    "\n",
    "# ---------- Containers ----------\n",
    "models = []\n",
    "coefs = []\n",
    "preprocs = []\n",
    "train_periods = []\n",
    "preds = []\n",
    "norm_var = True  # True = normalisation des colonnes non-exemptées (USREC reste brute)\n",
    "\n",
    "# ---------- Boucle EXPANDING WINDOW SUR LE TRAIN ----------\n",
    "for t, end in enumerate(range(step_size, len(df_train_global), step_size)):\n",
    "    df_train_local = df_train_global.iloc[:end].copy()\n",
    "\n",
    "    # 0) Construire X_train décalé (X_{t-12}) et aligner y_t\n",
    "    X_train_raw = df_tx_shifted.loc[df_train_local.index].copy()   # prédicteurs décalés\n",
    "    y_train_full = df_train_local[\"UNRATE\"].copy()\n",
    "\n",
    "    # supprimer les premières lignes qui deviennent NaN à cause du décalage\n",
    "    mask_valid = ~X_train_raw.isna().any(axis=1)\n",
    "    X_train_raw = X_train_raw.loc[mask_valid]\n",
    "    y_train = y_train_full.loc[X_train_raw.index]\n",
    "\n",
    "    # guardrail : si pas assez d'observations, on saute cette itération\n",
    "    if len(X_train_raw) < 2:\n",
    "        print(f\"[{t:02d}] Skip (pas assez d'obs après décalage).\")\n",
    "        continue\n",
    "\n",
    "    # 1️⃣ Winsorisation/normalisation apprises sur TRAIN — USREC exclue\n",
    "    X_train, prep = fit_preproc(\n",
    "        X_train_raw,\n",
    "        winsor_level=winsor_level,\n",
    "        do_norm=norm_var,\n",
    "        exempt_cols=[\"USREC\"]  # ⬅️ USREC pas de winsor ni normalisation\n",
    "    )\n",
    "\n",
    "    # 2️⃣ Entraînement OLS\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 3️⃣ Fenêtre de prédiction (année suivante) + intersection 1983–1989\n",
    "    start_pred = (df_train_local.index[-1] + relativedelta(months=1)).replace(day=1)\n",
    "    end_pred   = start_pred + relativedelta(months=12, days=-1)\n",
    "\n",
    "    start_use = max(start_pred, valid_start)\n",
    "    end_use   = min(end_pred, valid_end)\n",
    "\n",
    "    if start_use <= end_use:\n",
    "        # X_valid doit aussi être décalé de 12 mois\n",
    "        X_valid_raw = df_tx_shifted.loc[start_use:end_use].copy()\n",
    "        # enlever les éventuels NaN résiduels\n",
    "        X_valid_raw = X_valid_raw.dropna(how=\"any\")\n",
    "\n",
    "        if not X_valid_raw.empty:\n",
    "            # y_valid = y_t aligné au même index\n",
    "            y_valid = df_train_global.loc[X_valid_raw.index, \"UNRATE\"].values\n",
    "\n",
    "            # appliquer le prétraitement appris sur TRAIN\n",
    "            X_valid = apply_preproc(X_valid_raw, prep)\n",
    "\n",
    "            # prédire\n",
    "            yhat = model.predict(X_valid)\n",
    "\n",
    "            # stocker\n",
    "            df_out = pd.DataFrame({\"y_true\": y_valid, \"y_pred\": yhat}, index=X_valid_raw.index)\n",
    "            df_out[\"model_trained_until\"] = df_train_local.index[-1]\n",
    "            preds.append(df_out)\n",
    "\n",
    "    # 4️⃣ Sauvegarde des éléments d'entraînement\n",
    "    models.append(model)\n",
    "    coefs.append(pd.Series(model.coef_, index=cols_tx))\n",
    "    preprocs.append(prep)\n",
    "    train_periods.append(df_train_local.index[-1])\n",
    "\n",
    "    print(f\"[{t:02d}] Fin {df_train_local.index[-1].strftime('%Y-%m')} — \"\n",
    "          f\"shift={SHIFT_LAG}m — winsor({winsor_level:.2%}) + \"\n",
    "          f\"{'norm' if norm_var else 'no-norm'} — {len(X_train)} obs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "32b96255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "078318d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Entraînement + validation 1983–1989 terminés (prédicteurs décalés de 12 mois).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mita\\AppData\\Local\\Temp\\ipykernel_19192\\3596606838.py:16: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series(perf_report(g[\"y_true\"], g[\"y_pred\"])))\n"
     ]
    }
   ],
   "source": [
    "# === Bloc 4 — (inchangé) Agrégation OOS + métriques + sauvegarde d'expérience ===\n",
    "# (utilise 'preds' rempli ci-dessus ; USREC reste exclue du prétraitement)\n",
    "df_oos = (pd.concat(preds).sort_index() if len(preds) else\n",
    "          pd.DataFrame(columns=[\"y_true\",\"y_pred\"]))\n",
    "\n",
    "m83 = (df_oos.index >= \"1983-01-01\") & (df_oos.index <= \"1983-12-31\")\n",
    "perf_1983 = perf_report(df_oos.loc[m83, \"y_true\"], df_oos.loc[m83, \"y_pred\"]) if m83.any() else None\n",
    "\n",
    "m8389 = (df_oos.index >= valid_start) & (df_oos.index <= valid_end)\n",
    "perf_83_89 = perf_report(df_oos.loc[m8389, \"y_true\"], df_oos.loc[m8389, \"y_pred\"]) if m8389.any() else None\n",
    "\n",
    "if not df_oos.empty:\n",
    "    df_oos[\"year\"] = df_oos.index.year\n",
    "    annual_perf = (df_oos.loc[m8389]\n",
    "                   .groupby(\"year\")\n",
    "                   .apply(lambda g: pd.Series(perf_report(g[\"y_true\"], g[\"y_pred\"])))\n",
    "                   .to_dict(orient=\"index\"))\n",
    "else:\n",
    "    annual_perf = {}\n",
    "\n",
    "exp_results = {\n",
    "    \"models\": models,\n",
    "    \"coefs\": coefs,\n",
    "    \"train_periods\": train_periods,\n",
    "    \"features\": cols_tx,\n",
    "    \"preprocs\": preprocs,\n",
    "    \"oos_predictions\": df_oos,\n",
    "    \"annual_perf\": annual_perf,\n",
    "    \"perf_1983\": perf_1983,\n",
    "    \"perf_83_89\": perf_83_89,\n",
    "    \"params\": {\n",
    "        \"step_size\": step_size,\n",
    "        \"winsor_level\": winsor_level,\n",
    "        \"norm_var\": norm_var,\n",
    "        \"valid_window\": (\"1983-01\", \"1989-12\"),\n",
    "        \"window_len\": window_len,\n",
    "        \"exempt_cols\": [\"USREC\"],\n",
    "        \"shift_lag\": SHIFT_LAG\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n✅ Entraînement + validation 1983–1989 terminés (prédicteurs décalés de 12 mois).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "771eb12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fichier 'linear_regression.pkl' sauvegardé.\n",
      "✅ Fichier 'linear_regression_meta.csv' sauvegardé avec les métadonnées du modèle linéaire.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Sauvegarde du dictionnaire de résultats\n",
    "joblib.dump(exp_results, \"linear_regression.pkl\")\n",
    "print(\"✅ Fichier 'linear_regression.pkl' sauvegardé.\")\n",
    "\n",
    "# =========================\n",
    "# 🔹 Sauvegarde des métadonnées du modèle linéaire\n",
    "# =========================\n",
    "\n",
    "# On extrait les infos clés pour documenter le modèle\n",
    "meta_info = {\n",
    "    \"trained_until\": str(exp_results[\"train_periods\"][-1]) if exp_results.get(\"train_periods\") else None,\n",
    "    \"n_models\": len(exp_results.get(\"models\", [])),\n",
    "    \"features\": len(exp_results.get(\"features\", [])),\n",
    "    \"shift_lag\": exp_results[\"params\"].get(\"shift_lag\"),\n",
    "    \"winsor_level\": exp_results[\"params\"].get(\"winsor_level\"),\n",
    "    \"norm_var\": exp_results[\"params\"].get(\"norm_var\"),\n",
    "    \"valid_window_start\": exp_results[\"params\"][\"valid_window\"][0],\n",
    "    \"valid_window_end\": exp_results[\"params\"][\"valid_window\"][1],\n",
    "    \"mae_83_89\": exp_results[\"perf_83_89\"][\"MAE\"] if exp_results.get(\"perf_83_89\") else None,\n",
    "    \"rmse_83_89\": exp_results[\"perf_83_89\"][\"RMSE\"] if exp_results.get(\"perf_83_89\") else None,\n",
    "    \"r2_83_89\": exp_results[\"perf_83_89\"][\"R2\"] if exp_results.get(\"perf_83_89\") else None,\n",
    "}\n",
    "\n",
    "# Conversion en DataFrame pour export\n",
    "meta_df = pd.DataFrame.from_dict(meta_info, orient=\"index\", columns=[\"value\"])\n",
    "\n",
    "# Sauvegarde au format CSV\n",
    "meta_df.to_csv(\"linear_regression_meta.csv\")\n",
    "print(\"✅ Fichier 'linear_regression_meta.csv' sauvegardé avec les métadonnées du modèle linéaire.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ebefe84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ÉVALUATION PSEUDO–OOS (1983–1989) ===\n",
      "R²            : -0.300\n",
      "R² (origin)   : -0.243\n",
      "MAE           : 0.737\n",
      "RMSE          : 1.009\n",
      "Corr(y, ŷ)   : 0.424  (p=6.53e-05)\n",
      "Hit rate sign : 0.711\n",
      "AMD (|bias|)  : 0.534\n",
      "\n",
      "--- Benchmark naïf (zéro-changement) ---\n",
      "MAE_naïf0     : 0.813\n",
      "RMSE_naïf0    : 1.095\n",
      "\n",
      "--- MAE/RMSE par année (1983–1989) ---\n",
      " year    n      MAE     RMSE\n",
      " 1983 12.0 1.213098 1.425113\n",
      " 1984 12.0 1.760668 1.865827\n",
      " 1985 12.0 0.437982 0.546664\n",
      " 1986 12.0 0.310524 0.364724\n",
      " 1987 12.0 0.686419 0.763033\n",
      " 1988 12.0 0.544958 0.691979\n",
      " 1989 11.0 0.157393 0.206414\n",
      "\n",
      "✅ Évaluation 1983–1989 terminée. Résultats dans eval_results_83_89.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mita\\AppData\\Local\\Temp\\ipykernel_19192\\1257724684.py:77: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "# === Bloc 5 — Évaluation pseudo–OOS détaillée (1983–1989) ===\n",
    "# (USREC est déjà traitée correctement via les blocs précédents)\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# 0️⃣ Vérifs de base\n",
    "if \"df_oos\" not in globals():\n",
    "    raise RuntimeError(\"df_oos est introuvable. Assure-toi d'avoir exécuté le bloc d'entraînement/prédiction.\")\n",
    "\n",
    "# 1️⃣ Construire forecast_df à partir de df_oos, restreint à 1983–1989\n",
    "forecast_slice = df_oos.loc[\"1983-01-01\":\"1989-12-31\"].copy()\n",
    "\n",
    "# On crée explicitement 'target' depuis l'index (quel que soit son nom), et on renomme y_pred -> y_hat\n",
    "forecast_df = (\n",
    "    forecast_slice.rename(columns={\"y_pred\": \"y_hat\"})[[\"y_true\", \"y_hat\"]]\n",
    "                  .assign(target=forecast_slice.index)\n",
    "                  .dropna(subset=[\"y_true\", \"y_hat\"])\n",
    ")\n",
    "forecast_df[\"target\"] = pd.to_datetime(forecast_df[\"target\"])\n",
    "forecast_df = forecast_df.sort_values(\"target\").reset_index(drop=True)\n",
    "\n",
    "# 2️⃣ Fonctions auxiliaires\n",
    "def r2_origin_reg(y, yhat):\n",
    "    \"\"\"R² d'une régression à l’origine: y ≈ b * yhat (sans intercept).\"\"\"\n",
    "    y = np.asarray(y); yhat = np.asarray(yhat)\n",
    "    denom = np.dot(yhat, yhat)\n",
    "    if denom == 0:\n",
    "        return np.nan\n",
    "    b = np.dot(yhat, y) / denom\n",
    "    sse = np.sum((y - b * yhat) ** 2)\n",
    "    sst = np.sum((y - y.mean()) ** 2)\n",
    "    return 1 - sse / sst if sst > 0 else np.nan\n",
    "\n",
    "def corr_pvalue(y, yhat):\n",
    "    y = np.asarray(y); yhat = np.asarray(yhat)\n",
    "    if len(y) < 3:\n",
    "        return np.nan, np.nan\n",
    "    r, p = pearsonr(y, yhat)\n",
    "    return float(r), float(p)\n",
    "\n",
    "# 3️⃣ Évaluation\n",
    "if forecast_df.empty:\n",
    "    print(\"\\n[ÉVALUATION 83–89] Aucune prévision disponible (forecast_df est vide).\")\n",
    "    eval_results_83_89 = None\n",
    "else:\n",
    "    y    = forecast_df[\"y_true\"].values.astype(float)\n",
    "    yhat = forecast_df[\"y_hat\"].values.astype(float)\n",
    "\n",
    "    # Métriques principales\n",
    "    r2   = r2_score(y, yhat)\n",
    "    mae  = mean_absolute_error(y, yhat)\n",
    "    rmse = np.sqrt(mean_squared_error(y, yhat))   # ✅ compatible toutes versions\n",
    "    r2_o = r2_origin_reg(y, yhat)\n",
    "    corr, pval = corr_pvalue(y, yhat)\n",
    "    amd  = float(abs(np.mean(y - yhat)))          # biais absolu moyen\n",
    "\n",
    "    # Benchmark naïf (zéro-changement)\n",
    "    yhat_naive0 = np.zeros_like(y)\n",
    "    mae_naive0  = mean_absolute_error(y, yhat_naive0)\n",
    "    rmse_naive0 = np.sqrt(mean_squared_error(y, yhat_naive0))\n",
    "\n",
    "    # Précision directionnelle\n",
    "    hit_rate = float(np.mean(np.sign(y) == np.sign(yhat))) if len(y) > 0 else np.nan\n",
    "\n",
    "    # Calibration (Mincer–Zarnowitz): y = a + b * yhat\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(yhat.reshape(-1, 1), y)\n",
    "    a_calib = float(reg.intercept_)\n",
    "    b_calib = float(reg.coef_[0])\n",
    "\n",
    "    # Résumé par année 1983…1989\n",
    "    by_year = (\n",
    "        forecast_df.assign(year=forecast_df[\"target\"].dt.year)\n",
    "                   .groupby(\"year\")\n",
    "                   .apply(lambda g: pd.Series({\n",
    "                       \"n\": len(g),\n",
    "                       \"MAE\": mean_absolute_error(g[\"y_true\"], g[\"y_hat\"]),\n",
    "                       \"RMSE\": np.sqrt(mean_squared_error(g[\"y_true\"], g[\"y_hat\"]))\n",
    "                   }))\n",
    "                   .reset_index()\n",
    "    )\n",
    "\n",
    "    # Impression des résultats\n",
    "    print(\"\\n=== ÉVALUATION PSEUDO–OOS (1983–1989) ===\")\n",
    "    print(f\"R²            : {r2:.3f}\")\n",
    "    print(f\"R² (origin)   : {r2_o:.3f}\")\n",
    "    print(f\"MAE           : {mae:.3f}\")\n",
    "    print(f\"RMSE          : {rmse:.3f}\")\n",
    "    print(f\"Corr(y, ŷ)   : {corr:.3f}  (p={pval:.3g})\")\n",
    "    print(f\"Hit rate sign : {hit_rate:.3f}\")\n",
    "    print(f\"AMD (|bias|)  : {amd:.3f}\")\n",
    "\n",
    "    print(\"\\n--- Benchmark naïf (zéro-changement) ---\")\n",
    "    print(f\"MAE_naïf0     : {mae_naive0:.3f}\")\n",
    "    print(f\"RMSE_naïf0    : {rmse_naive0:.3f}\")\n",
    "\n",
    "    if not by_year.empty:\n",
    "        print(\"\\n--- MAE/RMSE par année (1983–1989) ---\")\n",
    "        print(by_year.to_string(index=False))\n",
    "\n",
    "    # Sauvegarde des résultats\n",
    "    eval_results_83_89 = {\n",
    "        \"overall\": {\n",
    "            \"r2\": float(r2), \"r2_origin\": float(r2_o), \"mae\": float(mae), \"rmse\": float(rmse),\n",
    "            \"corr\": float(corr), \"pval\": float(pval), \"hit_rate\": float(hit_rate), \"amd\": float(amd)\n",
    "        },\n",
    "        \"benchmark_naive0\": {\n",
    "            \"mae\": float(mae_naive0), \"rmse\": float(rmse_naive0)\n",
    "        },\n",
    "        \"calibration\": {\n",
    "            \"intercept\": a_calib, \"slope\": b_calib\n",
    "        },\n",
    "        \"by_year\": by_year,\n",
    "        \"forecast_df\": forecast_df\n",
    "    }\n",
    "    \n",
    "    print(\"\\n✅ Évaluation 1983–1989 terminée. Résultats dans eval_results_83_89.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7821abed",
   "metadata": {},
   "source": [
    "Le modèle linéaire à 12 mois d’horizon prévoit la direction du chômage mieux qu’un modèle naïf, mais échoue à reproduire l’ampleur des variations, surtout lors des retournements conjoncturels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ad858c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 🔍 IMPORTANCE PAR PERMUTATION — PSEUDO-OOS\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def permutation_importance_pseudo_oos(models, df_train_global, cols_tx, h=12, n_repeats=20, metric=mean_absolute_error):\n",
    "    \"\"\"\n",
    "    Importance par permutation pour une série de modèles OLS entraînés\n",
    "    selon une logique expanding window pseudo–OOS.\n",
    "    -> aucune réestimation\n",
    "    -> mesure la dégradation moyenne de la performance après permutation de chaque variable\n",
    "    \"\"\"\n",
    "    var_imp = {col: [] for col in cols_tx}\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        # Reconstituer la fenêtre utilisée par le modèle i\n",
    "        end_idx = (i + 1) * 12  # correspond à ton step_size = 12\n",
    "        df_win = df_train_global.iloc[:end_idx].copy()\n",
    "\n",
    "        if len(df_win) <= h:\n",
    "            continue\n",
    "\n",
    "        # Préparation des données (alignement X_t avec Y_{t+h})\n",
    "        X = df_win[cols_tx].iloc[:-h].copy()\n",
    "        y = df_win[\"UNRATE\"].shift(-h).iloc[:-h].copy()\n",
    "        valid = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "        X, y = X.loc[valid], y.loc[valid]\n",
    "\n",
    "        # Score de base (MAE sur données d'origine)\n",
    "        base_score = metric(y, model.predict(X))\n",
    "\n",
    "        # Boucle sur chaque variable\n",
    "        for col in cols_tx:\n",
    "            perm_scores = []\n",
    "            for _ in range(n_repeats):\n",
    "                X_perm = X.copy()\n",
    "                X_perm[col] = np.random.permutation(X_perm[col])\n",
    "                perm_scores.append(metric(y, model.predict(X_perm)))\n",
    "            perm_scores = np.array(perm_scores)\n",
    "            var_imp[col].append(np.mean(perm_scores) / base_score)\n",
    "\n",
    "    # Agrégation moyenne sur toutes les fenêtres\n",
    "    results = []\n",
    "    for col, ratios in var_imp.items():\n",
    "        if len(ratios) > 0:\n",
    "            results.append({\n",
    "                \"variable\": col,\n",
    "                \"perm_mae_ratio_mean\": np.mean(ratios),\n",
    "                \"perm_mae_ratio_std\": np.std(ratios),\n",
    "                \"n_windows\": len(ratios)\n",
    "            })\n",
    "\n",
    "    imp_df = pd.DataFrame(results).sort_values(\"perm_mae_ratio_mean\", ascending=False).reset_index(drop=True)\n",
    "    return imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa54405b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 🔍 Importance par permutation (pseudo–OOS) ===\n",
      "       variable  perm_mae_ratio_mean  perm_mae_ratio_std  n_windows\n",
      "          USREC             1.253742            0.135382         27\n",
      "          TB3MS             1.034833            0.029633         27\n",
      "        S&P 500             1.012959            0.004565         27\n",
      "DPCERA3M086SBEA             1.000301            0.000418         27\n",
      "         INDPRO             1.000260            0.001776         27\n",
      "      OILPRICEx             1.000161            0.000390         27\n",
      "            RPI             1.000154            0.000394         27\n",
      "       BUSLOANS             1.000140            0.000181         27\n",
      "           M2SL             1.000133            0.000219         27\n",
      "       CPIAUCSL             1.000030            0.000138         27\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 🧭 Appel de la fonction sur ton jeu de modèles OLS\n",
    "# ----------------------------------------------------------\n",
    "perm_df = permutation_importance_pseudo_oos(\n",
    "    models=models,\n",
    "    df_train_global=df_train_global,\n",
    "    cols_tx=cols_tx,\n",
    "    h=h,\n",
    "    n_repeats=20  # augmente à 50 pour des résultats plus stables\n",
    ")\n",
    "\n",
    "print(\"\\n=== 🔍 Importance par permutation (pseudo–OOS) ===\")\n",
    "print(perm_df.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9a06ae",
   "metadata": {},
   "source": [
    "👉 Règle simple :\n",
    "- Ratio ≈ 1 → la variable n’apporte pratiquement rien.\n",
    "- Ratio > 1.05 → variable réellement utile (la perte d’information est notable).\n",
    "- Ratio > 1.20 → forte importance.\n",
    "\n",
    "Ton modèle OLS prédit principalement le chômage via le cycle économique :\n",
    "- USREC (récession ou non) est la variable clé — sa permutation dégrade la performance de ~26 %.\n",
    "- TB3MS (taux court) les taux d’intérêt à court terme apportent une petite amélioration (signal de politique monétaire).\n",
    "- Les autres variables ont un effet négligeable.\n",
    "\n",
    "Conclusion : À horizon 12 mois, le modèle OLS base sa prévision du chômage principalement sur la présence ou non de récession.\n",
    "Les indicateurs financiers jouent un rôle secondaire, tandis que les autres variables macro (activité, prix, monnaie) ont une contribution négligeable dans les performances pseudo–OOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3cf89f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 💡 Importance SHAP (shares) ===\n",
      "       variable  shap_mean_abs  shap_share\n",
      "          USREC       0.342433    0.644562\n",
      "          TB3MS       0.167207    0.314733\n",
      "        S&P 500       0.017503    0.032945\n",
      "      OILPRICEx       0.001360    0.002560\n",
      "DPCERA3M086SBEA       0.001169    0.002200\n",
      "         INDPRO       0.000662    0.001246\n",
      "            RPI       0.000381    0.000717\n",
      "       BUSLOANS       0.000257    0.000484\n",
      "           M2SL       0.000188    0.000353\n",
      "       CPIAUCSL       0.000105    0.000198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\shap\\explainers\\_linear.py:99: FutureWarning: The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, maskers.Partition or maskers.Impute).\n",
      "  warnings.warn(wmsg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 💡 IMPORTANCE SHAPLEY (pour modèle OLS)\n",
    "# ==========================================================\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1️⃣ On utilise le dernier modèle entraîné\n",
    "model_final = models[-1]\n",
    "\n",
    "# 2️⃣ On reconstitue ses données finales (dernière fenêtre du train)\n",
    "df_final = df_train_global.copy()\n",
    "X_full = df_final[cols_tx].iloc[:-h].copy()\n",
    "Y_full = df_final[\"UNRATE\"].shift(-h).iloc[:-h].copy()\n",
    "valid = ~(X_full.isnull().any(axis=1) | Y_full.isnull())\n",
    "X_full = X_full.loc[valid]\n",
    "Y_full = Y_full.loc[valid]\n",
    "\n",
    "# 3️⃣ Calcul des valeurs SHAP\n",
    "# Pour les modèles linéaires, on peut utiliser shap.LinearExplainer (plus stable)\n",
    "explainer = shap.LinearExplainer(model_final, X_full, feature_perturbation=\"interventional\")\n",
    "shap_values = explainer(X_full)\n",
    "\n",
    "# 4️⃣ Importance moyenne absolue\n",
    "shap_df = pd.DataFrame({\n",
    "    \"variable\": X_full.columns,\n",
    "    \"shap_mean_abs\": np.abs(shap_values.values).mean(axis=0),\n",
    "})\n",
    "shap_df[\"shap_share\"] = shap_df[\"shap_mean_abs\"] / shap_df[\"shap_mean_abs\"].sum()\n",
    "shap_df = shap_df.sort_values(\"shap_mean_abs\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# 5️⃣ Affichage\n",
    "print(\"\\n=== 💡 Importance SHAP (shares) ===\")\n",
    "print(shap_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d47ec9c",
   "metadata": {},
   "source": [
    "- USREC (0.64)\t💥 Variable dominante (64 %) — la moitié des variations prédites du chômage viennent du seul indicateur de récession. Quand USREC = 1, le modèle anticipe une hausse significative du chômage.\t\n",
    "\n",
    "- TB3MS (0.31)\t⚙️ Deuxième facteur clé (31 %) — les taux courts jouent un rôle important : ils influencent fortement les prévisions à 12 mois, probablement comme proxy de la politique monétaire restrictive.\n",
    "\n",
    "Les valeurs SHAP confirment que la récession (USREC) et les taux d’intérêt à court terme (TB3MS) sont les deux principaux moteurs des prévisions du modèle linéaire.\n",
    "Ensemble, ils expliquent près de 95 % de la contribution totale aux variations prédites du chômage.\n",
    "\n",
    "Les autres variables macroéconomiques ont un poids négligeable, ce qui souligne la simplicité et la nature cyclique du lien entre conjoncture et chômage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db7b28b",
   "metadata": {},
   "source": [
    "# 📊 Métriques utilisées\n",
    "\n",
    "## 1) Performance globale\n",
    "\n",
    "**Coefficient de détermination (R²)**  \n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (y_i - \\bar{y})^2}\n",
    "$$  \n",
    "➡️ Part de la variance expliquée par le modèle (0 = pas mieux que la moyenne, 1 = parfait).\n",
    "\n",
    "**Erreur absolue moyenne (MAE)**  \n",
    "$$\n",
    "MAE = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i|\n",
    "$$  \n",
    "➡️ Écart absolu moyen entre valeurs réelles et prédites, robuste aux outliers.\n",
    "\n",
    "**Erreur quadratique moyenne (RMSE)**  \n",
    "$$\n",
    "RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n",
    "$$  \n",
    "➡️ Similaire au MAE mais pénalise davantage les grosses erreurs.\n",
    "\n",
    "**Corrélation de Pearson**  \n",
    "$$\n",
    "\\rho(y, \\hat{y}) = \\frac{\\text{Cov}(y, \\hat{y})}{\\sigma_y \\cdot \\sigma_{\\hat{y}}}\n",
    "$$  \n",
    "➡️ Mesure le degré de lien linéaire entre les prédictions et les observations.\n",
    "\n",
    "**Abs Mean Deviance (AMD)**  \n",
    "$$\n",
    "AMD = \\frac{1}{n}\\sum_{i=1}^n |\\hat{y}_i - \\bar{\\hat{y}}|\n",
    "$$  \n",
    "➡️ Écart moyen des prédictions par rapport à leur moyenne ; sert de référence pour la permutation prédiction-basée.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Importance par permutation\n",
    "La relation entre Y et X dépend du temps. Quand on perturbe la série X (en la mélangeant), on casse ce lien, et si l’erreur augmente, cela montre que X est une variable clé pour expliquer Y.\n",
    "\n",
    "**Ratio MAE**  \n",
    "$$\n",
    "PI^{MAE}_j = \\frac{MAE^{(perm)}_j}{MAE^{(base)}}\n",
    "$$  \n",
    "➡️ Si > 1, la variable est utile pour réduire l’erreur absolue.\n",
    "\n",
    "**Ratio RMSE**  \n",
    "$$\n",
    "PI^{RMSE}_j = \\frac{RMSE^{(perm)}_j}{RMSE^{(base)}}\n",
    "$$  \n",
    "➡️ Si > 1, la variable aide à limiter les grosses erreurs.\n",
    "\n",
    "**Déviance de prédiction**  \n",
    "$$\n",
    "PI^{dev}_j = \\frac{1}{n}\\sum_{i=1}^n \\big|\\hat{y}_i - \\hat{y}^{(perm)}_{i,j}\\big|\n",
    "$$  \n",
    "➡️ Mesure combien les prédictions changent quand on brouille une variable.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Importance Shapley\n",
    "\n",
    "**Décomposition des prédictions**  \n",
    "$$\n",
    "\\hat{y}_i = \\phi_0 + \\sum_{j=1}^p \\phi_{ij}\n",
    "$$  \n",
    "➡️ Chaque prédiction est expliquée par une contribution \\(\\phi_{ij}\\) par variable.\n",
    "\n",
    "**Importance absolue moyenne**  \n",
    "$$\n",
    "\\text{Mean}(|\\phi_j|) = \\frac{1}{n}\\sum_{i=1}^n |\\phi_{ij}|\n",
    "$$  \n",
    "➡️ Contribution moyenne (absolue) d’une variable sur toutes les prédictions.\n",
    "\n",
    "**Shapley share**  \n",
    "$$\n",
    "\\Gamma_j = \\frac{\\text{Mean}(|\\phi_j|)}{\\sum_{k=1}^p \\text{Mean}(|\\phi_k|)}\n",
    "$$  \n",
    "➡️ Part relative de la variable dans l’explication totale (somme des parts = 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a6535a",
   "metadata": {},
   "source": [
    "## Interprétation des résultats \n",
    "\n",
    "### Performance globale \n",
    "- R² = 0.2266 → modèle OLS explique ~23 % de la variance du chômage US.\n",
    "- MAE = 0.6774 → en moyenne, l’erreur absolue est de 0.68 points (dans l’unité de la variable cible).\n",
    "- RMSE = 0.8750 → un peu plus élevé que le MAE, ce qui indique la présence de grosses erreurs ponctuelles.\n",
    "- Corrélation = 0.4760 (p ≈ 10⁻³⁵) → lien positif et significatif entre prédictions et observations, mais seulement modéré. Ce qui peut expliquer la présence d'une relation \n",
    "- Abs Mean Deviance = 0.3370 → sert ici de référence pour l’importance prédiction-basée : les prédictions s’écartent en moyenne de 0.34 de leur propre moyenne.\n",
    "\n",
    "Lecture : le modèle OLS capte une partie utile du signal, mais laisse beaucoup de variance inexpliquée. La corrélation faible illustre éventuellement la présence des relations non-linéaire, et non captées par OLS.\n",
    "\n",
    "### 🔹 2. Importance par permutation\n",
    "- INDPRO (Industrial Production) : la plus influente. Sa permutation augmente MAE de +19 % et RMSE de +20 %, avec une forte déviance de prédiction (0.41).\n",
    "- TB3MS (Taux d’intérêt à 3 mois) : impact non négligeable, ratios ~1.02 et déviance ~0.10.\n",
    "- BUSLOANS (Prêts commerciaux) : rôle similaire (MAE ratio 1.017, déviance ~0.09).\n",
    "- S&P 500 : contribution modérée, ratios légèrement > 1.\n",
    "- RPI, M2SL : influence plus faible mais perceptible.\n",
    "- CPIAUCSL, OILPRICEx, DPCERA3M086SBEA : quasi neutres (ratios ≈ 1, déviance très faible).\n",
    "\n",
    "Lecture : INDPRO domine largement la performance, les autres apportent des compléments mais plus modestes.\n",
    "\n",
    "### 🔹 3. Importance Shapley (shares)\n",
    "- INDPRO : ~52 % de l’explication totale des prédictions → cohérence parfaite avec la permutation.\n",
    "- TB3MS (12 %) + BUSLOANS (12 %) : deux autres piliers importants.\n",
    "- S&P 500 (9,7 %) : contribue de façon notable.\n",
    "- M2SL (5 %), RPI (3 %), CPIAUCSL (3,5 %) : apports plus secondaires.\n",
    "- OILPRICEx et DPCERA3M086SBEA (<2 %) : quasi négligeables dans ce modèle.\n",
    "\n",
    "Lecture : INDPRO est la variable macroéconomique centrale, suivie par des indicateurs financiers (taux courts, prêts bancaires, marché actions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3b489",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662bb79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "saved_model = joblib.load(\"models/model_final.pkl\")\n",
    "model_final = saved_model[\"model\"]\n",
    "features = saved_model[\"features\"]\n",
    "winsor_level = saved_model[\"winsor_level\"]\n",
    "norm_var = saved_model[\"norm_var\"]\n",
    "mean_full = saved_model[\"mean_full\"]\n",
    "std_full = saved_model[\"std_full\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
