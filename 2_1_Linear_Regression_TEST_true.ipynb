{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2478ba5b",
   "metadata": {},
   "source": [
    "# Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41027a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Portofolio Data science\\Time Series\\Travaux pratiques Bases\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import shap\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.utils import shuffle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22973a6a",
   "metadata": {},
   "source": [
    "# Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedcdda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les donn√©es\n",
    "df_stationary_test = pd.read_csv(\"df_stationary_test.csv\", index_col=\"date\")\n",
    "df_stationary_test.index = pd.to_datetime(df_stationary_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aff52a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 29 mod√®les OLS recharg√©s avec succ√®s depuis 'OLS_h12_expanding_window.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Nom du fichier du mod√®le sauvegard√©\n",
    "file_name = \"OLS_h12_expanding_window.pkl\"\n",
    "\n",
    "# üîÅ Charger le fichier\n",
    "with open(file_name, \"rb\") as f:\n",
    "    exp_results = pickle.load(f)\n",
    "\n",
    "# Extraire uniquement les mod√®les\n",
    "models = exp_results[\"models\"]\n",
    "\n",
    "print(f\"‚úÖ {len(models)} mod√®les OLS recharg√©s avec succ√®s depuis '{file_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c1c02",
   "metadata": {},
   "source": [
    "# 1) Param√®tres & sous-ensemble TRAIN global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "081d728d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_stationary_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m h             = params.get(\u001b[33m\"\u001b[39m\u001b[33mh\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m12\u001b[39m)\n\u001b[32m     17\u001b[39m winsor_level  = params.get(\u001b[33m\"\u001b[39m\u001b[33mwinsor_level\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0.01\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m cols_tx       = params.get(\u001b[33m\"\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m\"\u001b[39m, [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf_stationary_train\u001b[49m.columns \u001b[38;5;28;01mif\u001b[39;00m c != \u001b[33m\"\u001b[39m\u001b[33mUNRATE\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 2) RECONSTRUIRE les transfos de la DERNI√àRE fen√™tre d'entra√Ænement\u001b[39;00m\n\u001b[32m     21\u001b[39m last_train_end = pd.to_datetime(train_periods[-\u001b[32m1\u001b[39m])  \u001b[38;5;66;03m# ex. 1989-12\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df_stationary_train' is not defined"
     ]
    }
   ],
   "source": [
    "# ================== TEST AVEC MOD√àLE PR√â-ENTRA√éN√â (reconstruit les transfos) ==================\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# 1) Charger le pack\n",
    "with open(\"OLS_h12_expanding_window.pkl\", \"rb\") as f:\n",
    "    exp_results = pickle.load(f)\n",
    "\n",
    "models        = exp_results[\"models\"]\n",
    "train_periods = exp_results[\"train_periods\"]\n",
    "params        = exp_results[\"params\"]\n",
    "\n",
    "# R√©cup params (fallback si manquants)\n",
    "h             = params.get(\"h\", 12)\n",
    "winsor_level  = params.get(\"winsor_level\", 0.01)\n",
    "cols_tx       = params.get(\"features\", [c for c in df_stationary_train.columns if c != \"UNRATE\"])\n",
    "\n",
    "# 2) RECONSTRUIRE les transfos de la DERNI√àRE fen√™tre d'entra√Ænement\n",
    "last_train_end = pd.to_datetime(train_periods[-1])  # ex. 1989-12\n",
    "df_train_global = df_stationary_train.loc[:last_train_end].copy().sort_index()\n",
    "\n",
    "# Aligner X_t avec Y_{t+h} pour le fit (comme lors de l'entra√Ænement)\n",
    "if len(df_train_global) <= h:\n",
    "    raise ValueError(\"Fen√™tre d'entra√Ænement trop courte pour reconstruire les transfos.\")\n",
    "X_last = df_train_global.loc[:, cols_tx].iloc[:-h].copy()\n",
    "Y_last = df_train_global[\"UNRATE\"].shift(-h).iloc[:-h].copy()\n",
    "valid  = ~(X_last.isnull().any(axis=1) | Y_last.isnull())\n",
    "X_last = X_last.loc[valid]\n",
    "\n",
    "# Winsor + normalisation (transfos √† r√©utiliser sur le test)\n",
    "lower_wins = X_last.quantile(winsor_level)\n",
    "upper_wins = X_last.quantile(1 - winsor_level)\n",
    "Xw_last    = X_last.clip(lower=lower_wins, upper=upper_wins, axis=1)\n",
    "mean_last  = Xw_last.mean()\n",
    "std_last   = Xw_last.std().replace(0, 1)\n",
    "\n",
    "# 3) Jeu de TEST (dates post√©rieures √† last_train_end)\n",
    "df_test_full = df_stationary_test.copy().sort_index()\n",
    "df_test      = df_test_full.loc[last_train_end:]  # pour chaque t on pr√©dit y_{t+h}\n",
    "\n",
    "# 4) Pr√©dire √† horizon h avec le DERNIER mod√®le (aucune r√©estimation)\n",
    "model_final = models[-1]\n",
    "records = []\n",
    "\n",
    "for t_date in df_test.index:\n",
    "    # v√©rifier t+h dans df_test_full\n",
    "    try:\n",
    "        t_pos = df_test_full.index.get_loc(t_date)\n",
    "    except KeyError:\n",
    "        continue\n",
    "    target_pos = t_pos + h\n",
    "    if target_pos >= len(df_test_full):\n",
    "        break\n",
    "\n",
    "    # X_t (1xK) + m√™mes transfos que sur la derni√®re fen√™tre d'entra√Ænement\n",
    "    x_t  = df_test_full.loc[[t_date], cols_tx].copy()\n",
    "    xw_t = x_t.clip(lower=lower_wins, upper=upper_wins, axis=1)\n",
    "    xs_t = (xw_t - mean_last) / std_last\n",
    "\n",
    "    y_hat = float(model_final.predict(xs_t)[0])\n",
    "    target_date = df_test_full.index[target_pos]\n",
    "    y_true = float(df_test_full.loc[target_date, \"UNRATE\"])\n",
    "\n",
    "    records.append({\"origin\": t_date, \"target\": target_date, \"y_hat\": y_hat, \"y_true\": y_true})\n",
    "\n",
    "forecast_test_df = pd.DataFrame(records)\n",
    "\n",
    "# 5) √âvaluation\n",
    "if forecast_test_df.empty:\n",
    "    print(\"Aucune pr√©vision produite sur le test (v√©rifie dates et horizon h).\")\n",
    "else:\n",
    "    y, yhat = forecast_test_df[\"y_true\"].values, forecast_test_df[\"y_hat\"].values\n",
    "    r2   = r2_score(y, yhat)\n",
    "    mae  = mean_absolute_error(y, yhat)\n",
    "    rmse = np.sqrt(mean_squared_error(y, yhat))\n",
    "    corr = np.corrcoef(y, yhat)[0, 1]\n",
    "\n",
    "    print(f\"\\n=== TEST pseudo-OOS (h = {h} mois) ===\")\n",
    "    print(f\"R¬≤   : {r2:.3f}\\nMAE  : {mae:.3f}\\nRMSE : {rmse:.3f}\\nCorr : {corr:.3f}\")\n",
    "\n",
    "    forecast_test_df.to_csv(\"forecast_test_OLS.csv\", index=False)\n",
    "    print(\"Pr√©visions test export√©es -> forecast_test_OLS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efd2fb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== √âVALUATION PSEUDO‚ÄìOOS (h = 12 mois) ===\n",
      "R¬≤            : 0.095\n",
      "R¬≤ (origin)   : 0.205\n",
      "MAE           : 0.809\n",
      "RMSE          : 1.058\n",
      "Corr(y, yÃÇ)   : 0.453  (p=0.0154)\n",
      "Hit rate sign : 0.714\n",
      "AMD (|bias|)  : 0.010\n",
      "\n",
      "--- Benchmark na√Øf (z√©ro-changement) ---\n",
      "MAE_na√Øf0     : 0.800\n",
      "RMSE_na√Øf0    : 1.113\n",
      "\n",
      "--- MAE/RMSE par d√©cennie ---\n",
      " decade    n      MAE     RMSE\n",
      "   1960  8.0 0.695125 1.136080\n",
      "   1970 10.0 0.772075 0.930614\n",
      "   1980 10.0 0.937675 1.112530\n",
      "\n",
      "‚úÖ √âvaluation termin√©e. R√©sultats enregistr√©s dans eval_results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mita\\AppData\\Local\\Temp\\ipykernel_14108\\1481986021.py:60: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "# ===================== √âVALUATION PSEUDO‚ÄìOUT-OF-SAMPLE =====================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# 1Ô∏è‚É£ Rassembler les pr√©visions\n",
    "forecast_df = pd.DataFrame(forecast_records).dropna(subset=[\"y_true\", \"y_hat\"]).copy()\n",
    "forecast_df = forecast_df.sort_values(\"target\").reset_index(drop=True)\n",
    "\n",
    "def r2_origin_reg(y, yhat):\n",
    "    \"\"\"R¬≤ d'une r√©gression √† l‚Äôorigine: y ‚âà b * yhat (sans intercept).\"\"\"\n",
    "    denom = np.dot(yhat, yhat)\n",
    "    if denom == 0:\n",
    "        return np.nan\n",
    "    b = np.dot(yhat, y) / denom\n",
    "    sse = np.sum((y - b * yhat) ** 2)\n",
    "    sst = np.sum((y - y.mean()) ** 2)\n",
    "    return 1 - sse / sst if sst > 0 else np.nan\n",
    "\n",
    "def corr_pvalue(y, yhat):\n",
    "    if len(y) < 3:\n",
    "        return np.nan, np.nan\n",
    "    r, p = pearsonr(y, yhat)\n",
    "    return float(r), float(p)\n",
    "\n",
    "if forecast_df.empty:\n",
    "    print(\"\\n[√âVALUATION] Aucune pr√©vision disponible (forecast_df est vide).\")\n",
    "else:\n",
    "    y = forecast_df[\"y_true\"].values\n",
    "    yhat = forecast_df[\"y_hat\"].values\n",
    "\n",
    "    # 2Ô∏è‚É£ M√©triques principales\n",
    "    r2   = r2_score(y, yhat)\n",
    "    mae  = mean_absolute_error(y, yhat)\n",
    "    rmse = np.sqrt(mean_squared_error(y, yhat))   # ‚úÖ compatible toutes versions\n",
    "    r2_o = r2_origin_reg(y, yhat)\n",
    "    corr, pval = corr_pvalue(y, yhat)\n",
    "    amd  = float(abs(np.mean(y - yhat)))          # biais absolu moyen\n",
    "\n",
    "    # 3Ô∏è‚É£ Benchmark na√Øf (utile si la cible est Œî12 UNRATE)\n",
    "    yhat_naive0 = np.zeros_like(y)\n",
    "    mae_naive0  = mean_absolute_error(y, yhat_naive0)\n",
    "    rmse_naive0 = np.sqrt(mean_squared_error(y, yhat_naive0))\n",
    "\n",
    "    # 4Ô∏è‚É£ Pr√©cision directionnelle\n",
    "    hit_rate = float(np.mean(np.sign(y) == np.sign(yhat))) if len(y) > 0 else np.nan\n",
    "\n",
    "    # 5Ô∏è‚É£ Calibration (Mincer‚ÄìZarnowitz): y = a + b * yhat\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(yhat.reshape(-1, 1), y)\n",
    "    a_calib = float(reg.intercept_)\n",
    "    b_calib = float(reg.coef_[0])\n",
    "\n",
    "    # 6Ô∏è‚É£ R√©sum√© par d√©cennie (facultatif)\n",
    "    by_decade = (\n",
    "        forecast_df.assign(decade=forecast_df[\"target\"].dt.year // 10 * 10)\n",
    "                   .groupby(\"decade\")\n",
    "                   .apply(lambda g: pd.Series({\n",
    "                       \"n\": len(g),\n",
    "                       \"MAE\": mean_absolute_error(g[\"y_true\"], g[\"y_hat\"]),\n",
    "                       \"RMSE\": np.sqrt(mean_squared_error(g[\"y_true\"], g[\"y_hat\"]))\n",
    "                   }))\n",
    "                   .reset_index()\n",
    "    )\n",
    "\n",
    "    # 7Ô∏è‚É£ Impression des r√©sultats\n",
    "    print(\"\\n=== √âVALUATION PSEUDO‚ÄìOOS (h = {} mois) ===\".format(h))\n",
    "    print(f\"R¬≤            : {r2:.3f}\")\n",
    "    print(f\"R¬≤ (origin)   : {r2_o:.3f}\")\n",
    "    print(f\"MAE           : {mae:.3f}\")\n",
    "    print(f\"RMSE          : {rmse:.3f}\")\n",
    "    print(f\"Corr(y, yÃÇ)   : {corr:.3f}  (p={pval:.3g})\")\n",
    "    print(f\"Hit rate sign : {hit_rate:.3f}\")\n",
    "    print(f\"AMD (|bias|)  : {amd:.3f}\")\n",
    "\n",
    "    print(\"\\n--- Benchmark na√Øf (z√©ro-changement) ---\")\n",
    "    print(f\"MAE_na√Øf0     : {mae_naive0:.3f}\")\n",
    "    print(f\"RMSE_na√Øf0    : {rmse_naive0:.3f}\")\n",
    "\n",
    "    if not by_decade.empty:\n",
    "        print(\"\\n--- MAE/RMSE par d√©cennie ---\")\n",
    "        print(by_decade.to_string(index=False))\n",
    "\n",
    "    # 8Ô∏è‚É£ Sauvegarde des r√©sultats\n",
    "    eval_results = {\n",
    "        \"overall\": {\n",
    "            \"r2\": r2, \"r2_origin\": r2_o, \"mae\": mae, \"rmse\": rmse,\n",
    "            \"corr\": corr, \"pval\": pval, \"hit_rate\": hit_rate, \"amd\": amd\n",
    "        },\n",
    "        \"benchmark_naive0\": {\n",
    "            \"mae\": mae_naive0, \"rmse\": rmse_naive0\n",
    "        },\n",
    "        \"calibration\": {\n",
    "            \"intercept\": a_calib, \"slope\": b_calib\n",
    "        },\n",
    "        \"by_decade\": by_decade,\n",
    "        \"forecast_df\": forecast_df\n",
    "    }\n",
    "\n",
    "    print(\"\\n‚úÖ √âvaluation termin√©e. R√©sultats enregistr√©s dans eval_results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad858c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üîç IMPORTANCE PAR PERMUTATION ‚Äî PSEUDO-OOS\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def permutation_importance_pseudo_oos(models, df_train_global, cols_tx, h=12, n_repeats=20, metric=mean_absolute_error):\n",
    "    \"\"\"\n",
    "    Importance par permutation pour une s√©rie de mod√®les OLS entra√Æn√©s\n",
    "    selon une logique expanding window pseudo‚ÄìOOS.\n",
    "    -> aucune r√©estimation\n",
    "    -> mesure la d√©gradation moyenne de la performance apr√®s permutation de chaque variable\n",
    "    \"\"\"\n",
    "    var_imp = {col: [] for col in cols_tx}\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        # Reconstituer la fen√™tre utilis√©e par le mod√®le i\n",
    "        end_idx = (i + 1) * 12  # correspond √† ton step_size = 12\n",
    "        df_win = df_train_global.iloc[:end_idx].copy()\n",
    "\n",
    "        if len(df_win) <= h:\n",
    "            continue\n",
    "\n",
    "        # Pr√©paration des donn√©es (alignement X_t avec Y_{t+h})\n",
    "        X = df_win[cols_tx].iloc[:-h].copy()\n",
    "        y = df_win[\"UNRATE\"].shift(-h).iloc[:-h].copy()\n",
    "        valid = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "        X, y = X.loc[valid], y.loc[valid]\n",
    "\n",
    "        # Score de base (MAE sur donn√©es d'origine)\n",
    "        base_score = metric(y, model.predict(X))\n",
    "\n",
    "        # Boucle sur chaque variable\n",
    "        for col in cols_tx:\n",
    "            perm_scores = []\n",
    "            for _ in range(n_repeats):\n",
    "                X_perm = X.copy()\n",
    "                X_perm[col] = np.random.permutation(X_perm[col])\n",
    "                perm_scores.append(metric(y, model.predict(X_perm)))\n",
    "            perm_scores = np.array(perm_scores)\n",
    "            var_imp[col].append(np.mean(perm_scores) / base_score)\n",
    "\n",
    "    # Agr√©gation moyenne sur toutes les fen√™tres\n",
    "    results = []\n",
    "    for col, ratios in var_imp.items():\n",
    "        if len(ratios) > 0:\n",
    "            results.append({\n",
    "                \"variable\": col,\n",
    "                \"perm_mae_ratio_mean\": np.mean(ratios),\n",
    "                \"perm_mae_ratio_std\": np.std(ratios),\n",
    "                \"n_windows\": len(ratios)\n",
    "            })\n",
    "\n",
    "    imp_df = pd.DataFrame(results).sort_values(\"perm_mae_ratio_mean\", ascending=False).reset_index(drop=True)\n",
    "    return imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54405b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== üîç Importance par permutation (pseudo‚ÄìOOS) ===\n",
      "       variable  perm_mae_ratio_mean  perm_mae_ratio_std  n_windows\n",
      "          USREC             1.092658            0.051073         28\n",
      "          TB3MS             1.028961            0.025485         28\n",
      "        S&P 500             1.013475            0.004537         28\n",
      "      OILPRICEx             1.004680            0.004453         28\n",
      "DPCERA3M086SBEA             1.000228            0.000298         28\n",
      "           M2SL             1.000208            0.000311         28\n",
      "         INDPRO             1.000093            0.001816         28\n",
      "       BUSLOANS             1.000056            0.000236         28\n",
      "            RPI             1.000051            0.000412         28\n",
      "       CPIAUCSL             1.000038            0.000150         28\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# üß≠ Appel de la fonction sur ton jeu de mod√®les OLS\n",
    "# ----------------------------------------------------------\n",
    "perm_df = permutation_importance_pseudo_oos(\n",
    "    models=models,\n",
    "    df_train_global=df_train_global,\n",
    "    cols_tx=cols_tx,\n",
    "    h=h,\n",
    "    n_repeats=20  # augmente √† 50 pour des r√©sultats plus stables\n",
    ")\n",
    "\n",
    "print(\"\\n=== üîç Importance par permutation (pseudo‚ÄìOOS) ===\")\n",
    "print(perm_df.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9a06ae",
   "metadata": {},
   "source": [
    "Ton mod√®le OLS pr√©dit principalement le ch√¥mage via le cycle √©conomique :\n",
    "- USREC (r√©cession) est la variable cl√© ‚Äî sa permutation d√©grade la performance de ~9 %.\n",
    "- TB3MS (taux court) joue un r√¥le secondaire.\n",
    "- Les autres variables ont un effet n√©gligeable.\n",
    "\n",
    "Conclusion : le pouvoir pr√©dictif du mod√®le vient surtout des variables cycliques (r√©cession, taux), les autres apportent peu d‚Äôinformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cf89f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== üí° Importance SHAP (shares) ===\n",
      "       variable  shap_mean_abs  shap_share\n",
      "          TB3MS       0.148710    0.485751\n",
      "          USREC       0.124740    0.407453\n",
      "        S&P 500       0.017688    0.057777\n",
      "      OILPRICEx       0.012534    0.040941\n",
      "DPCERA3M086SBEA       0.001011    0.003303\n",
      "         INDPRO       0.000871    0.002845\n",
      "       CPIAUCSL       0.000309    0.001010\n",
      "           M2SL       0.000204    0.000667\n",
      "            RPI       0.000043    0.000139\n",
      "       BUSLOANS       0.000035    0.000114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Portofolio Data science\\Time Series\\Travaux pratiques Bases\\.venv\\Lib\\site-packages\\shap\\explainers\\_linear.py:99: FutureWarning: The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, maskers.Partition or maskers.Impute).\n",
      "  warnings.warn(wmsg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# üí° IMPORTANCE SHAPLEY (pour mod√®le OLS)\n",
    "# ==========================================================\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1Ô∏è‚É£ On utilise le dernier mod√®le entra√Æn√©\n",
    "model_final = models[-1]\n",
    "\n",
    "# 2Ô∏è‚É£ On reconstitue ses donn√©es finales (derni√®re fen√™tre du train)\n",
    "df_final = df_train_global.copy()\n",
    "X_full = df_final[cols_tx].iloc[:-h].copy()\n",
    "Y_full = df_final[\"UNRATE\"].shift(-h).iloc[:-h].copy()\n",
    "valid = ~(X_full.isnull().any(axis=1) | Y_full.isnull())\n",
    "X_full = X_full.loc[valid]\n",
    "Y_full = Y_full.loc[valid]\n",
    "\n",
    "# 3Ô∏è‚É£ Calcul des valeurs SHAP\n",
    "# Pour les mod√®les lin√©aires, on peut utiliser shap.LinearExplainer (plus stable)\n",
    "explainer = shap.LinearExplainer(model_final, X_full, feature_perturbation=\"interventional\")\n",
    "shap_values = explainer(X_full)\n",
    "\n",
    "# 4Ô∏è‚É£ Importance moyenne absolue\n",
    "shap_df = pd.DataFrame({\n",
    "    \"variable\": X_full.columns,\n",
    "    \"shap_mean_abs\": np.abs(shap_values.values).mean(axis=0),\n",
    "})\n",
    "shap_df[\"shap_share\"] = shap_df[\"shap_mean_abs\"] / shap_df[\"shap_mean_abs\"].sum()\n",
    "shap_df = shap_df.sort_values(\"shap_mean_abs\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# 5Ô∏è‚É£ Affichage\n",
    "print(\"\\n=== üí° Importance SHAP (shares) ===\")\n",
    "print(shap_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d47ec9c",
   "metadata": {},
   "source": [
    "- TB3MS (0.49)\tüü¢ Repr√©sente ~49 % de l‚Äôinfluence totale du mod√®le. Le taux d‚Äôint√©r√™t √† 3 mois est donc la variable la plus d√©terminante pour les pr√©visions du ch√¥mage : quand les taux montent, le mod√®le anticipe souvent une hausse future du ch√¥mage.\n",
    "\n",
    "- USREC (0.41)\tüîµ Repr√©sente ~41 % de l‚Äôinfluence totale. Le dummy de r√©cession (NBER) p√®se presque autant : le simple fait d‚Äô√™tre en r√©cession ou non explique une large part des variations pr√©vues du ch√¥mage.\n",
    "\n",
    "- S&P 500, OILPRICEx üü† Poids faibles (~6 % et 4 %) : les conditions boursi√®res et le prix du p√©trole ont un impact marginal dans la version lin√©aire du mod√®le."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db7b28b",
   "metadata": {},
   "source": [
    "# üìä M√©triques utilis√©es\n",
    "\n",
    "## 1) Performance globale\n",
    "\n",
    "**Coefficient de d√©termination (R¬≤)**  \n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (y_i - \\bar{y})^2}\n",
    "$$  \n",
    "‚û°Ô∏è Part de la variance expliqu√©e par le mod√®le (0 = pas mieux que la moyenne, 1 = parfait).\n",
    "\n",
    "**Erreur absolue moyenne (MAE)**  \n",
    "$$\n",
    "MAE = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i|\n",
    "$$  \n",
    "‚û°Ô∏è √âcart absolu moyen entre valeurs r√©elles et pr√©dites, robuste aux outliers.\n",
    "\n",
    "**Erreur quadratique moyenne (RMSE)**  \n",
    "$$\n",
    "RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n",
    "$$  \n",
    "‚û°Ô∏è Similaire au MAE mais p√©nalise davantage les grosses erreurs.\n",
    "\n",
    "**Corr√©lation de Pearson**  \n",
    "$$\n",
    "\\rho(y, \\hat{y}) = \\frac{\\text{Cov}(y, \\hat{y})}{\\sigma_y \\cdot \\sigma_{\\hat{y}}}\n",
    "$$  \n",
    "‚û°Ô∏è Mesure le degr√© de lien lin√©aire entre les pr√©dictions et les observations.\n",
    "\n",
    "**Abs Mean Deviance (AMD)**  \n",
    "$$\n",
    "AMD = \\frac{1}{n}\\sum_{i=1}^n |\\hat{y}_i - \\bar{\\hat{y}}|\n",
    "$$  \n",
    "‚û°Ô∏è √âcart moyen des pr√©dictions par rapport √† leur moyenne ; sert de r√©f√©rence pour la permutation pr√©diction-bas√©e.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Importance par permutation\n",
    "La relation entre Y et X d√©pend du temps. Quand on perturbe la s√©rie X (en la m√©langeant), on casse ce lien, et si l‚Äôerreur augmente, cela montre que X est une variable cl√© pour expliquer Y.\n",
    "\n",
    "**Ratio MAE**  \n",
    "$$\n",
    "PI^{MAE}_j = \\frac{MAE^{(perm)}_j}{MAE^{(base)}}\n",
    "$$  \n",
    "‚û°Ô∏è Si > 1, la variable est utile pour r√©duire l‚Äôerreur absolue.\n",
    "\n",
    "**Ratio RMSE**  \n",
    "$$\n",
    "PI^{RMSE}_j = \\frac{RMSE^{(perm)}_j}{RMSE^{(base)}}\n",
    "$$  \n",
    "‚û°Ô∏è Si > 1, la variable aide √† limiter les grosses erreurs.\n",
    "\n",
    "**D√©viance de pr√©diction**  \n",
    "$$\n",
    "PI^{dev}_j = \\frac{1}{n}\\sum_{i=1}^n \\big|\\hat{y}_i - \\hat{y}^{(perm)}_{i,j}\\big|\n",
    "$$  \n",
    "‚û°Ô∏è Mesure combien les pr√©dictions changent quand on brouille une variable.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Importance Shapley\n",
    "\n",
    "**D√©composition des pr√©dictions**  \n",
    "$$\n",
    "\\hat{y}_i = \\phi_0 + \\sum_{j=1}^p \\phi_{ij}\n",
    "$$  \n",
    "‚û°Ô∏è Chaque pr√©diction est expliqu√©e par une contribution \\(\\phi_{ij}\\) par variable.\n",
    "\n",
    "**Importance absolue moyenne**  \n",
    "$$\n",
    "\\text{Mean}(|\\phi_j|) = \\frac{1}{n}\\sum_{i=1}^n |\\phi_{ij}|\n",
    "$$  \n",
    "‚û°Ô∏è Contribution moyenne (absolue) d‚Äôune variable sur toutes les pr√©dictions.\n",
    "\n",
    "**Shapley share**  \n",
    "$$\n",
    "\\Gamma_j = \\frac{\\text{Mean}(|\\phi_j|)}{\\sum_{k=1}^p \\text{Mean}(|\\phi_k|)}\n",
    "$$  \n",
    "‚û°Ô∏è Part relative de la variable dans l‚Äôexplication totale (somme des parts = 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a6535a",
   "metadata": {},
   "source": [
    "## Interpr√©tation des r√©sultats \n",
    "\n",
    "### Performance globale \n",
    "- R¬≤ = 0.2266 ‚Üí mod√®le OLS explique ~23 % de la variance du ch√¥mage US.\n",
    "- MAE = 0.6774 ‚Üí en moyenne, l‚Äôerreur absolue est de 0.68 points (dans l‚Äôunit√© de la variable cible).\n",
    "- RMSE = 0.8750 ‚Üí un peu plus √©lev√© que le MAE, ce qui indique la pr√©sence de grosses erreurs ponctuelles.\n",
    "- Corr√©lation = 0.4760 (p ‚âà 10‚Åª¬≥‚Åµ) ‚Üí lien positif et significatif entre pr√©dictions et observations, mais seulement mod√©r√©. Ce qui peut expliquer la pr√©sence d'une relation \n",
    "- Abs Mean Deviance = 0.3370 ‚Üí sert ici de r√©f√©rence pour l‚Äôimportance pr√©diction-bas√©e : les pr√©dictions s‚Äô√©cartent en moyenne de 0.34 de leur propre moyenne.\n",
    "\n",
    "Lecture : le mod√®le OLS capte une partie utile du signal, mais laisse beaucoup de variance inexpliqu√©e. La corr√©lation faible illustre √©ventuellement la pr√©sence des relations non-lin√©aire, et non capt√©es par OLS.\n",
    "\n",
    "### üîπ 2. Importance par permutation\n",
    "- INDPRO (Industrial Production) : la plus influente. Sa permutation augmente MAE de +19 % et RMSE de +20 %, avec une forte d√©viance de pr√©diction (0.41).\n",
    "- TB3MS (Taux d‚Äôint√©r√™t √† 3 mois) : impact non n√©gligeable, ratios ~1.02 et d√©viance ~0.10.\n",
    "- BUSLOANS (Pr√™ts commerciaux) : r√¥le similaire (MAE ratio 1.017, d√©viance ~0.09).\n",
    "- S&P 500 : contribution mod√©r√©e, ratios l√©g√®rement > 1.\n",
    "- RPI, M2SL : influence plus faible mais perceptible.\n",
    "- CPIAUCSL, OILPRICEx, DPCERA3M086SBEA : quasi neutres (ratios ‚âà 1, d√©viance tr√®s faible).\n",
    "\n",
    "Lecture : INDPRO domine largement la performance, les autres apportent des compl√©ments mais plus modestes.\n",
    "\n",
    "### üîπ 3. Importance Shapley (shares)\n",
    "- INDPRO : ~52 % de l‚Äôexplication totale des pr√©dictions ‚Üí coh√©rence parfaite avec la permutation.\n",
    "- TB3MS (12 %) + BUSLOANS (12 %) : deux autres piliers importants.\n",
    "- S&P 500 (9,7 %) : contribue de fa√ßon notable.\n",
    "- M2SL (5 %), RPI (3 %), CPIAUCSL (3,5 %) : apports plus secondaires.\n",
    "- OILPRICEx et DPCERA3M086SBEA (<2 %) : quasi n√©gligeables dans ce mod√®le.\n",
    "\n",
    "Lecture : INDPRO est la variable macro√©conomique centrale, suivie par des indicateurs financiers (taux courts, pr√™ts bancaires, march√© actions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3b489",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662bb79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "saved_model = joblib.load(\"models/model_final.pkl\")\n",
    "model_final = saved_model[\"model\"]\n",
    "features = saved_model[\"features\"]\n",
    "winsor_level = saved_model[\"winsor_level\"]\n",
    "norm_var = saved_model[\"norm_var\"]\n",
    "mean_full = saved_model[\"mean_full\"]\n",
    "std_full = saved_model[\"std_full\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Travaux pratiques Bases)",
   "language": "python",
   "name": "venv-travaux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
