{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2478ba5b",
   "metadata": {},
   "source": [
    "# Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41027a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Portofolio Data science\\Time Series\\Travaux pratiques Bases\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import shap\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.utils import shuffle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22973a6a",
   "metadata": {},
   "source": [
    "# Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedcdda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les données\n",
    "df_stationary_test = pd.read_csv(\"df_stationary_test.csv\", index_col=\"date\")\n",
    "df_stationary_test.index = pd.to_datetime(df_stationary_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aff52a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 29 modèles OLS rechargés avec succès depuis 'OLS_h12_expanding_window.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Nom du fichier du modèle sauvegardé\n",
    "file_name = \"OLS_h12_expanding_window.pkl\"\n",
    "\n",
    "# 🔁 Charger le fichier\n",
    "with open(file_name, \"rb\") as f:\n",
    "    exp_results = pickle.load(f)\n",
    "\n",
    "# Extraire uniquement les modèles\n",
    "models = exp_results[\"models\"]\n",
    "\n",
    "print(f\"✅ {len(models)} modèles OLS rechargés avec succès depuis '{file_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c1c02",
   "metadata": {},
   "source": [
    "# 1) Paramètres & sous-ensemble TRAIN global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "081d728d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_stationary_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m h             = params.get(\u001b[33m\"\u001b[39m\u001b[33mh\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m12\u001b[39m)\n\u001b[32m     17\u001b[39m winsor_level  = params.get(\u001b[33m\"\u001b[39m\u001b[33mwinsor_level\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0.01\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m cols_tx       = params.get(\u001b[33m\"\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m\"\u001b[39m, [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf_stationary_train\u001b[49m.columns \u001b[38;5;28;01mif\u001b[39;00m c != \u001b[33m\"\u001b[39m\u001b[33mUNRATE\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 2) RECONSTRUIRE les transfos de la DERNIÈRE fenêtre d'entraînement\u001b[39;00m\n\u001b[32m     21\u001b[39m last_train_end = pd.to_datetime(train_periods[-\u001b[32m1\u001b[39m])  \u001b[38;5;66;03m# ex. 1989-12\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df_stationary_train' is not defined"
     ]
    }
   ],
   "source": [
    "# ================== TEST AVEC MODÈLE PRÉ-ENTRAÎNÉ (reconstruit les transfos) ==================\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# 1) Charger le pack\n",
    "with open(\"OLS_h12_expanding_window.pkl\", \"rb\") as f:\n",
    "    exp_results = pickle.load(f)\n",
    "\n",
    "models        = exp_results[\"models\"]\n",
    "train_periods = exp_results[\"train_periods\"]\n",
    "params        = exp_results[\"params\"]\n",
    "\n",
    "# Récup params (fallback si manquants)\n",
    "h             = params.get(\"h\", 12)\n",
    "winsor_level  = params.get(\"winsor_level\", 0.01)\n",
    "cols_tx       = params.get(\"features\", [c for c in df_stationary_train.columns if c != \"UNRATE\"])\n",
    "\n",
    "# 2) RECONSTRUIRE les transfos de la DERNIÈRE fenêtre d'entraînement\n",
    "last_train_end = pd.to_datetime(train_periods[-1])  # ex. 1989-12\n",
    "df_train_global = df_stationary_train.loc[:last_train_end].copy().sort_index()\n",
    "\n",
    "# Aligner X_t avec Y_{t+h} pour le fit (comme lors de l'entraînement)\n",
    "if len(df_train_global) <= h:\n",
    "    raise ValueError(\"Fenêtre d'entraînement trop courte pour reconstruire les transfos.\")\n",
    "X_last = df_train_global.loc[:, cols_tx].iloc[:-h].copy()\n",
    "Y_last = df_train_global[\"UNRATE\"].shift(-h).iloc[:-h].copy()\n",
    "valid  = ~(X_last.isnull().any(axis=1) | Y_last.isnull())\n",
    "X_last = X_last.loc[valid]\n",
    "\n",
    "# Winsor + normalisation (transfos à réutiliser sur le test)\n",
    "lower_wins = X_last.quantile(winsor_level)\n",
    "upper_wins = X_last.quantile(1 - winsor_level)\n",
    "Xw_last    = X_last.clip(lower=lower_wins, upper=upper_wins, axis=1)\n",
    "mean_last  = Xw_last.mean()\n",
    "std_last   = Xw_last.std().replace(0, 1)\n",
    "\n",
    "# 3) Jeu de TEST (dates postérieures à last_train_end)\n",
    "df_test_full = df_stationary_test.copy().sort_index()\n",
    "df_test      = df_test_full.loc[last_train_end:]  # pour chaque t on prédit y_{t+h}\n",
    "\n",
    "# 4) Prédire à horizon h avec le DERNIER modèle (aucune réestimation)\n",
    "model_final = models[-1]\n",
    "records = []\n",
    "\n",
    "for t_date in df_test.index:\n",
    "    # vérifier t+h dans df_test_full\n",
    "    try:\n",
    "        t_pos = df_test_full.index.get_loc(t_date)\n",
    "    except KeyError:\n",
    "        continue\n",
    "    target_pos = t_pos + h\n",
    "    if target_pos >= len(df_test_full):\n",
    "        break\n",
    "\n",
    "    # X_t (1xK) + mêmes transfos que sur la dernière fenêtre d'entraînement\n",
    "    x_t  = df_test_full.loc[[t_date], cols_tx].copy()\n",
    "    xw_t = x_t.clip(lower=lower_wins, upper=upper_wins, axis=1)\n",
    "    xs_t = (xw_t - mean_last) / std_last\n",
    "\n",
    "    y_hat = float(model_final.predict(xs_t)[0])\n",
    "    target_date = df_test_full.index[target_pos]\n",
    "    y_true = float(df_test_full.loc[target_date, \"UNRATE\"])\n",
    "\n",
    "    records.append({\"origin\": t_date, \"target\": target_date, \"y_hat\": y_hat, \"y_true\": y_true})\n",
    "\n",
    "forecast_test_df = pd.DataFrame(records)\n",
    "\n",
    "# 5) Évaluation\n",
    "if forecast_test_df.empty:\n",
    "    print(\"Aucune prévision produite sur le test (vérifie dates et horizon h).\")\n",
    "else:\n",
    "    y, yhat = forecast_test_df[\"y_true\"].values, forecast_test_df[\"y_hat\"].values\n",
    "    r2   = r2_score(y, yhat)\n",
    "    mae  = mean_absolute_error(y, yhat)\n",
    "    rmse = np.sqrt(mean_squared_error(y, yhat))\n",
    "    corr = np.corrcoef(y, yhat)[0, 1]\n",
    "\n",
    "    print(f\"\\n=== TEST pseudo-OOS (h = {h} mois) ===\")\n",
    "    print(f\"R²   : {r2:.3f}\\nMAE  : {mae:.3f}\\nRMSE : {rmse:.3f}\\nCorr : {corr:.3f}\")\n",
    "\n",
    "    forecast_test_df.to_csv(\"forecast_test_OLS.csv\", index=False)\n",
    "    print(\"Prévisions test exportées -> forecast_test_OLS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efd2fb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ÉVALUATION PSEUDO–OOS (h = 12 mois) ===\n",
      "R²            : 0.095\n",
      "R² (origin)   : 0.205\n",
      "MAE           : 0.809\n",
      "RMSE          : 1.058\n",
      "Corr(y, ŷ)   : 0.453  (p=0.0154)\n",
      "Hit rate sign : 0.714\n",
      "AMD (|bias|)  : 0.010\n",
      "\n",
      "--- Benchmark naïf (zéro-changement) ---\n",
      "MAE_naïf0     : 0.800\n",
      "RMSE_naïf0    : 1.113\n",
      "\n",
      "--- MAE/RMSE par décennie ---\n",
      " decade    n      MAE     RMSE\n",
      "   1960  8.0 0.695125 1.136080\n",
      "   1970 10.0 0.772075 0.930614\n",
      "   1980 10.0 0.937675 1.112530\n",
      "\n",
      "✅ Évaluation terminée. Résultats enregistrés dans eval_results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mita\\AppData\\Local\\Temp\\ipykernel_14108\\1481986021.py:60: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "# ===================== ÉVALUATION PSEUDO–OUT-OF-SAMPLE =====================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# 1️⃣ Rassembler les prévisions\n",
    "forecast_df = pd.DataFrame(forecast_records).dropna(subset=[\"y_true\", \"y_hat\"]).copy()\n",
    "forecast_df = forecast_df.sort_values(\"target\").reset_index(drop=True)\n",
    "\n",
    "def r2_origin_reg(y, yhat):\n",
    "    \"\"\"R² d'une régression à l’origine: y ≈ b * yhat (sans intercept).\"\"\"\n",
    "    denom = np.dot(yhat, yhat)\n",
    "    if denom == 0:\n",
    "        return np.nan\n",
    "    b = np.dot(yhat, y) / denom\n",
    "    sse = np.sum((y - b * yhat) ** 2)\n",
    "    sst = np.sum((y - y.mean()) ** 2)\n",
    "    return 1 - sse / sst if sst > 0 else np.nan\n",
    "\n",
    "def corr_pvalue(y, yhat):\n",
    "    if len(y) < 3:\n",
    "        return np.nan, np.nan\n",
    "    r, p = pearsonr(y, yhat)\n",
    "    return float(r), float(p)\n",
    "\n",
    "if forecast_df.empty:\n",
    "    print(\"\\n[ÉVALUATION] Aucune prévision disponible (forecast_df est vide).\")\n",
    "else:\n",
    "    y = forecast_df[\"y_true\"].values\n",
    "    yhat = forecast_df[\"y_hat\"].values\n",
    "\n",
    "    # 2️⃣ Métriques principales\n",
    "    r2   = r2_score(y, yhat)\n",
    "    mae  = mean_absolute_error(y, yhat)\n",
    "    rmse = np.sqrt(mean_squared_error(y, yhat))   # ✅ compatible toutes versions\n",
    "    r2_o = r2_origin_reg(y, yhat)\n",
    "    corr, pval = corr_pvalue(y, yhat)\n",
    "    amd  = float(abs(np.mean(y - yhat)))          # biais absolu moyen\n",
    "\n",
    "    # 3️⃣ Benchmark naïf (utile si la cible est Δ12 UNRATE)\n",
    "    yhat_naive0 = np.zeros_like(y)\n",
    "    mae_naive0  = mean_absolute_error(y, yhat_naive0)\n",
    "    rmse_naive0 = np.sqrt(mean_squared_error(y, yhat_naive0))\n",
    "\n",
    "    # 4️⃣ Précision directionnelle\n",
    "    hit_rate = float(np.mean(np.sign(y) == np.sign(yhat))) if len(y) > 0 else np.nan\n",
    "\n",
    "    # 5️⃣ Calibration (Mincer–Zarnowitz): y = a + b * yhat\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(yhat.reshape(-1, 1), y)\n",
    "    a_calib = float(reg.intercept_)\n",
    "    b_calib = float(reg.coef_[0])\n",
    "\n",
    "    # 6️⃣ Résumé par décennie (facultatif)\n",
    "    by_decade = (\n",
    "        forecast_df.assign(decade=forecast_df[\"target\"].dt.year // 10 * 10)\n",
    "                   .groupby(\"decade\")\n",
    "                   .apply(lambda g: pd.Series({\n",
    "                       \"n\": len(g),\n",
    "                       \"MAE\": mean_absolute_error(g[\"y_true\"], g[\"y_hat\"]),\n",
    "                       \"RMSE\": np.sqrt(mean_squared_error(g[\"y_true\"], g[\"y_hat\"]))\n",
    "                   }))\n",
    "                   .reset_index()\n",
    "    )\n",
    "\n",
    "    # 7️⃣ Impression des résultats\n",
    "    print(\"\\n=== ÉVALUATION PSEUDO–OOS (h = {} mois) ===\".format(h))\n",
    "    print(f\"R²            : {r2:.3f}\")\n",
    "    print(f\"R² (origin)   : {r2_o:.3f}\")\n",
    "    print(f\"MAE           : {mae:.3f}\")\n",
    "    print(f\"RMSE          : {rmse:.3f}\")\n",
    "    print(f\"Corr(y, ŷ)   : {corr:.3f}  (p={pval:.3g})\")\n",
    "    print(f\"Hit rate sign : {hit_rate:.3f}\")\n",
    "    print(f\"AMD (|bias|)  : {amd:.3f}\")\n",
    "\n",
    "    print(\"\\n--- Benchmark naïf (zéro-changement) ---\")\n",
    "    print(f\"MAE_naïf0     : {mae_naive0:.3f}\")\n",
    "    print(f\"RMSE_naïf0    : {rmse_naive0:.3f}\")\n",
    "\n",
    "    if not by_decade.empty:\n",
    "        print(\"\\n--- MAE/RMSE par décennie ---\")\n",
    "        print(by_decade.to_string(index=False))\n",
    "\n",
    "    # 8️⃣ Sauvegarde des résultats\n",
    "    eval_results = {\n",
    "        \"overall\": {\n",
    "            \"r2\": r2, \"r2_origin\": r2_o, \"mae\": mae, \"rmse\": rmse,\n",
    "            \"corr\": corr, \"pval\": pval, \"hit_rate\": hit_rate, \"amd\": amd\n",
    "        },\n",
    "        \"benchmark_naive0\": {\n",
    "            \"mae\": mae_naive0, \"rmse\": rmse_naive0\n",
    "        },\n",
    "        \"calibration\": {\n",
    "            \"intercept\": a_calib, \"slope\": b_calib\n",
    "        },\n",
    "        \"by_decade\": by_decade,\n",
    "        \"forecast_df\": forecast_df\n",
    "    }\n",
    "\n",
    "    print(\"\\n✅ Évaluation terminée. Résultats enregistrés dans eval_results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad858c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 🔍 IMPORTANCE PAR PERMUTATION — PSEUDO-OOS\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def permutation_importance_pseudo_oos(models, df_train_global, cols_tx, h=12, n_repeats=20, metric=mean_absolute_error):\n",
    "    \"\"\"\n",
    "    Importance par permutation pour une série de modèles OLS entraînés\n",
    "    selon une logique expanding window pseudo–OOS.\n",
    "    -> aucune réestimation\n",
    "    -> mesure la dégradation moyenne de la performance après permutation de chaque variable\n",
    "    \"\"\"\n",
    "    var_imp = {col: [] for col in cols_tx}\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        # Reconstituer la fenêtre utilisée par le modèle i\n",
    "        end_idx = (i + 1) * 12  # correspond à ton step_size = 12\n",
    "        df_win = df_train_global.iloc[:end_idx].copy()\n",
    "\n",
    "        if len(df_win) <= h:\n",
    "            continue\n",
    "\n",
    "        # Préparation des données (alignement X_t avec Y_{t+h})\n",
    "        X = df_win[cols_tx].iloc[:-h].copy()\n",
    "        y = df_win[\"UNRATE\"].shift(-h).iloc[:-h].copy()\n",
    "        valid = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "        X, y = X.loc[valid], y.loc[valid]\n",
    "\n",
    "        # Score de base (MAE sur données d'origine)\n",
    "        base_score = metric(y, model.predict(X))\n",
    "\n",
    "        # Boucle sur chaque variable\n",
    "        for col in cols_tx:\n",
    "            perm_scores = []\n",
    "            for _ in range(n_repeats):\n",
    "                X_perm = X.copy()\n",
    "                X_perm[col] = np.random.permutation(X_perm[col])\n",
    "                perm_scores.append(metric(y, model.predict(X_perm)))\n",
    "            perm_scores = np.array(perm_scores)\n",
    "            var_imp[col].append(np.mean(perm_scores) / base_score)\n",
    "\n",
    "    # Agrégation moyenne sur toutes les fenêtres\n",
    "    results = []\n",
    "    for col, ratios in var_imp.items():\n",
    "        if len(ratios) > 0:\n",
    "            results.append({\n",
    "                \"variable\": col,\n",
    "                \"perm_mae_ratio_mean\": np.mean(ratios),\n",
    "                \"perm_mae_ratio_std\": np.std(ratios),\n",
    "                \"n_windows\": len(ratios)\n",
    "            })\n",
    "\n",
    "    imp_df = pd.DataFrame(results).sort_values(\"perm_mae_ratio_mean\", ascending=False).reset_index(drop=True)\n",
    "    return imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54405b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 🔍 Importance par permutation (pseudo–OOS) ===\n",
      "       variable  perm_mae_ratio_mean  perm_mae_ratio_std  n_windows\n",
      "          USREC             1.092658            0.051073         28\n",
      "          TB3MS             1.028961            0.025485         28\n",
      "        S&P 500             1.013475            0.004537         28\n",
      "      OILPRICEx             1.004680            0.004453         28\n",
      "DPCERA3M086SBEA             1.000228            0.000298         28\n",
      "           M2SL             1.000208            0.000311         28\n",
      "         INDPRO             1.000093            0.001816         28\n",
      "       BUSLOANS             1.000056            0.000236         28\n",
      "            RPI             1.000051            0.000412         28\n",
      "       CPIAUCSL             1.000038            0.000150         28\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 🧭 Appel de la fonction sur ton jeu de modèles OLS\n",
    "# ----------------------------------------------------------\n",
    "perm_df = permutation_importance_pseudo_oos(\n",
    "    models=models,\n",
    "    df_train_global=df_train_global,\n",
    "    cols_tx=cols_tx,\n",
    "    h=h,\n",
    "    n_repeats=20  # augmente à 50 pour des résultats plus stables\n",
    ")\n",
    "\n",
    "print(\"\\n=== 🔍 Importance par permutation (pseudo–OOS) ===\")\n",
    "print(perm_df.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9a06ae",
   "metadata": {},
   "source": [
    "Ton modèle OLS prédit principalement le chômage via le cycle économique :\n",
    "- USREC (récession) est la variable clé — sa permutation dégrade la performance de ~9 %.\n",
    "- TB3MS (taux court) joue un rôle secondaire.\n",
    "- Les autres variables ont un effet négligeable.\n",
    "\n",
    "Conclusion : le pouvoir prédictif du modèle vient surtout des variables cycliques (récession, taux), les autres apportent peu d’information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cf89f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 💡 Importance SHAP (shares) ===\n",
      "       variable  shap_mean_abs  shap_share\n",
      "          TB3MS       0.148710    0.485751\n",
      "          USREC       0.124740    0.407453\n",
      "        S&P 500       0.017688    0.057777\n",
      "      OILPRICEx       0.012534    0.040941\n",
      "DPCERA3M086SBEA       0.001011    0.003303\n",
      "         INDPRO       0.000871    0.002845\n",
      "       CPIAUCSL       0.000309    0.001010\n",
      "           M2SL       0.000204    0.000667\n",
      "            RPI       0.000043    0.000139\n",
      "       BUSLOANS       0.000035    0.000114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Portofolio Data science\\Time Series\\Travaux pratiques Bases\\.venv\\Lib\\site-packages\\shap\\explainers\\_linear.py:99: FutureWarning: The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, maskers.Partition or maskers.Impute).\n",
      "  warnings.warn(wmsg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 💡 IMPORTANCE SHAPLEY (pour modèle OLS)\n",
    "# ==========================================================\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1️⃣ On utilise le dernier modèle entraîné\n",
    "model_final = models[-1]\n",
    "\n",
    "# 2️⃣ On reconstitue ses données finales (dernière fenêtre du train)\n",
    "df_final = df_train_global.copy()\n",
    "X_full = df_final[cols_tx].iloc[:-h].copy()\n",
    "Y_full = df_final[\"UNRATE\"].shift(-h).iloc[:-h].copy()\n",
    "valid = ~(X_full.isnull().any(axis=1) | Y_full.isnull())\n",
    "X_full = X_full.loc[valid]\n",
    "Y_full = Y_full.loc[valid]\n",
    "\n",
    "# 3️⃣ Calcul des valeurs SHAP\n",
    "# Pour les modèles linéaires, on peut utiliser shap.LinearExplainer (plus stable)\n",
    "explainer = shap.LinearExplainer(model_final, X_full, feature_perturbation=\"interventional\")\n",
    "shap_values = explainer(X_full)\n",
    "\n",
    "# 4️⃣ Importance moyenne absolue\n",
    "shap_df = pd.DataFrame({\n",
    "    \"variable\": X_full.columns,\n",
    "    \"shap_mean_abs\": np.abs(shap_values.values).mean(axis=0),\n",
    "})\n",
    "shap_df[\"shap_share\"] = shap_df[\"shap_mean_abs\"] / shap_df[\"shap_mean_abs\"].sum()\n",
    "shap_df = shap_df.sort_values(\"shap_mean_abs\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# 5️⃣ Affichage\n",
    "print(\"\\n=== 💡 Importance SHAP (shares) ===\")\n",
    "print(shap_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d47ec9c",
   "metadata": {},
   "source": [
    "- TB3MS (0.49)\t🟢 Représente ~49 % de l’influence totale du modèle. Le taux d’intérêt à 3 mois est donc la variable la plus déterminante pour les prévisions du chômage : quand les taux montent, le modèle anticipe souvent une hausse future du chômage.\n",
    "\n",
    "- USREC (0.41)\t🔵 Représente ~41 % de l’influence totale. Le dummy de récession (NBER) pèse presque autant : le simple fait d’être en récession ou non explique une large part des variations prévues du chômage.\n",
    "\n",
    "- S&P 500, OILPRICEx 🟠 Poids faibles (~6 % et 4 %) : les conditions boursières et le prix du pétrole ont un impact marginal dans la version linéaire du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db7b28b",
   "metadata": {},
   "source": [
    "# 📊 Métriques utilisées\n",
    "\n",
    "## 1) Performance globale\n",
    "\n",
    "**Coefficient de détermination (R²)**  \n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (y_i - \\bar{y})^2}\n",
    "$$  \n",
    "➡️ Part de la variance expliquée par le modèle (0 = pas mieux que la moyenne, 1 = parfait).\n",
    "\n",
    "**Erreur absolue moyenne (MAE)**  \n",
    "$$\n",
    "MAE = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i|\n",
    "$$  \n",
    "➡️ Écart absolu moyen entre valeurs réelles et prédites, robuste aux outliers.\n",
    "\n",
    "**Erreur quadratique moyenne (RMSE)**  \n",
    "$$\n",
    "RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n",
    "$$  \n",
    "➡️ Similaire au MAE mais pénalise davantage les grosses erreurs.\n",
    "\n",
    "**Corrélation de Pearson**  \n",
    "$$\n",
    "\\rho(y, \\hat{y}) = \\frac{\\text{Cov}(y, \\hat{y})}{\\sigma_y \\cdot \\sigma_{\\hat{y}}}\n",
    "$$  \n",
    "➡️ Mesure le degré de lien linéaire entre les prédictions et les observations.\n",
    "\n",
    "**Abs Mean Deviance (AMD)**  \n",
    "$$\n",
    "AMD = \\frac{1}{n}\\sum_{i=1}^n |\\hat{y}_i - \\bar{\\hat{y}}|\n",
    "$$  \n",
    "➡️ Écart moyen des prédictions par rapport à leur moyenne ; sert de référence pour la permutation prédiction-basée.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Importance par permutation\n",
    "La relation entre Y et X dépend du temps. Quand on perturbe la série X (en la mélangeant), on casse ce lien, et si l’erreur augmente, cela montre que X est une variable clé pour expliquer Y.\n",
    "\n",
    "**Ratio MAE**  \n",
    "$$\n",
    "PI^{MAE}_j = \\frac{MAE^{(perm)}_j}{MAE^{(base)}}\n",
    "$$  \n",
    "➡️ Si > 1, la variable est utile pour réduire l’erreur absolue.\n",
    "\n",
    "**Ratio RMSE**  \n",
    "$$\n",
    "PI^{RMSE}_j = \\frac{RMSE^{(perm)}_j}{RMSE^{(base)}}\n",
    "$$  \n",
    "➡️ Si > 1, la variable aide à limiter les grosses erreurs.\n",
    "\n",
    "**Déviance de prédiction**  \n",
    "$$\n",
    "PI^{dev}_j = \\frac{1}{n}\\sum_{i=1}^n \\big|\\hat{y}_i - \\hat{y}^{(perm)}_{i,j}\\big|\n",
    "$$  \n",
    "➡️ Mesure combien les prédictions changent quand on brouille une variable.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Importance Shapley\n",
    "\n",
    "**Décomposition des prédictions**  \n",
    "$$\n",
    "\\hat{y}_i = \\phi_0 + \\sum_{j=1}^p \\phi_{ij}\n",
    "$$  \n",
    "➡️ Chaque prédiction est expliquée par une contribution \\(\\phi_{ij}\\) par variable.\n",
    "\n",
    "**Importance absolue moyenne**  \n",
    "$$\n",
    "\\text{Mean}(|\\phi_j|) = \\frac{1}{n}\\sum_{i=1}^n |\\phi_{ij}|\n",
    "$$  \n",
    "➡️ Contribution moyenne (absolue) d’une variable sur toutes les prédictions.\n",
    "\n",
    "**Shapley share**  \n",
    "$$\n",
    "\\Gamma_j = \\frac{\\text{Mean}(|\\phi_j|)}{\\sum_{k=1}^p \\text{Mean}(|\\phi_k|)}\n",
    "$$  \n",
    "➡️ Part relative de la variable dans l’explication totale (somme des parts = 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a6535a",
   "metadata": {},
   "source": [
    "## Interprétation des résultats \n",
    "\n",
    "### Performance globale \n",
    "- R² = 0.2266 → modèle OLS explique ~23 % de la variance du chômage US.\n",
    "- MAE = 0.6774 → en moyenne, l’erreur absolue est de 0.68 points (dans l’unité de la variable cible).\n",
    "- RMSE = 0.8750 → un peu plus élevé que le MAE, ce qui indique la présence de grosses erreurs ponctuelles.\n",
    "- Corrélation = 0.4760 (p ≈ 10⁻³⁵) → lien positif et significatif entre prédictions et observations, mais seulement modéré. Ce qui peut expliquer la présence d'une relation \n",
    "- Abs Mean Deviance = 0.3370 → sert ici de référence pour l’importance prédiction-basée : les prédictions s’écartent en moyenne de 0.34 de leur propre moyenne.\n",
    "\n",
    "Lecture : le modèle OLS capte une partie utile du signal, mais laisse beaucoup de variance inexpliquée. La corrélation faible illustre éventuellement la présence des relations non-linéaire, et non captées par OLS.\n",
    "\n",
    "### 🔹 2. Importance par permutation\n",
    "- INDPRO (Industrial Production) : la plus influente. Sa permutation augmente MAE de +19 % et RMSE de +20 %, avec une forte déviance de prédiction (0.41).\n",
    "- TB3MS (Taux d’intérêt à 3 mois) : impact non négligeable, ratios ~1.02 et déviance ~0.10.\n",
    "- BUSLOANS (Prêts commerciaux) : rôle similaire (MAE ratio 1.017, déviance ~0.09).\n",
    "- S&P 500 : contribution modérée, ratios légèrement > 1.\n",
    "- RPI, M2SL : influence plus faible mais perceptible.\n",
    "- CPIAUCSL, OILPRICEx, DPCERA3M086SBEA : quasi neutres (ratios ≈ 1, déviance très faible).\n",
    "\n",
    "Lecture : INDPRO domine largement la performance, les autres apportent des compléments mais plus modestes.\n",
    "\n",
    "### 🔹 3. Importance Shapley (shares)\n",
    "- INDPRO : ~52 % de l’explication totale des prédictions → cohérence parfaite avec la permutation.\n",
    "- TB3MS (12 %) + BUSLOANS (12 %) : deux autres piliers importants.\n",
    "- S&P 500 (9,7 %) : contribue de façon notable.\n",
    "- M2SL (5 %), RPI (3 %), CPIAUCSL (3,5 %) : apports plus secondaires.\n",
    "- OILPRICEx et DPCERA3M086SBEA (<2 %) : quasi négligeables dans ce modèle.\n",
    "\n",
    "Lecture : INDPRO est la variable macroéconomique centrale, suivie par des indicateurs financiers (taux courts, prêts bancaires, marché actions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3b489",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662bb79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "saved_model = joblib.load(\"models/model_final.pkl\")\n",
    "model_final = saved_model[\"model\"]\n",
    "features = saved_model[\"features\"]\n",
    "winsor_level = saved_model[\"winsor_level\"]\n",
    "norm_var = saved_model[\"norm_var\"]\n",
    "mean_full = saved_model[\"mean_full\"]\n",
    "std_full = saved_model[\"std_full\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Travaux pratiques Bases)",
   "language": "python",
   "name": "venv-travaux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
