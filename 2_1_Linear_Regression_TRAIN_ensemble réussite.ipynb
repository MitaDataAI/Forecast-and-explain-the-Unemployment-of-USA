{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2478ba5b",
   "metadata": {},
   "source": [
    "# Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41027a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Portofolio Data science\\Time Series\\Travaux pratiques Bases\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import shap\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.utils import shuffle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22973a6a",
   "metadata": {},
   "source": [
    "# Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dedcdda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stationary_train = pd.read_csv(\"df_stationary_train.csv\", index_col=\"date\")\n",
    "df_stationary_test = pd.read_csv(\"df_stationary_test.csv\", index_col=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d91223b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNRATE</th>\n",
       "      <th>TB3MS</th>\n",
       "      <th>RPI</th>\n",
       "      <th>INDPRO</th>\n",
       "      <th>DPCERA3M086SBEA</th>\n",
       "      <th>S&amp;P 500</th>\n",
       "      <th>BUSLOANS</th>\n",
       "      <th>CPIAUCSL</th>\n",
       "      <th>OILPRICEx</th>\n",
       "      <th>M2SL</th>\n",
       "      <th>USREC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1960-01-01</th>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.020977</td>\n",
       "      <td>0.091980</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.017909</td>\n",
       "      <td>0.011578</td>\n",
       "      <td>-0.006156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960-02-01</th>\n",
       "      <td>-1.1</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.014565</td>\n",
       "      <td>0.076964</td>\n",
       "      <td>0.006009</td>\n",
       "      <td>-0.025663</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>-0.003767</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960-03-01</th>\n",
       "      <td>-0.2</td>\n",
       "      <td>-1.18</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.007961</td>\n",
       "      <td>0.021240</td>\n",
       "      <td>-0.070857</td>\n",
       "      <td>-0.008356</td>\n",
       "      <td>-0.005455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960-04-01</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.12</td>\n",
       "      <td>0.006489</td>\n",
       "      <td>-0.025915</td>\n",
       "      <td>0.033752</td>\n",
       "      <td>-0.040442</td>\n",
       "      <td>-0.009098</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960-05-01</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>0.007747</td>\n",
       "      <td>-0.018121</td>\n",
       "      <td>0.009040</td>\n",
       "      <td>-0.010090</td>\n",
       "      <td>-0.000359</td>\n",
       "      <td>0.003383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            UNRATE  TB3MS       RPI    INDPRO  DPCERA3M086SBEA   S&P 500  \\\n",
       "date                                                                       \n",
       "1960-01-01    -0.8   0.30  0.020977  0.091980         0.001204  0.017909   \n",
       "1960-02-01    -1.1  -0.19  0.014565  0.076964         0.006009 -0.025663   \n",
       "1960-03-01    -0.2  -1.18  0.006250  0.007961         0.021240 -0.070857   \n",
       "1960-04-01     0.0  -1.12  0.006489 -0.025915         0.033752 -0.040442   \n",
       "1960-05-01     0.0  -0.67  0.007747 -0.018121         0.009040 -0.010090   \n",
       "\n",
       "            BUSLOANS  CPIAUCSL  OILPRICEx      M2SL  USREC  \n",
       "date                                                        \n",
       "1960-01-01  0.011578 -0.006156        0.0  0.001323      0  \n",
       "1960-02-01  0.011905 -0.003767        0.0  0.002007      0  \n",
       "1960-03-01 -0.008356 -0.005455        0.0  0.001324      0  \n",
       "1960-04-01 -0.009098  0.005090        0.0  0.000634      1  \n",
       "1960-05-01 -0.000359  0.003383        0.0  0.003977      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stationary_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aff52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stationary_train.index = pd.to_datetime(df_stationary_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58729f82",
   "metadata": {},
   "source": [
    "# stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c1c02",
   "metadata": {},
   "source": [
    "# 1) Paramètres & sous-ensemble TRAIN global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "081d728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# ---------- Paramètres ----------\n",
    "h = 12                     # horizon de prévision (mois)\n",
    "step_size = 12             # refit annuel\n",
    "winsor_level = 0.01\n",
    "df_full = df_stationary_train.copy()  # index mensuel trié\n",
    "df_train_global = df_full.loc[\"1960-01\":\"1989-12\"].copy()\n",
    "\n",
    "# ✅ Inclure toutes les features (aucune exclusion)\n",
    "cols_tx = [c for c in df_train_global.columns if c != \"UNRATE\"]\n",
    "\n",
    "# ---------- Containers ----------\n",
    "models = []\n",
    "coefs = []\n",
    "intercepts = []\n",
    "train_periods = []\n",
    "forecast_records = []  # (origin, target, y_true, y_hat, n_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "195f861d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] Fin 1961-12 | train=12 | ŷ(1962-12) = 2.466\n",
      "[02] Fin 1962-12 | train=24 | ŷ(1963-12) = -0.489\n",
      "[03] Fin 1963-12 | train=36 | ŷ(1964-12) = -0.571\n",
      "[04] Fin 1964-12 | train=48 | ŷ(1965-12) = -0.928\n",
      "[05] Fin 1965-12 | train=60 | ŷ(1966-12) = -0.478\n",
      "[06] Fin 1966-12 | train=72 | ŷ(1967-12) = -0.412\n",
      "[07] Fin 1967-12 | train=84 | ŷ(1968-12) = -0.704\n",
      "[08] Fin 1968-12 | train=96 | ŷ(1969-12) = -0.869\n",
      "[09] Fin 1969-12 | train=108 | ŷ(1970-12) = 0.650\n",
      "[10] Fin 1970-12 | train=120 | ŷ(1971-12) = 0.780\n",
      "[11] Fin 1971-12 | train=132 | ŷ(1972-12) = -0.210\n",
      "[12] Fin 1972-12 | train=144 | ŷ(1973-12) = -0.908\n",
      "[13] Fin 1973-12 | train=156 | ŷ(1974-12) = 1.567\n",
      "[14] Fin 1974-12 | train=168 | ŷ(1975-12) = 2.097\n",
      "[15] Fin 1975-12 | train=180 | ŷ(1976-12) = -0.305\n",
      "[16] Fin 1976-12 | train=192 | ŷ(1977-12) = -0.194\n",
      "[17] Fin 1977-12 | train=204 | ŷ(1978-12) = -0.156\n",
      "[18] Fin 1978-12 | train=216 | ŷ(1979-12) = 0.318\n",
      "[19] Fin 1979-12 | train=228 | ŷ(1980-12) = 0.462\n",
      "[20] Fin 1980-12 | train=240 | ŷ(1981-12) = -0.153\n",
      "[21] Fin 1981-12 | train=252 | ŷ(1982-12) = 0.133\n",
      "[22] Fin 1982-12 | train=264 | ŷ(1983-12) = -0.859\n",
      "[23] Fin 1983-12 | train=276 | ŷ(1984-12) = -0.450\n",
      "[24] Fin 1984-12 | train=288 | ŷ(1985-12) = -0.985\n",
      "[25] Fin 1985-12 | train=300 | ŷ(1986-12) = -0.994\n",
      "[26] Fin 1986-12 | train=312 | ŷ(1987-12) = -0.011\n",
      "[27] Fin 1987-12 | train=324 | ŷ(1988-12) = 0.247\n",
      "[28] Fin 1988-12 | train=336 | ŷ(1989-12) = 0.087\n",
      "[29] Fin 1989-12 | train=348 | ŷ(NA) = -0.103\n"
     ]
    }
   ],
   "source": [
    "# ---------- Boucle EXPANDING WINDOW ----------\n",
    "n_total = len(df_train_global)\n",
    "for t, end in enumerate(range(step_size, n_total + 1, step_size)):\n",
    "    df_train_local = df_train_global.iloc[:end].copy()   # ✅ corrige .iloc et .copy()\n",
    "\n",
    "    # Assez d'obs pour aligner X_t avec Y_{t+h} ?\n",
    "    if len(df_train_local) <= h:\n",
    "        continue\n",
    "\n",
    "    # X_train : jusqu'à t_end - h ; Y_train : UNRATE décalé de -h sur la même plage\n",
    "    X_train = df_train_local[cols_tx].iloc[:-h].copy()\n",
    "    Y_train = df_train_local[\"UNRATE\"].shift(-h).iloc[:-h].copy()\n",
    "\n",
    "    # Retire lignes avec NaN éventuels\n",
    "    valid = ~(X_train.isnull().any(axis=1) | Y_train.isnull())\n",
    "    X_train = X_train.loc[valid]\n",
    "    Y_train = Y_train.loc[valid]\n",
    "\n",
    "    # 1️⃣ Winsorisation (1% - 99%) apprise sur la fenêtre courante\n",
    "    lower_wins = X_train.quantile(winsor_level)\n",
    "    upper_wins = X_train.quantile(1 - winsor_level)\n",
    "    Xw = X_train.clip(lower=lower_wins, upper=upper_wins, axis=1)\n",
    "\n",
    "    # 2️⃣ Normalisation (toujours activée)\n",
    "    mean_train = Xw.mean()\n",
    "    std_train = Xw.std().replace(0, 1)\n",
    "    Xs = (Xw - mean_train) / std_train\n",
    "\n",
    "    # 3️⃣ Entraînement OLS\n",
    "    model = LinearRegression()\n",
    "    model.fit(Xs, Y_train)\n",
    "\n",
    "    # 4️⃣ Prévision pseudo-OOS à l'origine t_end : ŷ_{t_end+h}\n",
    "    #    -> on prend la dernière observation dispo (t_end) côté X\n",
    "    x_origin = df_train_local[cols_tx].iloc[[-1]].copy()\n",
    "    x_origin_w = x_origin.clip(lower=lower_wins, upper=upper_wins, axis=1)\n",
    "    x_origin_s = (x_origin_w - mean_train) / std_train\n",
    "    y_hat = float(model.predict(x_origin_s)[0])\n",
    "\n",
    "    origin_date = df_train_local.index[-1]\n",
    "    # date cible = origin + h si dispo dans df_full\n",
    "    idx_origin = df_full.index.get_loc(origin_date)\n",
    "    target_date = df_full.index[idx_origin + h] if idx_origin + h < len(df_full) else pd.NaT\n",
    "    y_true = float(df_full.loc[target_date, \"UNRATE\"]) if pd.notna(target_date) else np.nan\n",
    "\n",
    "    # 5️⃣ Sauvegarde\n",
    "    models.append(model)\n",
    "    coefs.append(pd.Series(model.coef_, index=cols_tx))\n",
    "    intercepts.append(model.intercept_)\n",
    "    train_periods.append(origin_date)\n",
    "    forecast_records.append({\n",
    "        \"origin\": origin_date, \"target\": target_date,\n",
    "        \"y_true\": y_true, \"y_hat\": y_hat, \"n_train\": len(Xs)\n",
    "    })\n",
    "\n",
    "    print(f\"[{t:02d}] Fin {origin_date.strftime('%Y-%m')} | train={len(Xs)} | \"\n",
    "          f\"ŷ({('NA' if pd.isna(target_date) else target_date.strftime('%Y-%m'))}) = {y_hat:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6836860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "for t, end in enumerate(range(step_size, n_total + 1, step_size)):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efd2fb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ÉVALUATION PSEUDO–OOS (h = 12 mois) ===\n",
      "R²            : 0.095\n",
      "R² (origin)   : 0.205\n",
      "MAE           : 0.809\n",
      "RMSE          : 1.058\n",
      "Corr(y, ŷ)   : 0.453  (p=0.0154)\n",
      "Hit rate sign : 0.714\n",
      "AMD (|bias|)  : 0.010\n",
      "\n",
      "--- Benchmark naïf (zéro-changement) ---\n",
      "MAE_naïf0     : 0.800\n",
      "RMSE_naïf0    : 1.113\n",
      "\n",
      "--- MAE/RMSE par décennie ---\n",
      " decade    n      MAE     RMSE\n",
      "   1960  8.0 0.695125 1.136080\n",
      "   1970 10.0 0.772075 0.930614\n",
      "   1980 10.0 0.937675 1.112530\n",
      "\n",
      "✅ Évaluation terminée. Résultats enregistrés dans eval_results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mita\\AppData\\Local\\Temp\\ipykernel_14108\\1481986021.py:60: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "# ===================== ÉVALUATION PSEUDO–OUT-OF-SAMPLE =====================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# 1️⃣ Rassembler les prévisions\n",
    "forecast_df = pd.DataFrame(forecast_records).dropna(subset=[\"y_true\", \"y_hat\"]).copy()\n",
    "forecast_df = forecast_df.sort_values(\"target\").reset_index(drop=True)\n",
    "\n",
    "def r2_origin_reg(y, yhat):\n",
    "    \"\"\"R² d'une régression à l’origine: y ≈ b * yhat (sans intercept).\"\"\"\n",
    "    denom = np.dot(yhat, yhat)\n",
    "    if denom == 0:\n",
    "        return np.nan\n",
    "    b = np.dot(yhat, y) / denom\n",
    "    sse = np.sum((y - b * yhat) ** 2)\n",
    "    sst = np.sum((y - y.mean()) ** 2)\n",
    "    return 1 - sse / sst if sst > 0 else np.nan\n",
    "\n",
    "def corr_pvalue(y, yhat):\n",
    "    if len(y) < 3:\n",
    "        return np.nan, np.nan\n",
    "    r, p = pearsonr(y, yhat)\n",
    "    return float(r), float(p)\n",
    "\n",
    "if forecast_df.empty:\n",
    "    print(\"\\n[ÉVALUATION] Aucune prévision disponible (forecast_df est vide).\")\n",
    "else:\n",
    "    y = forecast_df[\"y_true\"].values\n",
    "    yhat = forecast_df[\"y_hat\"].values\n",
    "\n",
    "    # 2️⃣ Métriques principales\n",
    "    r2   = r2_score(y, yhat)\n",
    "    mae  = mean_absolute_error(y, yhat)\n",
    "    rmse = np.sqrt(mean_squared_error(y, yhat))   # ✅ compatible toutes versions\n",
    "    r2_o = r2_origin_reg(y, yhat)\n",
    "    corr, pval = corr_pvalue(y, yhat)\n",
    "    amd  = float(abs(np.mean(y - yhat)))          # biais absolu moyen\n",
    "\n",
    "    # 3️⃣ Benchmark naïf (utile si la cible est Δ12 UNRATE)\n",
    "    yhat_naive0 = np.zeros_like(y)\n",
    "    mae_naive0  = mean_absolute_error(y, yhat_naive0)\n",
    "    rmse_naive0 = np.sqrt(mean_squared_error(y, yhat_naive0))\n",
    "\n",
    "    # 4️⃣ Précision directionnelle\n",
    "    hit_rate = float(np.mean(np.sign(y) == np.sign(yhat))) if len(y) > 0 else np.nan\n",
    "\n",
    "    # 5️⃣ Calibration (Mincer–Zarnowitz): y = a + b * yhat\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(yhat.reshape(-1, 1), y)\n",
    "    a_calib = float(reg.intercept_)\n",
    "    b_calib = float(reg.coef_[0])\n",
    "\n",
    "    # 6️⃣ Résumé par décennie (facultatif)\n",
    "    by_decade = (\n",
    "        forecast_df.assign(decade=forecast_df[\"target\"].dt.year // 10 * 10)\n",
    "                   .groupby(\"decade\")\n",
    "                   .apply(lambda g: pd.Series({\n",
    "                       \"n\": len(g),\n",
    "                       \"MAE\": mean_absolute_error(g[\"y_true\"], g[\"y_hat\"]),\n",
    "                       \"RMSE\": np.sqrt(mean_squared_error(g[\"y_true\"], g[\"y_hat\"]))\n",
    "                   }))\n",
    "                   .reset_index()\n",
    "    )\n",
    "\n",
    "    # 7️⃣ Impression des résultats\n",
    "    print(\"\\n=== ÉVALUATION PSEUDO–OOS (h = {} mois) ===\".format(h))\n",
    "    print(f\"R²            : {r2:.3f}\")\n",
    "    print(f\"R² (origin)   : {r2_o:.3f}\")\n",
    "    print(f\"MAE           : {mae:.3f}\")\n",
    "    print(f\"RMSE          : {rmse:.3f}\")\n",
    "    print(f\"Corr(y, ŷ)   : {corr:.3f}  (p={pval:.3g})\")\n",
    "    print(f\"Hit rate sign : {hit_rate:.3f}\")\n",
    "    print(f\"AMD (|bias|)  : {amd:.3f}\")\n",
    "\n",
    "    print(\"\\n--- Benchmark naïf (zéro-changement) ---\")\n",
    "    print(f\"MAE_naïf0     : {mae_naive0:.3f}\")\n",
    "    print(f\"RMSE_naïf0    : {rmse_naive0:.3f}\")\n",
    "\n",
    "    if not by_decade.empty:\n",
    "        print(\"\\n--- MAE/RMSE par décennie ---\")\n",
    "        print(by_decade.to_string(index=False))\n",
    "\n",
    "    # 8️⃣ Sauvegarde des résultats\n",
    "    eval_results = {\n",
    "        \"overall\": {\n",
    "            \"r2\": r2, \"r2_origin\": r2_o, \"mae\": mae, \"rmse\": rmse,\n",
    "            \"corr\": corr, \"pval\": pval, \"hit_rate\": hit_rate, \"amd\": amd\n",
    "        },\n",
    "        \"benchmark_naive0\": {\n",
    "            \"mae\": mae_naive0, \"rmse\": rmse_naive0\n",
    "        },\n",
    "        \"calibration\": {\n",
    "            \"intercept\": a_calib, \"slope\": b_calib\n",
    "        },\n",
    "        \"by_decade\": by_decade,\n",
    "        \"forecast_df\": forecast_df\n",
    "    }\n",
    "\n",
    "    print(\"\\n✅ Évaluation terminée. Résultats enregistrés dans eval_results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ad858c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 🔍 IMPORTANCE PAR PERMUTATION — PSEUDO-OOS\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def permutation_importance_pseudo_oos(models, df_train_global, cols_tx, h=12, n_repeats=20, metric=mean_absolute_error):\n",
    "    \"\"\"\n",
    "    Importance par permutation pour une série de modèles OLS entraînés\n",
    "    selon une logique expanding window pseudo–OOS.\n",
    "    -> aucune réestimation\n",
    "    -> mesure la dégradation moyenne de la performance après permutation de chaque variable\n",
    "    \"\"\"\n",
    "    var_imp = {col: [] for col in cols_tx}\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        # Reconstituer la fenêtre utilisée par le modèle i\n",
    "        end_idx = (i + 1) * 12  # correspond à ton step_size = 12\n",
    "        df_win = df_train_global.iloc[:end_idx].copy()\n",
    "\n",
    "        if len(df_win) <= h:\n",
    "            continue\n",
    "\n",
    "        # Préparation des données (alignement X_t avec Y_{t+h})\n",
    "        X = df_win[cols_tx].iloc[:-h].copy()\n",
    "        y = df_win[\"UNRATE\"].shift(-h).iloc[:-h].copy()\n",
    "        valid = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "        X, y = X.loc[valid], y.loc[valid]\n",
    "\n",
    "        # Score de base (MAE sur données d'origine)\n",
    "        base_score = metric(y, model.predict(X))\n",
    "\n",
    "        # Boucle sur chaque variable\n",
    "        for col in cols_tx:\n",
    "            perm_scores = []\n",
    "            for _ in range(n_repeats):\n",
    "                X_perm = X.copy()\n",
    "                X_perm[col] = np.random.permutation(X_perm[col])\n",
    "                perm_scores.append(metric(y, model.predict(X_perm)))\n",
    "            perm_scores = np.array(perm_scores)\n",
    "            var_imp[col].append(np.mean(perm_scores) / base_score)\n",
    "\n",
    "    # Agrégation moyenne sur toutes les fenêtres\n",
    "    results = []\n",
    "    for col, ratios in var_imp.items():\n",
    "        if len(ratios) > 0:\n",
    "            results.append({\n",
    "                \"variable\": col,\n",
    "                \"perm_mae_ratio_mean\": np.mean(ratios),\n",
    "                \"perm_mae_ratio_std\": np.std(ratios),\n",
    "                \"n_windows\": len(ratios)\n",
    "            })\n",
    "\n",
    "    imp_df = pd.DataFrame(results).sort_values(\"perm_mae_ratio_mean\", ascending=False).reset_index(drop=True)\n",
    "    return imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa54405b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 🔍 Importance par permutation (pseudo–OOS) ===\n",
      "       variable  perm_mae_ratio_mean  perm_mae_ratio_std  n_windows\n",
      "          USREC             1.089988            0.055342         28\n",
      "          TB3MS             1.028684            0.025290         28\n",
      "        S&P 500             1.013657            0.004299         28\n",
      "      OILPRICEx             1.004552            0.004326         28\n",
      "DPCERA3M086SBEA             1.000241            0.000357         28\n",
      "           M2SL             1.000235            0.000371         28\n",
      "            RPI             1.000066            0.000359         28\n",
      "       BUSLOANS             1.000058            0.000205         28\n",
      "         INDPRO             1.000039            0.002126         28\n",
      "       CPIAUCSL             1.000038            0.000153         28\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 🧭 Appel de la fonction sur ton jeu de modèles OLS\n",
    "# ----------------------------------------------------------\n",
    "perm_df = permutation_importance_pseudo_oos(\n",
    "    models=models,\n",
    "    df_train_global=df_train_global,\n",
    "    cols_tx=cols_tx,\n",
    "    h=h,\n",
    "    n_repeats=20  # augmente à 50 pour des résultats plus stables\n",
    ")\n",
    "\n",
    "print(\"\\n=== 🔍 Importance par permutation (pseudo–OOS) ===\")\n",
    "print(perm_df.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9a06ae",
   "metadata": {},
   "source": [
    "Ton modèle OLS prédit principalement le chômage via le cycle économique :\n",
    "- USREC (récession) est la variable clé — sa permutation dégrade la performance de ~9 %.\n",
    "- TB3MS (taux court) joue un rôle secondaire.\n",
    "- Les autres variables ont un effet négligeable.\n",
    "\n",
    "Conclusion : le pouvoir prédictif du modèle vient surtout des variables cycliques (récession, taux), les autres apportent peu d’information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3cf89f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 💡 Importance SHAP (shares) ===\n",
      "       variable  shap_mean_abs  shap_share\n",
      "          TB3MS       0.148710    0.485751\n",
      "          USREC       0.124740    0.407453\n",
      "        S&P 500       0.017688    0.057777\n",
      "      OILPRICEx       0.012534    0.040941\n",
      "DPCERA3M086SBEA       0.001011    0.003303\n",
      "         INDPRO       0.000871    0.002845\n",
      "       CPIAUCSL       0.000309    0.001010\n",
      "           M2SL       0.000204    0.000667\n",
      "            RPI       0.000043    0.000139\n",
      "       BUSLOANS       0.000035    0.000114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Portofolio Data science\\Time Series\\Travaux pratiques Bases\\.venv\\Lib\\site-packages\\shap\\explainers\\_linear.py:99: FutureWarning: The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, maskers.Partition or maskers.Impute).\n",
      "  warnings.warn(wmsg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 💡 IMPORTANCE SHAPLEY (pour modèle OLS)\n",
    "# ==========================================================\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1️⃣ On utilise le dernier modèle entraîné\n",
    "model_final = models[-1]\n",
    "\n",
    "# 2️⃣ On reconstitue ses données finales (dernière fenêtre du train)\n",
    "df_final = df_train_global.copy()\n",
    "X_full = df_final[cols_tx].iloc[:-h].copy()\n",
    "Y_full = df_final[\"UNRATE\"].shift(-h).iloc[:-h].copy()\n",
    "valid = ~(X_full.isnull().any(axis=1) | Y_full.isnull())\n",
    "X_full = X_full.loc[valid]\n",
    "Y_full = Y_full.loc[valid]\n",
    "\n",
    "# 3️⃣ Calcul des valeurs SHAP\n",
    "# Pour les modèles linéaires, on peut utiliser shap.LinearExplainer (plus stable)\n",
    "explainer = shap.LinearExplainer(model_final, X_full, feature_perturbation=\"interventional\")\n",
    "shap_values = explainer(X_full)\n",
    "\n",
    "# 4️⃣ Importance moyenne absolue\n",
    "shap_df = pd.DataFrame({\n",
    "    \"variable\": X_full.columns,\n",
    "    \"shap_mean_abs\": np.abs(shap_values.values).mean(axis=0),\n",
    "})\n",
    "shap_df[\"shap_share\"] = shap_df[\"shap_mean_abs\"] / shap_df[\"shap_mean_abs\"].sum()\n",
    "shap_df = shap_df.sort_values(\"shap_mean_abs\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# 5️⃣ Affichage\n",
    "print(\"\\n=== 💡 Importance SHAP (shares) ===\")\n",
    "print(shap_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d47ec9c",
   "metadata": {},
   "source": [
    "- TB3MS (0.49)\t🟢 Représente ~49 % de l’influence totale du modèle. Le taux d’intérêt à 3 mois est donc la variable la plus déterminante pour les prévisions du chômage : quand les taux montent, le modèle anticipe souvent une hausse future du chômage.\n",
    "\n",
    "- USREC (0.41)\t🔵 Représente ~41 % de l’influence totale. Le dummy de récession (NBER) pèse presque autant : le simple fait d’être en récession ou non explique une large part des variations prévues du chômage.\n",
    "\n",
    "- S&P 500, OILPRICEx 🟠 Poids faibles (~6 % et 4 %) : les conditions boursières et le prix du pétrole ont un impact marginal dans la version linéaire du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db7b28b",
   "metadata": {},
   "source": [
    "# 📊 Métriques utilisées\n",
    "\n",
    "## 1) Performance globale\n",
    "\n",
    "**Coefficient de détermination (R²)**  \n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (y_i - \\bar{y})^2}\n",
    "$$  \n",
    "➡️ Part de la variance expliquée par le modèle (0 = pas mieux que la moyenne, 1 = parfait).\n",
    "\n",
    "**Erreur absolue moyenne (MAE)**  \n",
    "$$\n",
    "MAE = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i|\n",
    "$$  \n",
    "➡️ Écart absolu moyen entre valeurs réelles et prédites, robuste aux outliers.\n",
    "\n",
    "**Erreur quadratique moyenne (RMSE)**  \n",
    "$$\n",
    "RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n",
    "$$  \n",
    "➡️ Similaire au MAE mais pénalise davantage les grosses erreurs.\n",
    "\n",
    "**Corrélation de Pearson**  \n",
    "$$\n",
    "\\rho(y, \\hat{y}) = \\frac{\\text{Cov}(y, \\hat{y})}{\\sigma_y \\cdot \\sigma_{\\hat{y}}}\n",
    "$$  \n",
    "➡️ Mesure le degré de lien linéaire entre les prédictions et les observations.\n",
    "\n",
    "**Abs Mean Deviance (AMD)**  \n",
    "$$\n",
    "AMD = \\frac{1}{n}\\sum_{i=1}^n |\\hat{y}_i - \\bar{\\hat{y}}|\n",
    "$$  \n",
    "➡️ Écart moyen des prédictions par rapport à leur moyenne ; sert de référence pour la permutation prédiction-basée.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Importance par permutation\n",
    "La relation entre Y et X dépend du temps. Quand on perturbe la série X (en la mélangeant), on casse ce lien, et si l’erreur augmente, cela montre que X est une variable clé pour expliquer Y.\n",
    "\n",
    "**Ratio MAE**  \n",
    "$$\n",
    "PI^{MAE}_j = \\frac{MAE^{(perm)}_j}{MAE^{(base)}}\n",
    "$$  \n",
    "➡️ Si > 1, la variable est utile pour réduire l’erreur absolue.\n",
    "\n",
    "**Ratio RMSE**  \n",
    "$$\n",
    "PI^{RMSE}_j = \\frac{RMSE^{(perm)}_j}{RMSE^{(base)}}\n",
    "$$  \n",
    "➡️ Si > 1, la variable aide à limiter les grosses erreurs.\n",
    "\n",
    "**Déviance de prédiction**  \n",
    "$$\n",
    "PI^{dev}_j = \\frac{1}{n}\\sum_{i=1}^n \\big|\\hat{y}_i - \\hat{y}^{(perm)}_{i,j}\\big|\n",
    "$$  \n",
    "➡️ Mesure combien les prédictions changent quand on brouille une variable.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Importance Shapley\n",
    "\n",
    "**Décomposition des prédictions**  \n",
    "$$\n",
    "\\hat{y}_i = \\phi_0 + \\sum_{j=1}^p \\phi_{ij}\n",
    "$$  \n",
    "➡️ Chaque prédiction est expliquée par une contribution \\(\\phi_{ij}\\) par variable.\n",
    "\n",
    "**Importance absolue moyenne**  \n",
    "$$\n",
    "\\text{Mean}(|\\phi_j|) = \\frac{1}{n}\\sum_{i=1}^n |\\phi_{ij}|\n",
    "$$  \n",
    "➡️ Contribution moyenne (absolue) d’une variable sur toutes les prédictions.\n",
    "\n",
    "**Shapley share**  \n",
    "$$\n",
    "\\Gamma_j = \\frac{\\text{Mean}(|\\phi_j|)}{\\sum_{k=1}^p \\text{Mean}(|\\phi_k|)}\n",
    "$$  \n",
    "➡️ Part relative de la variable dans l’explication totale (somme des parts = 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a6535a",
   "metadata": {},
   "source": [
    "## Interprétation des résultats \n",
    "\n",
    "### Performance globale \n",
    "- R² = 0.2266 → modèle OLS explique ~23 % de la variance du chômage US.\n",
    "- MAE = 0.6774 → en moyenne, l’erreur absolue est de 0.68 points (dans l’unité de la variable cible).\n",
    "- RMSE = 0.8750 → un peu plus élevé que le MAE, ce qui indique la présence de grosses erreurs ponctuelles.\n",
    "- Corrélation = 0.4760 (p ≈ 10⁻³⁵) → lien positif et significatif entre prédictions et observations, mais seulement modéré. Ce qui peut expliquer la présence d'une relation \n",
    "- Abs Mean Deviance = 0.3370 → sert ici de référence pour l’importance prédiction-basée : les prédictions s’écartent en moyenne de 0.34 de leur propre moyenne.\n",
    "\n",
    "Lecture : le modèle OLS capte une partie utile du signal, mais laisse beaucoup de variance inexpliquée. La corrélation faible illustre éventuellement la présence des relations non-linéaire, et non captées par OLS.\n",
    "\n",
    "### 🔹 2. Importance par permutation\n",
    "- INDPRO (Industrial Production) : la plus influente. Sa permutation augmente MAE de +19 % et RMSE de +20 %, avec une forte déviance de prédiction (0.41).\n",
    "- TB3MS (Taux d’intérêt à 3 mois) : impact non négligeable, ratios ~1.02 et déviance ~0.10.\n",
    "- BUSLOANS (Prêts commerciaux) : rôle similaire (MAE ratio 1.017, déviance ~0.09).\n",
    "- S&P 500 : contribution modérée, ratios légèrement > 1.\n",
    "- RPI, M2SL : influence plus faible mais perceptible.\n",
    "- CPIAUCSL, OILPRICEx, DPCERA3M086SBEA : quasi neutres (ratios ≈ 1, déviance très faible).\n",
    "\n",
    "Lecture : INDPRO domine largement la performance, les autres apportent des compléments mais plus modestes.\n",
    "\n",
    "### 🔹 3. Importance Shapley (shares)\n",
    "- INDPRO : ~52 % de l’explication totale des prédictions → cohérence parfaite avec la permutation.\n",
    "- TB3MS (12 %) + BUSLOANS (12 %) : deux autres piliers importants.\n",
    "- S&P 500 (9,7 %) : contribue de façon notable.\n",
    "- M2SL (5 %), RPI (3 %), CPIAUCSL (3,5 %) : apports plus secondaires.\n",
    "- OILPRICEx et DPCERA3M086SBEA (<2 %) : quasi négligeables dans ce modèle.\n",
    "\n",
    "Lecture : INDPRO est la variable macroéconomique centrale, suivie par des indicateurs financiers (taux courts, prêts bancaires, marché actions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3b489",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "662bb79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modèle 'OLS_h12_expanding_window' sauvegardé sous 'OLS_h12_expanding_window.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# 🔖 Nom du modèle\n",
    "model_name = \"OLS_h12_expanding_window\"\n",
    "\n",
    "# 🔹 Rassembler tout dans un dictionnaire structuré\n",
    "exp_results = {\n",
    "    \"model_name\": model_name,\n",
    "    \"model_type\": \"LinearRegression (OLS)\",\n",
    "    \"description\": \"Modèle OLS avec fenêtre expanding, horizon h=12 mois.\",\n",
    "    \"models\": models,\n",
    "    \"coefs\": coefs,\n",
    "    \"intercepts\": intercepts,\n",
    "    \"train_periods\": train_periods,\n",
    "    \"forecast_records\": forecast_records,\n",
    "    \"params\": {\n",
    "        \"h\": h,\n",
    "        \"step_size\": step_size,\n",
    "        \"winsor_level\": winsor_level,\n",
    "        \"norm_var\": True,\n",
    "        \"features\": cols_tx\n",
    "    }\n",
    "}\n",
    "\n",
    "# 🔸 Nom du fichier de sortie (automatique)\n",
    "file_name = f\"{model_name}.pkl\"\n",
    "\n",
    "# 💾 Sauvegarde\n",
    "with open(file_name, \"wb\") as f:\n",
    "    pickle.dump(exp_results, f)\n",
    "\n",
    "print(f\"✅ Modèle '{model_name}' sauvegardé sous '{file_name}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Travaux pratiques Bases)",
   "language": "python",
   "name": "venv-travaux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
