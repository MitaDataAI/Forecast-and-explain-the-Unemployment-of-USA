{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c37aae7",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b8aea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d8d201",
   "metadata": {},
   "source": [
    "# Importation des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78c5c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stationary = pd.read_csv(\"df_stationary.csv\", index_col=\"date\")\n",
    "df_stationary_test = pd.read_csv(\"df_stationary_test.csv\", index_col=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c150f38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['UNRATE', 'TB3MS', 'RPI', 'INDPRO', 'DPCERA3M086SBEA', 'S&P500',\n",
       "       'BUSLOANS', 'CPIAUCSL', 'OILPRICEx', 'M2SL', 'USREC'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stationary.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91cc0a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['UNRATE', 'TB3MS', 'RPI', 'INDPRO', 'DPCERA3M086SBEA', 'S&P500',\n",
       "       'BUSLOANS', 'CPIAUCSL', 'OILPRICEx', 'M2SL', 'USREC'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stationary_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944583e",
   "metadata": {},
   "source": [
    "# RIDGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37bbdc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Donn√©es pr√™tes : 1960-01-01 ‚Üí 2025-08-01 | n=788 | freq=MS\n",
      "Features (10): ['TB3MS', 'RPI', 'INDPRO', 'DPCERA3M086SBEA', 'S&P500', 'BUSLOANS'] ...\n",
      "\n",
      "‚úÖ Pseudo-OOS (Ridge) termin√© ‚Äî n pr√©visions = 741\n",
      "              y_pred  y_true  y_pred_base  y_pred_p05  y_pred_p95\n",
      "date                                                             \n",
      "1963-12-01  0.093785     0.0    -0.181263   -0.455216    1.220200\n",
      "1964-01-01  0.348080    -0.1    -0.071449   -0.302035    1.165286\n",
      "1964-02-01  0.390935    -0.5     0.058525   -0.508924    1.117637\n",
      "\n",
      "üìä Validation 83‚Äì89 ‚Äî n=84 | MAE=0.819 | RMSE=1.025 | R¬≤=-0.346\n",
      "üìä Test 90‚Äì2025 ‚Äî n=428 | MAE=0.821 | RMSE=1.464 | R¬≤=0.080\n",
      "‚û°Ô∏è  Gain bagging (ŒîMAE) = -0.012\n",
      "\n",
      "üíæ Bundle sauvegard√© ‚Üí ridge_regression.pkl\n",
      "üíæ M√©ta sauvegard√©e ‚Üí ridge_regression_meta.csv\n",
      "üì¶ Contenu du bundle : ['oos_predictions', 'params', 'meta', 'train_fit_dates', 'models', 'preprocs']\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# üîπ Ridge Regression + Bagging (pseudo-OOS)\n",
    "#    ‚Üí m√™me structure/artefacts que la version LinearRegression\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# ---------- Param√®tres g√©n√©raux ----------\n",
    "h = 12\n",
    "min_train_n = 36           # ‚â• 3 ans avant de commencer √† pr√©voir\n",
    "winsor_level = 0.01        # winsorisation (1er/99e percentiles)\n",
    "norm_var = True            # normaliser ou non\n",
    "target_col = \"UNRATE\"      # cible dans df_stationary\n",
    "\n",
    "# S√©lection d'alpha : \"cv\" (CV 5-fold) OU une valeur float (ex: 1.0)\n",
    "alpha_mode = \"cv\"\n",
    "alpha_grid = np.logspace(-4, 4, 30)   # utilis√© si alpha_mode=\"cv\"\n",
    "\n",
    "# Fen√™tres d'√©valuation / test\n",
    "eval_start = pd.Timestamp(\"1983-01-01\")\n",
    "eval_end   = pd.Timestamp(\"1989-12-31\")\n",
    "test_start = pd.Timestamp(\"1990-01-01\")\n",
    "test_end   = pd.Timestamp(\"2025-12-31\")   # ajuste si besoin\n",
    "\n",
    "# ---------- Bagging (bootstrap en blocs) ----------\n",
    "use_bagging = True\n",
    "B_boot = 30               # comme les auteurs\n",
    "L_block = 12              # blocs annuels (12 mois)\n",
    "rng = np.random.default_rng(123)  # seed bootstrap\n",
    "\n",
    "# ---------- Fichiers de sortie ----------\n",
    "RIDGE_PKL  = \"ridge_regression.pkl\"        # bundle (dict)\n",
    "RIDGE_META = \"ridge_regression_meta.csv\"   # m√©ta r√©sum√©\n",
    "\n",
    "# ---------- Pr√©paration df_stationary ----------\n",
    "def _ensure_ms_index(df):\n",
    "    \"\"\"Force un index DatetimeIndex en d√©but de mois (MS).\"\"\"\n",
    "    if \"date\" in df.columns:\n",
    "        df = df.set_index(\"date\")\n",
    "    idx = pd.to_datetime(df.index)\n",
    "    df = df.copy()\n",
    "    df.index = idx.to_period(\"M\").to_timestamp(how=\"start\")\n",
    "    return df.asfreq(\"MS\")\n",
    "\n",
    "# On part de df_stationary (toutes donn√©es : 1960‚Üí2025), d√©j√† charg√© en m√©moire\n",
    "df_all = _ensure_ms_index(df_stationary).sort_index()\n",
    "\n",
    "if target_col not in df_all.columns:\n",
    "    raise ValueError(f\"La colonne cible '{target_col}' est absente de df_stationary.\")\n",
    "\n",
    "y_all = df_all[target_col].astype(float)\n",
    "X_all = df_all.drop(columns=[target_col]).astype(float)\n",
    "features = list(X_all.columns)\n",
    "\n",
    "print(f\"‚úÖ Donn√©es pr√™tes : {df_all.index.min().date()} ‚Üí {df_all.index.max().date()} | n={len(df_all)} | freq=MS\")\n",
    "print(f\"Features ({len(features)}): {features[:6]}{' ...' if len(features)>6 else ''}\")\n",
    "\n",
    "# ---------- Pr√©proc ----------\n",
    "def fit_preproc(X, wins=0.01, do_norm=True):\n",
    "    \"\"\"Apprend winsor + normalisation sur TRAIN et renvoie (X_trans, prep).\"\"\"\n",
    "    lower = X.quantile(wins)\n",
    "    upper = X.quantile(1 - wins)\n",
    "    Xw = X.clip(lower=lower, upper=upper, axis=1)\n",
    "    if do_norm:\n",
    "        mean = Xw.mean()\n",
    "        std  = Xw.std().replace(0, 1)\n",
    "        Xn   = (Xw - mean) / std\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": mean, \"std\": std, \"norm\": True}\n",
    "        return Xn, prep\n",
    "    else:\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": None, \"std\": None, \"norm\": False}\n",
    "        return Xw, prep\n",
    "\n",
    "def apply_preproc(X, prep):\n",
    "    \"\"\"Applique le pr√©proc appris (pas de fuite).\"\"\"\n",
    "    Xp = X.clip(lower=prep[\"lower\"], upper=prep[\"upper\"], axis=1)\n",
    "    if prep[\"norm\"]:\n",
    "        Xp = (Xp - prep[\"mean\"]) / prep[\"std\"].replace(0, 1)\n",
    "    return Xp\n",
    "\n",
    "# ---------- S√©lection d'alpha ----------\n",
    "def select_alpha(X_tr_p, y_tr, mode=\"cv\"):\n",
    "    \"\"\"Renvoie (alpha, cv_mae) si mode='cv', sinon (alpha, np.nan).\"\"\"\n",
    "    if mode == \"cv\":\n",
    "        model = Ridge(fit_intercept=True)\n",
    "        grid = GridSearchCV(\n",
    "            model,\n",
    "            {\"alpha\": alpha_grid},\n",
    "            scoring=\"neg_mean_absolute_error\",\n",
    "            cv=5,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        grid.fit(X_tr_p, y_tr.values)\n",
    "        best_alpha = float(grid.best_estimator_.alpha)\n",
    "        cv_mae = float(-grid.best_score_)\n",
    "        return best_alpha, cv_mae\n",
    "    else:\n",
    "        # mode = valeur fixe (float)\n",
    "        try:\n",
    "            a = float(mode)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"alpha_mode doit √™tre 'cv' ou un float. Re√ßu: {mode}\") from e\n",
    "        return a, np.nan\n",
    "\n",
    "# ---------- Bootstrap utils ----------\n",
    "def block_bootstrap_rows(index, L, rng):\n",
    "    \"\"\"\n",
    "    Moving-block bootstrap sur index (positions).\n",
    "    Renvoie un array d'indices (longueur = n).\n",
    "    \"\"\"\n",
    "    n = len(index)\n",
    "    if n < 3:\n",
    "        return np.arange(n)  # fallback\n",
    "    L = max(2, min(int(L), n-1))\n",
    "    nb = int(np.ceil(n / L))\n",
    "    starts = rng.integers(0, n - L + 1, size=nb)\n",
    "    ix = np.concatenate([np.arange(s, s+L) for s in starts])[:n]\n",
    "    return ix\n",
    "\n",
    "def bagged_predict_ridge(X_tr_raw, y_tr, x_fore_raw, prep, alpha, B, L, rng):\n",
    "    \"\"\"\n",
    "    Bagging (moving-block bootstrap) pour Ridge :\n",
    "      - pr√©proc fix√© sur TRAIN original (pas r√©-appris)\n",
    "      - r√©√©chantillon par blocs (lignes) (X, y)\n",
    "      - fit et pr√©diction h\n",
    "      - renvoie (moyenne, distribution compl√®te, base_pred)\n",
    "    \"\"\"\n",
    "    # Base fit (r√©f√©rence)\n",
    "    X_tr_p = apply_preproc(X_tr_raw, prep)\n",
    "    base = Ridge(alpha=alpha, fit_intercept=True)\n",
    "    base.fit(X_tr_p, y_tr.values)\n",
    "    yhat_base = float(base.predict(apply_preproc(x_fore_raw, prep))[0])\n",
    "\n",
    "    preds = []\n",
    "    for _ in range(B):\n",
    "        ix = block_bootstrap_rows(X_tr_raw.index, L, rng)\n",
    "        Xb = X_tr_raw.iloc[ix]\n",
    "        yb = y_tr.iloc[ix]\n",
    "        Xb_p = apply_preproc(Xb, prep)  # IMPORTANT: m√™me prep\n",
    "        m = Ridge(alpha=alpha, fit_intercept=True)\n",
    "        m.fit(Xb_p, yb.values)\n",
    "        preds.append(float(m.predict(apply_preproc(x_fore_raw, prep))[0]))\n",
    "    return float(np.mean(preds)), np.array(preds), yhat_base\n",
    "\n",
    "# ---------- Boucle pseudo-OOS ----------\n",
    "rows = []                 # (date, y_pred, y_true, y_pred_base, p05, p95)\n",
    "models = []               # stockage dernier fit (optionnel)\n",
    "preprocs = []             # stockage prep (optionnel)\n",
    "train_ends = []           # dates de fin train (pour trace)\n",
    "alpha_history = []        # alpha utilis√© par fen√™tre\n",
    "cv_mae_history = []       # MAE CV (si mode=\"cv\"), sinon NaN\n",
    "\n",
    "last_t_end = y_all.index.max() - relativedelta(months=h)\n",
    "last_model = None\n",
    "last_fit_end = None\n",
    "\n",
    "for t_end in y_all.index:\n",
    "    if t_end > last_t_end:\n",
    "        break\n",
    "\n",
    "    y_tr = y_all.loc[:t_end]\n",
    "    X_tr = X_all.loc[:t_end]\n",
    "    if len(y_tr) < min_train_n:\n",
    "        continue\n",
    "\n",
    "    # Pr√©proc appris sur TRAIN courant\n",
    "    X_tr_p, prep = fit_preproc(X_tr, wins=winsor_level, do_norm=norm_var)\n",
    "\n",
    "    # Choix alpha (CV ou fixe)\n",
    "    alpha, cv_mae = select_alpha(X_tr_p, y_tr, mode=alpha_mode)\n",
    "    alpha_history.append(alpha)\n",
    "    cv_mae_history.append(cv_mae)\n",
    "\n",
    "    # Horizon cibl√©\n",
    "    t_fore = t_end + relativedelta(months=h)\n",
    "    if t_fore in y_all.index:\n",
    "        x_fore_raw = X_all.loc[[t_fore]]\n",
    "\n",
    "        if use_bagging:\n",
    "            # (Option) reseed par mois : rng = np.random.default_rng(int(t_end.strftime(\"%Y%m\")))\n",
    "            yhat_h, dist, yhat_base = bagged_predict_ridge(\n",
    "                X_tr_raw=X_tr, y_tr=y_tr, x_fore_raw=x_fore_raw,\n",
    "                prep=prep, alpha=alpha, B=B_boot, L=L_block, rng=rng\n",
    "            )\n",
    "            y_p05 = float(np.percentile(dist, 5))\n",
    "            y_p95 = float(np.percentile(dist, 95))\n",
    "        else:\n",
    "            model_tmp = Ridge(alpha=alpha, fit_intercept=True)\n",
    "            model_tmp.fit(X_tr_p, y_tr.values)\n",
    "            yhat_h = float(model_tmp.predict(apply_preproc(x_fore_raw, prep))[0])\n",
    "            yhat_base = yhat_h\n",
    "            y_p05, y_p95 = (np.nan, np.nan)\n",
    "\n",
    "        rows.append((t_fore, yhat_h, float(y_all.loc[t_fore]), yhat_base, y_p05, y_p95))\n",
    "\n",
    "    # trace / dernier mod√®le base (utile pour sauvegarde)\n",
    "    last_model = Ridge(alpha=alpha, fit_intercept=True).fit(X_tr_p, y_tr.values)\n",
    "    last_fit_end = t_end\n",
    "    models.append(last_model)\n",
    "    preprocs.append(prep)\n",
    "    train_ends.append(t_end)\n",
    "\n",
    "# ---------- DataFrame OOS ----------\n",
    "if rows:\n",
    "    df_oos = (\n",
    "        pd.DataFrame(rows, columns=[\"date\", \"y_pred\", \"y_true\", \"y_pred_base\", \"y_pred_p05\", \"y_pred_p95\"])\n",
    "          .assign(date=lambda d: pd.to_datetime(d[\"date\"]).dt.to_period(\"M\").dt.to_timestamp(how=\"start\"))\n",
    "          .set_index(\"date\").sort_index()\n",
    "    )\n",
    "else:\n",
    "    df_oos = pd.DataFrame(columns=[\"y_pred\", \"y_true\", \"y_pred_base\", \"y_pred_p05\", \"y_pred_p95\"])\n",
    "    df_oos.index = pd.to_datetime(pd.Index([]))\n",
    "\n",
    "print(f\"\\n‚úÖ Pseudo-OOS (Ridge) termin√© ‚Äî n pr√©visions = {len(df_oos)}\")\n",
    "print(df_oos.head(3))\n",
    "\n",
    "# ---------- Scores ----------\n",
    "def _scores(df):\n",
    "    if len(df) == 0:\n",
    "        return {\"MAE\": np.nan, \"RMSE\": np.nan, \"R2\": np.nan}\n",
    "    mae  = mean_absolute_error(df[\"y_true\"], df[\"y_pred\"])\n",
    "    rmse = np.sqrt(mean_squared_error(df[\"y_true\"], df[\"y_pred\"]))\n",
    "    r2   = r2_score(df[\"y_true\"], df[\"y_pred\"]) if len(df) > 1 else np.nan\n",
    "    return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2)}\n",
    "\n",
    "df_val  = df_oos.loc[eval_start:eval_end].copy()\n",
    "df_test = df_oos.loc[test_start:test_end].copy()\n",
    "\n",
    "sc_val  = _scores(df_val)\n",
    "sc_test = _scores(df_test)\n",
    "\n",
    "print(f\"\\nüìä Validation 83‚Äì89 ‚Äî n={len(df_val)} | MAE={sc_val['MAE']:.3f} | RMSE={sc_val['RMSE']:.3f} | R¬≤={sc_val['R2']:.3f}\")\n",
    "print(f\"üìä Test 90‚Äì2025 ‚Äî n={len(df_test)} | MAE={sc_test['MAE']:.3f} | RMSE={sc_test['RMSE']:.3f} | R¬≤={sc_test['R2']:.3f}\")\n",
    "\n",
    "# (option) Comparaison bagging vs base\n",
    "if \"y_pred_base\" in df_oos and df_oos[\"y_pred_base\"].notna().any():\n",
    "    mae_bag  = mean_absolute_error(df_oos[\"y_true\"], df_oos[\"y_pred\"])\n",
    "    mae_base = mean_absolute_error(df_oos[\"y_true\"], df_oos[\"y_pred_base\"])\n",
    "    print(f\"‚û°Ô∏è  Gain bagging (ŒîMAE) = {mae_base - mae_bag:.3f}\")\n",
    "\n",
    "# ---------- Sauvegardes ----------\n",
    "bundle = {\n",
    "    \"oos_predictions\": df_oos.reset_index(),     # (date, y_pred, y_true, y_pred_base, y_pred_p05, y_pred_p95)\n",
    "    \"params\": {\n",
    "        \"model\": \"Ridge\",\n",
    "        \"horizon\": h,\n",
    "        \"min_train_n\": min_train_n,\n",
    "        \"winsor_level\": winsor_level,\n",
    "        \"norm_var\": norm_var,\n",
    "        \"features\": features,\n",
    "        \"eval_window\": (str(eval_start.date()), str(eval_end.date())),\n",
    "        \"test_window\": (str(test_start.date()), str(test_end.date())),\n",
    "        # ---- bagging ----\n",
    "        \"use_bagging\": bool(use_bagging),\n",
    "        \"B_boot\": int(B_boot),\n",
    "        \"L_block\": int(L_block),\n",
    "        # ---- alpha ----\n",
    "        \"alpha_mode\": alpha_mode,\n",
    "        \"alpha_grid\": list(alpha_grid) if alpha_mode == \"cv\" else None,\n",
    "        \"best_alpha_last\": (alpha_history[-1] if len(alpha_history) else None),\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"trained_until\": str(last_fit_end.date()) if last_fit_end is not None else None,\n",
    "        \"index_freq\": \"MS\",\n",
    "        \"n_obs_all\": int(len(df_all)),\n",
    "        \"n_forecasts\": int(len(df_oos)),\n",
    "        \"alpha_history\": alpha_history,\n",
    "        \"cv_mae_history\": cv_mae_history,\n",
    "    },\n",
    "    \"train_fit_dates\": pd.to_datetime(pd.Index(train_ends)),\n",
    "\n",
    "    # ‚úÖ Pour permutation_importance_pseudo_oos & SHAP\n",
    "    \"models\":   models,     # liste des mod√®les Ridge (un par fen√™tre)\n",
    "    \"preprocs\": preprocs,   # liste des pr√©proc (dict) align√©s aux mod√®les\n",
    "}\n",
    "\n",
    "# --- Sauvegarde du bundle complet ---\n",
    "with open(RIDGE_PKL, \"wb\") as f:\n",
    "    pickle.dump(bundle, f)\n",
    "\n",
    "# --- Sauvegarde du r√©sum√© m√©ta s√©par√© (lisible rapidement) ---\n",
    "pd.DataFrame([{\n",
    "    \"model\": \"Ridge\",\n",
    "    \"horizon\": h,\n",
    "    \"min_train_n\": min_train_n,\n",
    "    \"winsor_level\": winsor_level,\n",
    "    \"norm_var\": norm_var,\n",
    "    \"use_bagging\": bool(use_bagging),\n",
    "    \"B_boot\": int(B_boot),\n",
    "    \"L_block\": int(L_block),\n",
    "    \"alpha_mode\": alpha_mode,\n",
    "    \"best_alpha_last\": bundle[\"params\"][\"best_alpha_last\"],\n",
    "    \"trained_until\": bundle[\"meta\"][\"trained_until\"],\n",
    "    \"n_forecasts\": bundle[\"meta\"][\"n_forecasts\"],\n",
    "}]).to_csv(RIDGE_META, index=False)\n",
    "\n",
    "print(f\"\\nüíæ Bundle sauvegard√© ‚Üí {RIDGE_PKL}\")\n",
    "print(f\"üíæ M√©ta sauvegard√©e ‚Üí {RIDGE_META}\")\n",
    "print(f\"üì¶ Contenu du bundle : {list(bundle.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a27eb",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdb0d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# LightGBM + Bagging (pseudo-OOS, h=12) ‚Äî structure \"comme Ridge\"\n",
    "# - Refit annuel, retune hyperparams tous les 36 mois (d√®s 1983)\n",
    "# - hv-block CV (5 folds, gap=12), scoring=MAE\n",
    "# - Winsorisation 1%/99% + normalisation (apprises sur TRAIN)\n",
    "# - Bagging : n_boot bootstrap (proportions/indices), moyenne des pr√©dictions\n",
    "# - Bundle complet : oos_predictions + models + preprocs + train_fit_dates\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# ---------- Param√®tres g√©n√©raux ----------\n",
    "h = 12\n",
    "min_train_n = 36\n",
    "winsor_level = 0.01\n",
    "norm_var = True\n",
    "target_col = \"UNRATE\"\n",
    "\n",
    "# Fen√™tres d‚Äô√©val / test\n",
    "eval_start = pd.Timestamp(\"1983-01-01\")\n",
    "eval_end   = pd.Timestamp(\"1989-12-31\")\n",
    "test_start = pd.Timestamp(\"1990-01-01\")\n",
    "test_end   = pd.Timestamp(\"2025-12-31\")\n",
    "\n",
    "# ---------- Refit/Retune ----------\n",
    "refit_every_months  = 12     # refit chaque 12 mois\n",
    "retune_every_months = 36     # retune hyperparams chaque 36 mois\n",
    "\n",
    "# ---------- Bagging ----------\n",
    "use_bagging = True\n",
    "n_boot = 30\n",
    "bootstrap_proportion = 1.0\n",
    "rng = np.random.default_rng(12345)\n",
    "\n",
    "# ---------- Fichiers de sortie ----------\n",
    "LGBM_BUNDLE     = \"lightgbm_regression.pkl\"\n",
    "LGBM_META       = \"lightgbm_regression_meta.csv\"\n",
    "LGBM_LAST_PKL   = \"LGBM_last_trained_model.pkl\"\n",
    "LGBM_LAST_META  = \"LGBM_last_trained_model_meta.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c56304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Donn√©es pr√™tes : 1960-01-01 ‚Üí 2025-08-01 | n=788 | freq=MS\n",
      "Features (10): ['TB3MS', 'RPI', 'INDPRO', 'DPCERA3M086SBEA', 'S&P_500', 'BUSLOANS'] ...\n"
     ]
    }
   ],
   "source": [
    "# ---------- Pr√©paration df_stationary ----------\n",
    "def _ensure_ms_index(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Index mensuel (MS). Si 'date' existe, on l'utilise comme index.\"\"\"\n",
    "    df = df.copy()\n",
    "    if \"date\" in df.columns:\n",
    "        df = df.set_index(\"date\")\n",
    "    idx = pd.to_datetime(df.index)\n",
    "    df.index = idx.to_period(\"M\").to_timestamp(how=\"start\")\n",
    "    return df.asfreq(\"MS\")\n",
    "\n",
    "# ‚ö†Ô∏è On suppose df_stationary dispo\n",
    "df_all = _ensure_ms_index(df_stationary).sort_index()\n",
    "\n",
    "if target_col not in df_all.columns:\n",
    "    raise ValueError(f\"La cible '{target_col}' est absente de df_stationary.\")\n",
    "\n",
    "y_all = df_all[target_col].astype(float)\n",
    "X_all = df_all.drop(columns=[target_col]).astype(float)\n",
    "\n",
    "# LightGBM : √©viter espaces dans noms de colonnes\n",
    "X_all.columns = [str(c).replace(\" \", \"_\") for c in X_all.columns]\n",
    "features = list(X_all.columns)\n",
    "\n",
    "print(f\"‚úÖ Donn√©es pr√™tes : {df_all.index.min().date()} ‚Üí {df_all.index.max().date()} | n={len(df_all)} | freq=MS\")\n",
    "print(f\"Features ({len(features)}): {features[:6]}{' ...' if len(features)>6 else ''}\")\n",
    "\n",
    "# ---------- Pr√©proc ----------\n",
    "def fit_preproc(X: pd.DataFrame, wins=0.01, do_norm=True):\n",
    "    lower = X.quantile(wins)\n",
    "    upper = X.quantile(1 - wins)\n",
    "    Xw = X.clip(lower=lower, upper=upper, axis=1)\n",
    "    if do_norm:\n",
    "        mean = Xw.mean()\n",
    "        std  = Xw.std().replace(0, 1)\n",
    "        Xn   = (Xw - mean) / std\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": mean, \"std\": std, \"norm\": True}\n",
    "        return Xn, prep\n",
    "    else:\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": None, \"std\": None, \"norm\": False}\n",
    "        return Xw, prep\n",
    "\n",
    "def apply_preproc(X: pd.DataFrame, prep: dict):\n",
    "    Xp = X.clip(lower=prep[\"lower\"], upper=prep[\"upper\"], axis=1)\n",
    "    if prep[\"norm\"]:\n",
    "        Xp = (Xp - prep[\"mean\"]) / prep[\"std\"].replace(0, 1)\n",
    "    return Xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a01c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- hv-block CV ----------\n",
    "class HVBlockCV:\n",
    "    def __init__(self, n_splits=5, gap=12):\n",
    "        self.n_splits = n_splits\n",
    "        self.gap = gap\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n = len(X)\n",
    "        fold_sizes = np.full(self.n_splits, n // self.n_splits, dtype=int)\n",
    "        fold_sizes[: n % self.n_splits] += 1\n",
    "        idx = np.arange(n)\n",
    "        cur = 0\n",
    "        for fs in fold_sizes:\n",
    "            start, stop = cur, cur + fs\n",
    "            test_idx = idx[start:stop]\n",
    "            train_mask = np.ones(n, dtype=bool)\n",
    "            left = max(0, start - self.gap)\n",
    "            right = min(n, stop + self.gap)\n",
    "            train_mask[left:right] = False\n",
    "            train_idx = idx[train_mask]\n",
    "            cur = stop\n",
    "            if len(train_idx) == 0 or len(test_idx) == 0:\n",
    "                continue\n",
    "            yield train_idx, test_idx\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e175d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Grille LightGBM (stabilit√©) ----------\n",
    "param_dist = {\n",
    "    \"subsample\":        [0.05,.1,.2,.3,.4,.5,.6,.7,.8,.9,1.0],\n",
    "    \"colsample_bytree\": [.2,.3,.4,.5,.6,.7,1.0],\n",
    "    \"num_leaves\":       [2,3,4,5,8,10,20,40,70,100],\n",
    "    \"n_estimators\":     [5,10,20,30,40,50,75,100],\n",
    "    \"max_depth\":        [1,2,3,5,8,15,-1],          # -1 = illimit√©\n",
    "    \"reg_alpha\":        [0, .1, 1, 2, 7, 10, 50, 100],\n",
    "    \"reg_lambda\":       [0, .1, 1, 10, 20, 50, 100],\n",
    "    \"min_child_samples\":[5,10,15],\n",
    "    \"min_split_gain\":   [0.0, 0.01, 0.05],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6f7f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lgbm(X_tr_p: pd.DataFrame, y_tr: np.ndarray, seed=12345):\n",
    "    cv = HVBlockCV(n_splits=5, gap=12)\n",
    "    base = LGBMRegressor(\n",
    "        boosting_type=\"gbdt\",\n",
    "        objective=\"regression\",\n",
    "        importance_type=\"gain\",\n",
    "        random_state=seed,\n",
    "        n_jobs=1,    # √©viter sur-parall√©lisation quand RandomizedSearchCV utilise n_jobs=-1\n",
    "        verbose=-1\n",
    "    )\n",
    "    rs = RandomizedSearchCV(\n",
    "        estimator=base,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=100,\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=seed,\n",
    "        refit=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    rs.fit(X_tr_p, y_tr)\n",
    "    return rs.best_params_, float(-rs.best_score_), rs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f79bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Bootstrap utils ----------\n",
    "def bootstrap_indices(n, proportion=1.0, seed=None):\n",
    "    m = int(round(n * proportion))\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    return rng_local.integers(0, n, size=m, endpoint=False)\n",
    "\n",
    "def bagged_predict_lgbm(X_tr_raw, y_tr, x_fore_raw, prep, best_params, B, proportion, seed0):\n",
    "    \"\"\"\n",
    "    Bagging LightGBM :\n",
    "      - pr√©proc fixe (appris sur TRAIN original)\n",
    "      - bootstrap simple d'indices (proportion)\n",
    "      - fit LGBM(**best_params) et pr√©diction\n",
    "      - retourne la moyenne + distribution + pr√©diction base (fit sur tout TRAIN)\n",
    "    \"\"\"\n",
    "    # Base (r√©f√©rence) : fit sur tout TRAIN pr√©trait√©\n",
    "    X_tr_p = apply_preproc(X_tr_raw, prep)\n",
    "    base = LGBMRegressor(\n",
    "        boosting_type=\"gbdt\",\n",
    "        objective=\"regression\",\n",
    "        importance_type=\"gain\",\n",
    "        n_jobs=1,\n",
    "        random_state=seed0,\n",
    "        verbose=-1,\n",
    "        **best_params\n",
    "    )\n",
    "    base.fit(X_tr_p, y_tr.values)\n",
    "    yhat_base = float(base.predict(apply_preproc(x_fore_raw, prep))[0])\n",
    "\n",
    "    preds = []\n",
    "    n = len(X_tr_raw)\n",
    "    for b in range(B):\n",
    "        ix = bootstrap_indices(n, proportion=proportion, seed=seed0 + b)\n",
    "        Xb = X_tr_raw.iloc[ix]\n",
    "        yb = y_tr.iloc[ix]\n",
    "        Xb_p, _ = fit_preproc(Xb, wins=winsor_level, do_norm=norm_var)  # pr√©proc appris sur bootstrap\n",
    "        m = LGBMRegressor(\n",
    "            boosting_type=\"gbdt\",\n",
    "            objective=\"regression\",\n",
    "            importance_type=\"gain\",\n",
    "            n_jobs=1,\n",
    "            random_state=seed0 + b,\n",
    "            verbose=-1,\n",
    "            **best_params\n",
    "        )\n",
    "        m.fit(Xb_p, yb.values)\n",
    "        preds.append(float(m.predict(apply_preproc(x_fore_raw, _))[0]))  # appliquer le prep du bootstrap (_)\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    return float(np.mean(preds)), preds, yhat_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e31984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Validation 83‚Äì89 ‚Äî n=84 | MAE=0.788 | RMSE=1.040 | R¬≤=-0.385\n",
      "üìä Test 90‚Äì2025 ‚Äî n=428 | MAE=0.777 | RMSE=1.379 | R¬≤=0.183\n"
     ]
    }
   ],
   "source": [
    "# ---------- Boucle pseudo-OOS ----------\n",
    "rows = []                 # (date, y_pred, y_true, y_pred_base, p05, p95)\n",
    "models = []               # mod√®les \"base\" LGBM par refit (pour permutation/SHAP)\n",
    "preprocs = []             # pr√©procs align√©s\n",
    "train_ends = []           # dates de refit\n",
    "cv_mae_history = []       # historique MAE CV lors des retunes\n",
    "best_params_hist = []     # historique des params\n",
    "\n",
    "last_fit_end = None\n",
    "last_t_end   = y_all.index.max() - relativedelta(months=h)\n",
    "last_refit_t = None\n",
    "last_tune_t  = None\n",
    "\n",
    "best_params = {}          # params courants (remplis au 1er retune)\n",
    "base_model = None         # s√©curit√© si use_bagging=False\n",
    "boot_seed = 12345         # reseed √† chaque refit\n",
    "seed0 = 12345\n",
    "\n",
    "for t_end in y_all.index:\n",
    "    if t_end > last_t_end:\n",
    "        break\n",
    "\n",
    "    y_tr = y_all.loc[:t_end]\n",
    "    X_tr = X_all.loc[:t_end]\n",
    "    if len(y_tr) < min_train_n:\n",
    "        continue\n",
    "\n",
    "    # cadence refit\n",
    "    if last_refit_t is None:\n",
    "        need_refit = True\n",
    "    else:\n",
    "        months_since_refit = (t_end.year - last_refit_t.year)*12 + (t_end.month - last_refit_t.month)\n",
    "        need_refit = months_since_refit >= refit_every_months\n",
    "\n",
    "    # cadence retune\n",
    "    need_tune = False\n",
    "    if t_end >= eval_start:\n",
    "        if last_tune_t is None:\n",
    "            need_tune = True\n",
    "        else:\n",
    "            months_since_tune = (t_end.year - last_tune_t.year)*12 + (t_end.month - last_tune_t.month)\n",
    "            need_tune = months_since_tune >= retune_every_months\n",
    "\n",
    "    # Pr√©proc global (pour tuning/refit)\n",
    "    X_tr_p_global, prep_global = fit_preproc(X_tr, wins=winsor_level, do_norm=norm_var)\n",
    "\n",
    "    if need_tune:\n",
    "        best_params, best_cv_mae, _ = tune_lgbm(X_tr_p_global, y_tr.values, seed=seed0)\n",
    "        last_tune_t = t_end\n",
    "        cv_mae_history.append(best_cv_mae)\n",
    "        best_params_hist.append(best_params.copy())\n",
    "    else:\n",
    "        cv_mae_history.append(np.nan)\n",
    "        best_params_hist.append(best_params.copy() if best_params else {})\n",
    "\n",
    "    # Valeurs s√ªres si aucun retune n'a encore eu lieu\n",
    "    if not best_params:\n",
    "        best_params = dict(\n",
    "            subsample=0.7, colsample_bytree=0.7, num_leaves=31,\n",
    "            n_estimators=100, max_depth=-1, reg_alpha=0.0, reg_lambda=0.0,\n",
    "            min_child_samples=10, min_split_gain=0.0\n",
    "        )\n",
    "\n",
    "    if need_refit:\n",
    "        # Mod√®le \"base\" stock√© pour permutation/SHAP\n",
    "        base_model = LGBMRegressor(\n",
    "            boosting_type=\"gbdt\",\n",
    "            objective=\"regression\",\n",
    "            importance_type=\"gain\",\n",
    "            n_jobs=1,\n",
    "            random_state=seed0,\n",
    "            verbose=-1,\n",
    "            **best_params\n",
    "        )\n",
    "        base_model.fit(X_tr_p_global, y_tr.values)\n",
    "        models.append(base_model)\n",
    "        preprocs.append(prep_global)\n",
    "\n",
    "        train_ends.append(t_end)\n",
    "        last_refit_t = t_end\n",
    "        last_fit_end = t_end\n",
    "\n",
    "        # reseed bagging √† chaque refit\n",
    "        boot_seed += 9973\n",
    "\n",
    "    # Pr√©vision h=12\n",
    "    t_fore = t_end + relativedelta(months=h)\n",
    "    if t_fore in y_all.index:\n",
    "        x_fore_raw = X_all.loc[[t_fore]]\n",
    "\n",
    "        if use_bagging:\n",
    "            yhat, dist, yhat_base = bagged_predict_lgbm(\n",
    "                X_tr_raw=X_tr, y_tr=y_tr, x_fore_raw=x_fore_raw,\n",
    "                prep=prep_global, best_params=best_params,\n",
    "                B=n_boot, proportion=bootstrap_proportion, seed0=boot_seed\n",
    "            )\n",
    "            y_p05 = float(np.percentile(dist, 5))\n",
    "            y_p95 = float(np.percentile(dist, 95))\n",
    "        else:\n",
    "            # sans bagging : utiliser / garantir un base_model\n",
    "            if base_model is None:\n",
    "                base_model = LGBMRegressor(\n",
    "                    boosting_type=\"gbdt\",\n",
    "                    objective=\"regression\",\n",
    "                    importance_type=\"gain\",\n",
    "                    n_jobs=1,\n",
    "                    random_state=seed0,\n",
    "                    verbose=-1,\n",
    "                    **best_params\n",
    "                ).fit(X_tr_p_global, y_tr.values)\n",
    "            yhat = float(base_model.predict(apply_preproc(x_fore_raw, prep_global))[0])\n",
    "            yhat_base = yhat\n",
    "            y_p05, y_p95 = (np.nan, np.nan)\n",
    "\n",
    "        rows.append((t_fore, yhat, float(y_all.loc[t_fore]), yhat_base, y_p05, y_p95))\n",
    "\n",
    "# ---------- DataFrame OOS ----------\n",
    "if rows:\n",
    "    df_oos = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\"date\", \"y_pred\", \"y_true\", \"y_pred_base\", \"y_pred_p05\", \"y_pred_p95\"]\n",
    "    )\n",
    "    # ‚úÖ Conversion date ‚Üí d√©but de mois (MS)\n",
    "    df_oos[\"date\"] = pd.to_datetime(df_oos[\"date\"]).dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "    df_oos = df_oos.set_index(\"date\").sort_index()\n",
    "else:\n",
    "    df_oos = pd.DataFrame(columns=[\"y_pred\", \"y_true\", \"y_pred_base\", \"y_pred_p05\", \"y_pred_p95\"])\n",
    "    df_oos.index = pd.to_datetime(pd.Index([]))\n",
    "\n",
    "# ---------- Scores ----------\n",
    "def _scores(df):\n",
    "    if len(df) == 0:\n",
    "        return {\"MAE\": np.nan, \"RMSE\": np.nan, \"R2\": np.nan}\n",
    "    mae  = mean_absolute_error(df[\"y_true\"], df[\"y_pred\"])\n",
    "    rmse = np.sqrt(mean_squared_error(df[\"y_true\"], df[\"y_pred\"]))\n",
    "    ssr  = np.sum((df[\"y_true\"] - df[\"y_pred\"])**2)\n",
    "    sst  = np.sum((df[\"y_true\"] - df[\"y_true\"].mean())**2)\n",
    "    r2   = 1 - ssr/sst if sst > 0 else np.nan\n",
    "    return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2)}\n",
    "\n",
    "df_val  = df_oos.loc[eval_start:eval_end].copy()\n",
    "df_test = df_oos.loc[test_start:test_end].copy()\n",
    "\n",
    "sc_val  = _scores(df_val)\n",
    "sc_test = _scores(df_test)\n",
    "\n",
    "print(f\"\\nüìä Validation 83‚Äì89 ‚Äî n={len(df_val)} | MAE={sc_val['MAE']:.3f} | RMSE={sc_val['RMSE']:.3f} | R¬≤={sc_val['R2']:.3f}\")\n",
    "print(f\"üìä Test 90‚Äì{test_end.year} ‚Äî n={len(df_test)} | MAE={sc_test['MAE']:.3f} | RMSE={sc_test['RMSE']:.3f} | R¬≤={sc_test['R2']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d47e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Bundle OOS sauvegard√© ‚Üí lightgbm_regression.pkl\n",
      "üíæ M√©ta bundle       ‚Üí lightgbm_regression_meta.csv\n",
      "üíæ Dernier mod√®le    ‚Üí LGBM_last_trained_model.pkl\n",
      "üíæ M√©ta dernier fit  ‚Üí LGBM_last_trained_model_meta.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------- Sauvegardes ----------\n",
    "bundle = {\n",
    "    \"oos_predictions\": df_oos.reset_index(),\n",
    "    \"params\": {\n",
    "        \"model\": \"LightGBM + Bagging\",\n",
    "        \"horizon\": h,\n",
    "        \"min_train_n\": min_train_n,\n",
    "        \"winsor_level\": winsor_level,\n",
    "        \"norm_var\": norm_var,\n",
    "        \"features\": features,\n",
    "        \"eval_window\": (str(eval_start.date()), str(eval_end.date())),\n",
    "        \"test_window\": (str(test_start.date()), str(test_end.date())),\n",
    "        # plan refit/retune\n",
    "        \"refit_every_months\": refit_every_months,\n",
    "        \"retune_every_months\": retune_every_months,\n",
    "        # search\n",
    "        \"hyper_search\": \"RandomizedSearchCV (100 iters) + hv-block CV (5 folds, gap=12), scoring=MAE\",\n",
    "        \"best_params_last\": best_params.copy(),\n",
    "        # bagging\n",
    "        \"use_bagging\": bool(use_bagging),\n",
    "        \"n_boot\": int(n_boot),\n",
    "        \"bootstrap_proportion\": float(bootstrap_proportion),\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"trained_until\": str(last_fit_end.date()) if last_fit_end is not None else None,\n",
    "        \"index_freq\": \"MS\",\n",
    "        \"n_obs_all\": int(len(df_all)),\n",
    "        \"n_forecasts\": int(len(df_oos)),\n",
    "        \"cv_mae_history\": cv_mae_history,\n",
    "        \"best_params_history\": best_params_hist\n",
    "    },\n",
    "    \"train_fit_dates\": pd.to_datetime(pd.Index(train_ends)),\n",
    "\n",
    "    # ‚úÖ Pour permutation_importance_pseudo_oos & SHAP\n",
    "    \"models\":   models,     # liste des mod√®les \"base\" LGBM (un par refit)\n",
    "    \"preprocs\": preprocs,   # liste des pr√©proc (dict) align√©s aux mod√®les\n",
    "}\n",
    "\n",
    "with open(LGBM_BUNDLE, \"wb\") as f:\n",
    "    pickle.dump(bundle, f)\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"model\": \"LightGBM+Bagging\",\n",
    "    \"horizon\": h,\n",
    "    \"min_train_n\": min_train_n,\n",
    "    \"winsor_level\": winsor_level,\n",
    "    \"norm_var\": norm_var,\n",
    "    \"refit_every_months\": refit_every_months,\n",
    "    \"retune_every_months\": retune_every_months,\n",
    "    \"use_bagging\": bool(use_bagging),\n",
    "    \"n_boot\": int(n_boot),\n",
    "    \"bootstrap_proportion\": float(bootstrap_proportion),\n",
    "    \"trained_until\": bundle[\"meta\"][\"trained_until\"],\n",
    "    \"n_forecasts\": bundle[\"meta\"][\"n_forecasts\"]\n",
    "}]).to_csv(LGBM_META, index=False)\n",
    "\n",
    "# Artefact dernier ensemble (optionnel : pour audit)\n",
    "lgbm_artifact = {\n",
    "    \"trained_until\": bundle[\"meta\"][\"trained_until\"],\n",
    "    \"horizon\": h,\n",
    "    \"features\": features,\n",
    "    \"n_models_base\": len(models),\n",
    "    \"best_params_last\": best_params.copy(),\n",
    "}\n",
    "with open(LGBM_LAST_PKL, \"wb\") as f:\n",
    "    pickle.dump(lgbm_artifact, f)\n",
    "pd.DataFrame([{\n",
    "    \"trained_until\": lgbm_artifact[\"trained_until\"],\n",
    "    \"n_features\": len(features),\n",
    "    \"horizon\": h,\n",
    "    \"n_models_base\": lgbm_artifact[\"n_models_base\"]\n",
    "}]).to_csv(LGBM_LAST_META, index=False)\n",
    "\n",
    "print(f\"\\nüíæ Bundle OOS sauvegard√© ‚Üí {LGBM_BUNDLE}\")\n",
    "print(f\"üíæ M√©ta bundle       ‚Üí {LGBM_META}\")\n",
    "print(f\"üíæ Dernier mod√®le    ‚Üí {LGBM_LAST_PKL}\")\n",
    "print(f\"üíæ M√©ta dernier fit  ‚Üí {LGBM_LAST_META}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63de93c",
   "metadata": {},
   "source": [
    "# LightGBM vrai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9a66ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Donn√©es pr√™tes : 1960-01-01 ‚Üí 2025-08-01 | n=788 | freq=MS\n",
      "Features (10): ['TB3MS', 'RPI', 'INDPRO', 'DPCERA3M086SBEA', 'S&P500', 'BUSLOANS'] ...\n",
      "\n",
      "üìä Validation 83‚Äì89 ‚Äî n=84 | MAE=0.799 | RMSE=1.047 | R¬≤=-0.403\n",
      "üìä Test 90‚Äì2025 ‚Äî n=428 | MAE=0.780 | RMSE=1.388 | R¬≤=0.172\n",
      "\n",
      "üíæ Bundle OOS sauvegard√© ‚Üí lightgbm_regression.pkl\n",
      "üíæ M√©ta bundle       ‚Üí lightgbm_regression_meta.csv\n",
      "üíæ Dernier mod√®le    ‚Üí LGBM_last_trained_model.pkl\n",
      "üíæ M√©ta dernier fit  ‚Üí LGBM_last_trained_model_meta.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# üîπ LightGBM + Bagging en blocs (pseudo-OOS, h=12)\n",
    "#    ‚Üí m√™me structure/artefacts que Linear/Ridge\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# ---------- Param√®tres g√©n√©raux ----------\n",
    "h = 12\n",
    "min_train_n = 36\n",
    "winsor_level = 0.01\n",
    "norm_var = True\n",
    "target_col = \"UNRATE\"\n",
    "\n",
    "# Fen√™tres d‚Äô√©val / test\n",
    "eval_start = pd.Timestamp(\"1983-01-01\")\n",
    "eval_end   = pd.Timestamp(\"1989-12-31\")\n",
    "test_start = pd.Timestamp(\"1990-01-01\")\n",
    "test_end   = pd.Timestamp(\"2025-12-31\")\n",
    "\n",
    "# ---------- Refit/Retune ----------\n",
    "refit_every_months  = 12     # refit chaque 12 mois\n",
    "retune_every_months = 36     # retune hyperparams chaque 36 mois\n",
    "\n",
    "# ---------- Bagging (moving-block bootstrap, comme Ridge) ----------\n",
    "use_bagging = True\n",
    "B_boot = 30                 # nb. de bootstraps\n",
    "L_block = 12                # longueur de bloc (mois)\n",
    "rng = np.random.default_rng(12345)\n",
    "\n",
    "# ---------- Fichiers de sortie ----------\n",
    "LGBM_BUNDLE     = \"lightgbm_regression.pkl\"\n",
    "LGBM_META       = \"lightgbm_regression_meta.csv\"\n",
    "LGBM_LAST_PKL   = \"LGBM_last_trained_model.pkl\"\n",
    "LGBM_LAST_META  = \"LGBM_last_trained_model_meta.csv\"\n",
    "\n",
    "# ---------- Pr√©paration df_stationary ----------\n",
    "def _ensure_ms_index(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Index mensuel (MS). Si 'date' existe, on l'utilise comme index.\"\"\"\n",
    "    df = df.copy()\n",
    "    if \"date\" in df.columns:\n",
    "        df = df.set_index(\"date\")\n",
    "    idx = pd.to_datetime(df.index)\n",
    "    df.index = idx.to_period(\"M\").to_timestamp(how=\"start\")\n",
    "    return df.asfreq(\"MS\")\n",
    "\n",
    "# ‚ö†Ô∏è On suppose df_stationary dispo\n",
    "df_all = _ensure_ms_index(df_stationary).sort_index()\n",
    "\n",
    "if target_col not in df_all.columns:\n",
    "    raise ValueError(f\"La cible '{target_col}' est absente de df_stationary.\")\n",
    "\n",
    "y_all = df_all[target_col].astype(float)\n",
    "X_all = df_all.drop(columns=[target_col]).astype(float)\n",
    "\n",
    "# LightGBM : √©viter espaces dans noms de colonnes\n",
    "X_all.columns = [str(c).replace(\" \", \"_\") for c in X_all.columns]\n",
    "features = list(X_all.columns)\n",
    "\n",
    "print(f\"‚úÖ Donn√©es pr√™tes : {df_all.index.min().date()} ‚Üí {df_all.index.max().date()} | n={len(df_all)} | freq=MS\")\n",
    "print(f\"Features ({len(features)}): {features[:6]}{' ...' if len(features)>6 else ''}\")\n",
    "\n",
    "# ---------- Pr√©proc ----------\n",
    "def fit_preproc(X: pd.DataFrame, wins=0.01, do_norm=True):\n",
    "    lower = X.quantile(wins)\n",
    "    upper = X.quantile(1 - wins)\n",
    "    Xw = X.clip(lower=lower, upper=upper, axis=1)\n",
    "    if do_norm:\n",
    "        mean = Xw.mean()\n",
    "        std  = Xw.std().replace(0, 1)\n",
    "        Xn   = (Xw - mean) / std\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": mean, \"std\": std, \"norm\": True}\n",
    "        return Xn, prep\n",
    "    else:\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": None, \"std\": None, \"norm\": False}\n",
    "        return Xw, prep\n",
    "\n",
    "def apply_preproc(X: pd.DataFrame, prep: dict):\n",
    "    Xp = X.clip(lower=prep[\"lower\"], upper=prep[\"upper\"], axis=1)\n",
    "    if prep[\"norm\"]:\n",
    "        Xp = (Xp - prep[\"mean\"]) / prep[\"std\"].replace(0, 1)\n",
    "    return Xp\n",
    "\n",
    "# ---------- hv-block CV ----------\n",
    "class HVBlockCV:\n",
    "    def __init__(self, n_splits=5, gap=12):\n",
    "        self.n_splits = n_splits\n",
    "        self.gap = gap\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n = len(X)\n",
    "        fold_sizes = np.full(self.n_splits, n // self.n_splits, dtype=int)\n",
    "        fold_sizes[: n % self.n_splits] += 1\n",
    "        idx = np.arange(n)\n",
    "        cur = 0\n",
    "        for fs in fold_sizes:\n",
    "            start, stop = cur, cur + fs\n",
    "            test_idx = idx[start:stop]\n",
    "            train_mask = np.ones(n, dtype=bool)\n",
    "            left = max(0, start - self.gap)\n",
    "            right = min(n, stop + self.gap)\n",
    "            train_mask[left:right] = False\n",
    "            train_idx = idx[train_mask]\n",
    "            cur = stop\n",
    "            if len(train_idx) == 0 or len(test_idx) == 0:\n",
    "                continue\n",
    "            yield train_idx, test_idx\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "# ---------- Grille LightGBM ----------\n",
    "param_dist = {\n",
    "    \"subsample\":        [0.05,.1,.2,.3,.4,.5,.6,.7,.8,.9,1.0],\n",
    "    \"colsample_bytree\": [.2,.3,.4,.5,.6,.7,1.0],\n",
    "    \"num_leaves\":       [2,3,4,5,8,10,20,40,70,100],\n",
    "    \"n_estimators\":     [5,10,20,30,40,50,75,100],\n",
    "    \"max_depth\":        [1,2,3,5,8,15,-1],\n",
    "    \"reg_alpha\":        [0, .1, 1, 2, 7, 10, 50, 100],\n",
    "    \"reg_lambda\":       [0, .1, 1, 10, 20, 50, 100],\n",
    "    \"min_child_samples\":[5,10,15],\n",
    "    \"min_split_gain\":   [0.0, 0.01, 0.05],\n",
    "}\n",
    "\n",
    "def tune_lgbm(X_tr_p: pd.DataFrame, y_tr: np.ndarray, seed=12345):\n",
    "    cv = HVBlockCV(n_splits=5, gap=12)\n",
    "    base = LGBMRegressor(\n",
    "        boosting_type=\"gbdt\",\n",
    "        objective=\"regression\",\n",
    "        importance_type=\"gain\",\n",
    "        random_state=seed,\n",
    "        n_jobs=1,    # √©viter sur-parall√©lisation quand RandomizedSearchCV utilise n_jobs=-1\n",
    "        verbose=-1\n",
    "    )\n",
    "    rs = RandomizedSearchCV(\n",
    "        estimator=base,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=100,\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=seed,\n",
    "        refit=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    rs.fit(X_tr_p, y_tr)\n",
    "    return rs.best_params_, float(-rs.best_score_), rs.best_estimator_\n",
    "\n",
    "# ---------- Bootstrap utils (moving-block comme Ridge) ----------\n",
    "def block_bootstrap_rows(index, L, rng):\n",
    "    n = len(index)\n",
    "    if n < 3:\n",
    "        return np.arange(n)\n",
    "    L = max(2, min(int(L), n-1))\n",
    "    nb = int(np.ceil(n / L))\n",
    "    starts = rng.integers(0, n - L + 1, size=nb)\n",
    "    ix = np.concatenate([np.arange(s, s+L) for s in starts])[:n]\n",
    "    return ix\n",
    "\n",
    "def bagged_predict_lgbm(X_tr_raw, y_tr, x_fore_raw, prep, best_params, B, L, rng):\n",
    "    \"\"\"\n",
    "    Bagging en blocs pour LightGBM (pr√©proc fix√© comme Ridge) :\n",
    "      - pr√©proc fix√© sur TRAIN original (pas r√©-appris)\n",
    "      - moving-block bootstrap sur les lignes (indices align√©s)\n",
    "      - fit LGBM(**best_params) et pr√©diction\n",
    "      - retourne (moyenne, distribution, pr√©diction base)\n",
    "    \"\"\"\n",
    "    # Base fit (r√©f√©rence) sur TRAIN complet pr√©trait√©\n",
    "    X_tr_p = apply_preproc(X_tr_raw, prep)\n",
    "    base = LGBMRegressor(\n",
    "        boosting_type=\"gbdt\",\n",
    "        objective=\"regression\",\n",
    "        importance_type=\"gain\",\n",
    "        n_jobs=1,\n",
    "        random_state=123,\n",
    "        verbose=-1,\n",
    "        **best_params\n",
    "    )\n",
    "    base.fit(X_tr_p, y_tr.values)\n",
    "    yhat_base = float(base.predict(apply_preproc(x_fore_raw, prep))[0])\n",
    "\n",
    "    preds = []\n",
    "    for b in range(B):\n",
    "        ix = block_bootstrap_rows(X_tr_raw.index, L, rng)\n",
    "        Xb = X_tr_raw.iloc[ix]\n",
    "        yb = y_tr.iloc[ix]\n",
    "        Xb_p = apply_preproc(Xb, prep)   # ‚ö†Ô∏è m√™me prep (pas de r√©apprentissage)\n",
    "        m = LGBMRegressor(\n",
    "            boosting_type=\"gbdt\",\n",
    "            objective=\"regression\",\n",
    "            importance_type=\"gain\",\n",
    "            n_jobs=1,\n",
    "            random_state=123 + b,\n",
    "            verbose=-1,\n",
    "            **best_params\n",
    "        )\n",
    "        m.fit(Xb_p, yb.values)\n",
    "        preds.append(float(m.predict(apply_preproc(x_fore_raw, prep))[0]))\n",
    "    return float(np.mean(preds)), np.array(preds), yhat_base\n",
    "\n",
    "# ---------- Boucle pseudo-OOS ----------\n",
    "def _months_between(a: pd.Timestamp, b: pd.Timestamp) -> int:\n",
    "    return (b.year - a.year) * 12 + (b.month - a.month)\n",
    "\n",
    "rows = []\n",
    "models = []               # mod√®les \"base\" LGBM par refit (pour permutation/SHAP)\n",
    "preprocs = []             # pr√©procs align√©s\n",
    "train_ends = []           # dates de refit\n",
    "cv_mae_history = []       # historique MAE CV lors des retunes\n",
    "best_params_hist = []     # historique des params\n",
    "\n",
    "last_fit_end = None\n",
    "last_t_end   = y_all.index.max() - relativedelta(months=h)\n",
    "last_refit_t = None\n",
    "last_tune_t  = None\n",
    "\n",
    "best_params = {}          # params courants (remplis au 1er retune)\n",
    "base_model = None\n",
    "\n",
    "for t_end in y_all.index:\n",
    "    if t_end > last_t_end:\n",
    "        break\n",
    "\n",
    "    y_tr = y_all.loc[:t_end]\n",
    "    X_tr = X_all.loc[:t_end]\n",
    "    if len(y_tr) < min_train_n:\n",
    "        continue\n",
    "\n",
    "    # cadence refit / retune\n",
    "    need_refit = (last_refit_t is None) or (_months_between(last_refit_t, t_end) >= refit_every_months)\n",
    "    need_tune  = (t_end >= eval_start) and (last_tune_t is None or _months_between(last_tune_t, t_end) >= retune_every_months)\n",
    "\n",
    "    # Pr√©proc global (fix√© pour ce t_end)\n",
    "    X_tr_p_global, prep_global = fit_preproc(X_tr, wins=winsor_level, do_norm=norm_var)\n",
    "\n",
    "    # Tuning si n√©cessaire\n",
    "    if need_tune:\n",
    "        best_params, best_cv_mae, _ = tune_lgbm(X_tr_p_global, y_tr.values, seed=12345)\n",
    "        last_tune_t = t_end\n",
    "        cv_mae_history.append(best_cv_mae)\n",
    "        best_params_hist.append(best_params.copy())\n",
    "    else:\n",
    "        cv_mae_history.append(np.nan)\n",
    "        best_params_hist.append(best_params.copy() if best_params else {})\n",
    "\n",
    "    # Valeurs s√ªres si aucun retune n'a encore eu lieu\n",
    "    if not best_params:\n",
    "        best_params = dict(\n",
    "            subsample=0.7, colsample_bytree=0.7, num_leaves=31,\n",
    "            n_estimators=100, max_depth=-1, reg_alpha=0.0, reg_lambda=0.0,\n",
    "            min_child_samples=10, min_split_gain=0.0\n",
    "        )\n",
    "\n",
    "    # Refit si n√©cessaire (stockage du mod√®le base + prep align√©s)\n",
    "    if need_refit:\n",
    "        base_model = LGBMRegressor(\n",
    "            boosting_type=\"gbdt\",\n",
    "            objective=\"regression\",\n",
    "            importance_type=\"gain\",\n",
    "            n_jobs=1,\n",
    "            random_state=12345,\n",
    "            verbose=-1,\n",
    "            **best_params\n",
    "        )\n",
    "        base_model.fit(X_tr_p_global, y_tr.values)\n",
    "        models.append(base_model)\n",
    "        preprocs.append(prep_global)\n",
    "        train_ends.append(t_end)\n",
    "        last_refit_t = t_end\n",
    "        last_fit_end = t_end\n",
    "\n",
    "    # Pr√©vision h=12\n",
    "    t_fore = t_end + relativedelta(months=h)\n",
    "    if t_fore in y_all.index:\n",
    "        x_fore_raw = X_all.loc[[t_fore]]\n",
    "        if use_bagging:\n",
    "            yhat, dist, yhat_base = bagged_predict_lgbm(\n",
    "                X_tr_raw=X_tr, y_tr=y_tr, x_fore_raw=x_fore_raw,\n",
    "                prep=prep_global, best_params=best_params,\n",
    "                B=B_boot, L=L_block, rng=rng\n",
    "            )\n",
    "            y_p05 = float(np.percentile(dist, 5))\n",
    "            y_p95 = float(np.percentile(dist, 95))\n",
    "        else:\n",
    "            if base_model is None:\n",
    "                base_model = LGBMRegressor(\n",
    "                    boosting_type=\"gbdt\",\n",
    "                    objective=\"regression\",\n",
    "                    importance_type=\"gain\",\n",
    "                    n_jobs=1,\n",
    "                    random_state=12345,\n",
    "                    verbose=-1,\n",
    "                    **best_params\n",
    "                ).fit(X_tr_p_global, y_tr.values)\n",
    "            yhat = float(base_model.predict(apply_preproc(x_fore_raw, prep_global))[0])\n",
    "            yhat_base = yhat\n",
    "            y_p05, y_p95 = (np.nan, np.nan)\n",
    "\n",
    "        rows.append((t_fore, yhat, float(y_all.loc[t_fore]), yhat_base, y_p05, y_p95))\n",
    "\n",
    "# ---------- DataFrame OOS ----------\n",
    "if rows:\n",
    "    df_oos = (\n",
    "        pd.DataFrame(rows, columns=[\"date\", \"y_pred\", \"y_true\", \"y_pred_base\", \"y_pred_p05\", \"y_pred_p95\"])\n",
    "          .assign(date=lambda d: pd.to_datetime(d[\"date\"]).dt.to_period(\"M\").dt.to_timestamp(how=\"start\"))\n",
    "          .set_index(\"date\").sort_index()\n",
    "    )\n",
    "else:\n",
    "    df_oos = pd.DataFrame(columns=[\"y_pred\",\"y_true\",\"y_pred_base\",\"y_pred_p05\",\"y_pred_p95\"])\n",
    "    df_oos.index = pd.to_datetime(pd.Index([]))\n",
    "\n",
    "# ---------- Scores ----------\n",
    "def _scores(df):\n",
    "    if len(df) == 0:\n",
    "        return {\"MAE\": np.nan, \"RMSE\": np.nan, \"R2\": np.nan}\n",
    "    mae  = mean_absolute_error(df[\"y_true\"], df[\"y_pred\"])\n",
    "    rmse = np.sqrt(mean_squared_error(df[\"y_true\"], df[\"y_pred\"]))\n",
    "    r2   = r2_score(df[\"y_true\"], df[\"y_pred\"]) if len(df) > 1 else np.nan\n",
    "    return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2)}\n",
    "\n",
    "df_val  = df_oos.loc[eval_start:eval_end].copy()\n",
    "df_test = df_oos.loc[test_start:test_end].copy()\n",
    "sc_val  = _scores(df_val)\n",
    "sc_test = _scores(df_test)\n",
    "\n",
    "print(f\"\\nüìä Validation 83‚Äì89 ‚Äî n={len(df_val)} | MAE={sc_val['MAE']:.3f} | RMSE={sc_val['RMSE']:.3f} | R¬≤={sc_val['R2']:.3f}\")\n",
    "print(f\"üìä Test 90‚Äì{test_end.year} ‚Äî n={len(df_test)} | MAE={sc_test['MAE']:.3f} | RMSE={sc_test['RMSE']:.3f} | R¬≤={sc_test['R2']:.3f}\")\n",
    "\n",
    "# ---------- Sauvegardes ----------\n",
    "bundle = {\n",
    "    \"oos_predictions\": df_oos.reset_index(),\n",
    "    \"params\": {\n",
    "        \"model\": \"LightGBM + Bagging\",\n",
    "        \"horizon\": h,\n",
    "        \"min_train_n\": min_train_n,\n",
    "        \"winsor_level\": winsor_level,\n",
    "        \"norm_var\": norm_var,\n",
    "        \"features\": features,\n",
    "        \"eval_window\": (str(eval_start.date()), str(eval_end.date())),\n",
    "        \"test_window\": (str(test_start.date()), str(test_end.date())),\n",
    "        # plan refit/retune\n",
    "        \"refit_every_months\": int(refit_every_months),\n",
    "        \"retune_every_months\": int(retune_every_months),\n",
    "        # search\n",
    "        \"hyper_search\": \"RandomizedSearchCV (100 iters) + hv-block CV (5 folds, gap=12), scoring=MAE\",\n",
    "        \"best_params_last\": best_params.copy(),\n",
    "        # bagging (m√™mes noms que Ridge)\n",
    "        \"use_bagging\": bool(use_bagging),\n",
    "        \"B_boot\": int(B_boot),\n",
    "        \"L_block\": int(L_block),\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"trained_until\": str(last_fit_end.date()) if last_fit_end is not None else None,\n",
    "        \"index_freq\": \"MS\",\n",
    "        \"n_obs_all\": int(len(df_all)),\n",
    "        \"n_forecasts\": int(len(df_oos)),\n",
    "        \"cv_mae_history\": cv_mae_history,\n",
    "        \"best_params_history\": best_params_hist\n",
    "    },\n",
    "    \"train_fit_dates\": pd.to_datetime(pd.Index(train_ends)),\n",
    "\n",
    "    # ‚úÖ Pour permutation_importance_pseudo_oos & SHAP\n",
    "    \"models\":   models,     # liste des mod√®les \"base\" LGBM (un par refit)\n",
    "    \"preprocs\": preprocs,   # liste des pr√©proc (dict) align√©s aux mod√®les\n",
    "}\n",
    "\n",
    "with open(LGBM_BUNDLE, \"wb\") as f:\n",
    "    pickle.dump(bundle, f)\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"model\": \"LightGBM+Bagging\",\n",
    "    \"horizon\": h,\n",
    "    \"min_train_n\": min_train_n,\n",
    "    \"winsor_level\": winsor_level,\n",
    "    \"norm_var\": norm_var,\n",
    "    \"refit_every_months\": int(refit_every_months),\n",
    "    \"retune_every_months\": int(retune_every_months),\n",
    "    \"use_bagging\": bool(use_bagging),\n",
    "    \"B_boot\": int(B_boot),\n",
    "    \"L_block\": int(L_block),\n",
    "    \"trained_until\": bundle[\"meta\"][\"trained_until\"],\n",
    "    \"n_forecasts\": bundle[\"meta\"][\"n_forecasts\"]\n",
    "}]).to_csv(LGBM_META, index=False)\n",
    "\n",
    "# Artefact dernier ensemble (optionnel : pour audit)\n",
    "lgbm_artifact = {\n",
    "    \"trained_until\": bundle[\"meta\"][\"trained_until\"],\n",
    "    \"horizon\": h,\n",
    "    \"features\": features,\n",
    "    \"n_models_base\": len(models),\n",
    "    \"best_params_last\": best_params.copy(),\n",
    "}\n",
    "with open(LGBM_LAST_PKL, \"wb\") as f:\n",
    "    pickle.dump(lgbm_artifact, f)\n",
    "pd.DataFrame([{\n",
    "    \"trained_until\": lgbm_artifact[\"trained_until\"],\n",
    "    \"n_features\": len(features),\n",
    "    \"horizon\": h,\n",
    "    \"n_models_base\": lgbm_artifact[\"n_models_base\"]\n",
    "}]).to_csv(LGBM_LAST_META, index=False)\n",
    "\n",
    "print(f\"\\nüíæ Bundle OOS sauvegard√© ‚Üí {LGBM_BUNDLE}\")\n",
    "print(f\"üíæ M√©ta bundle       ‚Üí {LGBM_META}\")\n",
    "print(f\"üíæ Dernier mod√®le    ‚Üí {LGBM_LAST_PKL}\")\n",
    "print(f\"üíæ M√©ta dernier fit  ‚Üí {LGBM_LAST_META}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
