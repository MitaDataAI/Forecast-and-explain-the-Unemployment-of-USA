{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c37aae7",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b8aea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d8d201",
   "metadata": {},
   "source": [
    "# Importation des donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78c5c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stationary = pd.read_csv(\"df_stationary.csv\", index_col=\"date\")\n",
    "df_stationary_test = pd.read_csv(\"df_stationary_test.csv\", index_col=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c150f38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['UNRATE', 'TB3MS', 'RPI', 'INDPRO', 'DPCERA3M086SBEA', 'S&P500',\n",
       "       'BUSLOANS', 'CPIAUCSL', 'OILPRICEx', 'M2SL', 'USREC'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stationary.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91cc0a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['UNRATE', 'TB3MS', 'RPI', 'INDPRO', 'DPCERA3M086SBEA', 'S&P500',\n",
       "       'BUSLOANS', 'CPIAUCSL', 'OILPRICEx', 'M2SL', 'USREC', 'UNRATE_lag12'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stationary_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944583e",
   "metadata": {},
   "source": [
    "# RIDGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37bbdc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DonnÃ©es prÃªtes : 1960-01-01 â†’ 2025-08-01 | n=788 | freq=MS\n",
      "Features (10): ['TB3MS', 'RPI', 'INDPRO', 'DPCERA3M086SBEA', 'S&P500', 'BUSLOANS'] ...\n",
      "\n",
      "âœ… Pseudo-OOS (Ridge) terminÃ© â€” n prÃ©visions = 741\n",
      "              y_pred  y_true  y_pred_base  y_pred_p05  y_pred_p95\n",
      "date                                                             \n",
      "1963-12-01  0.093785     0.0    -0.181263   -0.455216    1.220200\n",
      "1964-01-01  0.348080    -0.1    -0.071449   -0.302035    1.165286\n",
      "1964-02-01  0.390935    -0.5     0.058525   -0.508924    1.117637\n",
      "\n",
      "ðŸ“Š Validation 83â€“89 â€” n=84 | MAE=0.819 | RMSE=1.025 | RÂ²=-0.346\n",
      "ðŸ“Š Test 90â€“2025 â€” n=428 | MAE=0.821 | RMSE=1.464 | RÂ²=0.080\n",
      "âž¡ï¸  Gain bagging (Î”MAE) = -0.012\n",
      "\n",
      "ðŸ’¾ Bundle sauvegardÃ© â†’ ridge_regression.pkl\n",
      "ðŸ’¾ MÃ©ta sauvegardÃ©e â†’ ridge_regression_meta.csv\n",
      "ðŸ“¦ Contenu du bundle : ['oos_predictions', 'params', 'meta', 'train_fit_dates', 'models', 'preprocs']\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# ðŸ”¹ Ridge Regression + Bagging (pseudo-OOS)\n",
    "#    â†’ mÃªme structure/artefacts que la version LinearRegression\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# ---------- ParamÃ¨tres gÃ©nÃ©raux ----------\n",
    "h = 12\n",
    "min_train_n = 36           # â‰¥ 3 ans avant de commencer Ã  prÃ©voir\n",
    "winsor_level = 0.01        # winsorisation (1er/99e percentiles)\n",
    "norm_var = True            # normaliser ou non\n",
    "target_col = \"UNRATE\"      # cible dans df_stationary\n",
    "\n",
    "# SÃ©lection d'alpha : \"cv\" (CV 5-fold) OU une valeur float (ex: 1.0)\n",
    "alpha_mode = \"cv\"\n",
    "alpha_grid = np.logspace(-4, 4, 30)   # utilisÃ© si alpha_mode=\"cv\"\n",
    "\n",
    "# FenÃªtres d'Ã©valuation / test\n",
    "eval_start = pd.Timestamp(\"1983-01-01\")\n",
    "eval_end   = pd.Timestamp(\"1989-12-31\")\n",
    "test_start = pd.Timestamp(\"1990-01-01\")\n",
    "test_end   = pd.Timestamp(\"2025-12-31\")   # ajuste si besoin\n",
    "\n",
    "# ---------- Bagging (bootstrap en blocs) ----------\n",
    "use_bagging = True\n",
    "B_boot = 30               # comme les auteurs\n",
    "L_block = 12              # blocs annuels (12 mois)\n",
    "rng = np.random.default_rng(123)  # seed bootstrap\n",
    "\n",
    "# ---------- Fichiers de sortie ----------\n",
    "RIDGE_PKL  = \"ridge_regression.pkl\"        # bundle (dict)\n",
    "RIDGE_META = \"ridge_regression_meta.csv\"   # mÃ©ta rÃ©sumÃ©\n",
    "\n",
    "# ---------- PrÃ©paration df_stationary ----------\n",
    "def _ensure_ms_index(df):\n",
    "    \"\"\"Force un index DatetimeIndex en dÃ©but de mois (MS).\"\"\"\n",
    "    if \"date\" in df.columns:\n",
    "        df = df.set_index(\"date\")\n",
    "    idx = pd.to_datetime(df.index)\n",
    "    df = df.copy()\n",
    "    df.index = idx.to_period(\"M\").to_timestamp(how=\"start\")\n",
    "    return df.asfreq(\"MS\")\n",
    "\n",
    "# On part de df_stationary (toutes donnÃ©es : 1960â†’2025), dÃ©jÃ  chargÃ© en mÃ©moire\n",
    "df_all = _ensure_ms_index(df_stationary).sort_index()\n",
    "\n",
    "if target_col not in df_all.columns:\n",
    "    raise ValueError(f\"La colonne cible '{target_col}' est absente de df_stationary.\")\n",
    "\n",
    "y_all = df_all[target_col].astype(float)\n",
    "X_all = df_all.drop(columns=[target_col]).astype(float)\n",
    "features = list(X_all.columns)\n",
    "\n",
    "print(f\"âœ… DonnÃ©es prÃªtes : {df_all.index.min().date()} â†’ {df_all.index.max().date()} | n={len(df_all)} | freq=MS\")\n",
    "print(f\"Features ({len(features)}): {features[:6]}{' ...' if len(features)>6 else ''}\")\n",
    "\n",
    "# ---------- PrÃ©proc ----------\n",
    "def fit_preproc(X, wins=0.01, do_norm=True):\n",
    "    \"\"\"Apprend winsor + normalisation sur TRAIN et renvoie (X_trans, prep).\"\"\"\n",
    "    lower = X.quantile(wins)\n",
    "    upper = X.quantile(1 - wins)\n",
    "    Xw = X.clip(lower=lower, upper=upper, axis=1)\n",
    "    if do_norm:\n",
    "        mean = Xw.mean()\n",
    "        std  = Xw.std().replace(0, 1)\n",
    "        Xn   = (Xw - mean) / std\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": mean, \"std\": std, \"norm\": True}\n",
    "        return Xn, prep\n",
    "    else:\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": None, \"std\": None, \"norm\": False}\n",
    "        return Xw, prep\n",
    "\n",
    "def apply_preproc(X, prep):\n",
    "    \"\"\"Applique le prÃ©proc appris (pas de fuite).\"\"\"\n",
    "    Xp = X.clip(lower=prep[\"lower\"], upper=prep[\"upper\"], axis=1)\n",
    "    if prep[\"norm\"]:\n",
    "        Xp = (Xp - prep[\"mean\"]) / prep[\"std\"].replace(0, 1)\n",
    "    return Xp\n",
    "\n",
    "# ---------- SÃ©lection d'alpha ----------\n",
    "def select_alpha(X_tr_p, y_tr, mode=\"cv\"):\n",
    "    \"\"\"Renvoie (alpha, cv_mae) si mode='cv', sinon (alpha, np.nan).\"\"\"\n",
    "    if mode == \"cv\":\n",
    "        model = Ridge(fit_intercept=True)\n",
    "        grid = GridSearchCV(\n",
    "            model,\n",
    "            {\"alpha\": alpha_grid},\n",
    "            scoring=\"neg_mean_absolute_error\",\n",
    "            cv=5,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        grid.fit(X_tr_p, y_tr.values)\n",
    "        best_alpha = float(grid.best_estimator_.alpha)\n",
    "        cv_mae = float(-grid.best_score_)\n",
    "        return best_alpha, cv_mae\n",
    "    else:\n",
    "        # mode = valeur fixe (float)\n",
    "        try:\n",
    "            a = float(mode)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"alpha_mode doit Ãªtre 'cv' ou un float. ReÃ§u: {mode}\") from e\n",
    "        return a, np.nan\n",
    "\n",
    "# ---------- Bootstrap utils ----------\n",
    "def block_bootstrap_rows(index, L, rng):\n",
    "    \"\"\"\n",
    "    Moving-block bootstrap sur index (positions).\n",
    "    Renvoie un array d'indices (longueur = n).\n",
    "    \"\"\"\n",
    "    n = len(index)\n",
    "    if n < 3:\n",
    "        return np.arange(n)  # fallback\n",
    "    L = max(2, min(int(L), n-1))\n",
    "    nb = int(np.ceil(n / L))\n",
    "    starts = rng.integers(0, n - L + 1, size=nb)\n",
    "    ix = np.concatenate([np.arange(s, s+L) for s in starts])[:n]\n",
    "    return ix\n",
    "\n",
    "def bagged_predict_ridge(X_tr_raw, y_tr, x_fore_raw, prep, alpha, B, L, rng):\n",
    "    \"\"\"\n",
    "    Bagging (moving-block bootstrap) pour Ridge :\n",
    "      - prÃ©proc fixÃ© sur TRAIN original (pas rÃ©-appris)\n",
    "      - rÃ©Ã©chantillon par blocs (lignes) (X, y)\n",
    "      - fit et prÃ©diction h\n",
    "      - renvoie (moyenne, distribution complÃ¨te, base_pred)\n",
    "    \"\"\"\n",
    "    # Base fit (rÃ©fÃ©rence)\n",
    "    X_tr_p = apply_preproc(X_tr_raw, prep)\n",
    "    base = Ridge(alpha=alpha, fit_intercept=True)\n",
    "    base.fit(X_tr_p, y_tr.values)\n",
    "    yhat_base = float(base.predict(apply_preproc(x_fore_raw, prep))[0])\n",
    "\n",
    "    preds = []\n",
    "    for _ in range(B):\n",
    "        ix = block_bootstrap_rows(X_tr_raw.index, L, rng)\n",
    "        Xb = X_tr_raw.iloc[ix]\n",
    "        yb = y_tr.iloc[ix]\n",
    "        Xb_p = apply_preproc(Xb, prep)  # IMPORTANT: mÃªme prep\n",
    "        m = Ridge(alpha=alpha, fit_intercept=True)\n",
    "        m.fit(Xb_p, yb.values)\n",
    "        preds.append(float(m.predict(apply_preproc(x_fore_raw, prep))[0]))\n",
    "    return float(np.mean(preds)), np.array(preds), yhat_base\n",
    "\n",
    "# ---------- Boucle pseudo-OOS ----------\n",
    "rows = []                 # (date, y_pred, y_true, y_pred_base, p05, p95)\n",
    "models = []               # stockage dernier fit (optionnel)\n",
    "preprocs = []             # stockage prep (optionnel)\n",
    "train_ends = []           # dates de fin train (pour trace)\n",
    "alpha_history = []        # alpha utilisÃ© par fenÃªtre\n",
    "cv_mae_history = []       # MAE CV (si mode=\"cv\"), sinon NaN\n",
    "\n",
    "last_t_end = y_all.index.max() - relativedelta(months=h)\n",
    "last_model = None\n",
    "last_fit_end = None\n",
    "\n",
    "for t_end in y_all.index:\n",
    "    if t_end > last_t_end:\n",
    "        break\n",
    "\n",
    "    y_tr = y_all.loc[:t_end]\n",
    "    X_tr = X_all.loc[:t_end]\n",
    "    if len(y_tr) < min_train_n:\n",
    "        continue\n",
    "\n",
    "    # PrÃ©proc appris sur TRAIN courant\n",
    "    X_tr_p, prep = fit_preproc(X_tr, wins=winsor_level, do_norm=norm_var)\n",
    "\n",
    "    # Choix alpha (CV ou fixe)\n",
    "    alpha, cv_mae = select_alpha(X_tr_p, y_tr, mode=alpha_mode)\n",
    "    alpha_history.append(alpha)\n",
    "    cv_mae_history.append(cv_mae)\n",
    "\n",
    "    # Horizon ciblÃ©\n",
    "    t_fore = t_end + relativedelta(months=h)\n",
    "    if t_fore in y_all.index:\n",
    "        x_fore_raw = X_all.loc[[t_fore]]\n",
    "\n",
    "        if use_bagging:\n",
    "            # (Option) reseed par mois : rng = np.random.default_rng(int(t_end.strftime(\"%Y%m\")))\n",
    "            yhat_h, dist, yhat_base = bagged_predict_ridge(\n",
    "                X_tr_raw=X_tr, y_tr=y_tr, x_fore_raw=x_fore_raw,\n",
    "                prep=prep, alpha=alpha, B=B_boot, L=L_block, rng=rng\n",
    "            )\n",
    "            y_p05 = float(np.percentile(dist, 5))\n",
    "            y_p95 = float(np.percentile(dist, 95))\n",
    "        else:\n",
    "            model_tmp = Ridge(alpha=alpha, fit_intercept=True)\n",
    "            model_tmp.fit(X_tr_p, y_tr.values)\n",
    "            yhat_h = float(model_tmp.predict(apply_preproc(x_fore_raw, prep))[0])\n",
    "            yhat_base = yhat_h\n",
    "            y_p05, y_p95 = (np.nan, np.nan)\n",
    "\n",
    "        rows.append((t_fore, yhat_h, float(y_all.loc[t_fore]), yhat_base, y_p05, y_p95))\n",
    "\n",
    "    # trace / dernier modÃ¨le base (utile pour sauvegarde)\n",
    "    last_model = Ridge(alpha=alpha, fit_intercept=True).fit(X_tr_p, y_tr.values)\n",
    "    last_fit_end = t_end\n",
    "    models.append(last_model)\n",
    "    preprocs.append(prep)\n",
    "    train_ends.append(t_end)\n",
    "\n",
    "# ---------- DataFrame OOS ----------\n",
    "if rows:\n",
    "    df_oos = (\n",
    "        pd.DataFrame(rows, columns=[\"date\", \"y_pred\", \"y_true\", \"y_pred_base\", \"y_pred_p05\", \"y_pred_p95\"])\n",
    "          .assign(date=lambda d: pd.to_datetime(d[\"date\"]).dt.to_period(\"M\").dt.to_timestamp(how=\"start\"))\n",
    "          .set_index(\"date\").sort_index()\n",
    "    )\n",
    "else:\n",
    "    df_oos = pd.DataFrame(columns=[\"y_pred\", \"y_true\", \"y_pred_base\", \"y_pred_p05\", \"y_pred_p95\"])\n",
    "    df_oos.index = pd.to_datetime(pd.Index([]))\n",
    "\n",
    "print(f\"\\nâœ… Pseudo-OOS (Ridge) terminÃ© â€” n prÃ©visions = {len(df_oos)}\")\n",
    "print(df_oos.head(3))\n",
    "\n",
    "# ---------- Scores ----------\n",
    "def _scores(df):\n",
    "    if len(df) == 0:\n",
    "        return {\"MAE\": np.nan, \"RMSE\": np.nan, \"R2\": np.nan}\n",
    "    mae  = mean_absolute_error(df[\"y_true\"], df[\"y_pred\"])\n",
    "    rmse = np.sqrt(mean_squared_error(df[\"y_true\"], df[\"y_pred\"]))\n",
    "    r2   = r2_score(df[\"y_true\"], df[\"y_pred\"]) if len(df) > 1 else np.nan\n",
    "    return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2)}\n",
    "\n",
    "df_val  = df_oos.loc[eval_start:eval_end].copy()\n",
    "df_test = df_oos.loc[test_start:test_end].copy()\n",
    "\n",
    "sc_val  = _scores(df_val)\n",
    "sc_test = _scores(df_test)\n",
    "\n",
    "print(f\"\\nðŸ“Š Validation 83â€“89 â€” n={len(df_val)} | MAE={sc_val['MAE']:.3f} | RMSE={sc_val['RMSE']:.3f} | RÂ²={sc_val['R2']:.3f}\")\n",
    "print(f\"ðŸ“Š Test 90â€“2025 â€” n={len(df_test)} | MAE={sc_test['MAE']:.3f} | RMSE={sc_test['RMSE']:.3f} | RÂ²={sc_test['R2']:.3f}\")\n",
    "\n",
    "# (option) Comparaison bagging vs base\n",
    "if \"y_pred_base\" in df_oos and df_oos[\"y_pred_base\"].notna().any():\n",
    "    mae_bag  = mean_absolute_error(df_oos[\"y_true\"], df_oos[\"y_pred\"])\n",
    "    mae_base = mean_absolute_error(df_oos[\"y_true\"], df_oos[\"y_pred_base\"])\n",
    "    print(f\"âž¡ï¸  Gain bagging (Î”MAE) = {mae_base - mae_bag:.3f}\")\n",
    "\n",
    "# ---------- Sauvegardes ----------\n",
    "bundle = {\n",
    "    \"oos_predictions\": df_oos.reset_index(),     # (date, y_pred, y_true, y_pred_base, y_pred_p05, y_pred_p95)\n",
    "    \"params\": {\n",
    "        \"model\": \"Ridge\",\n",
    "        \"horizon\": h,\n",
    "        \"min_train_n\": min_train_n,\n",
    "        \"winsor_level\": winsor_level,\n",
    "        \"norm_var\": norm_var,\n",
    "        \"features\": features,\n",
    "        \"eval_window\": (str(eval_start.date()), str(eval_end.date())),\n",
    "        \"test_window\": (str(test_start.date()), str(test_end.date())),\n",
    "        # ---- bagging ----\n",
    "        \"use_bagging\": bool(use_bagging),\n",
    "        \"B_boot\": int(B_boot),\n",
    "        \"L_block\": int(L_block),\n",
    "        # ---- alpha ----\n",
    "        \"alpha_mode\": alpha_mode,\n",
    "        \"alpha_grid\": list(alpha_grid) if alpha_mode == \"cv\" else None,\n",
    "        \"best_alpha_last\": (alpha_history[-1] if len(alpha_history) else None),\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"trained_until\": str(last_fit_end.date()) if last_fit_end is not None else None,\n",
    "        \"index_freq\": \"MS\",\n",
    "        \"n_obs_all\": int(len(df_all)),\n",
    "        \"n_forecasts\": int(len(df_oos)),\n",
    "        \"alpha_history\": alpha_history,\n",
    "        \"cv_mae_history\": cv_mae_history,\n",
    "    },\n",
    "    \"train_fit_dates\": pd.to_datetime(pd.Index(train_ends)),\n",
    "\n",
    "    # âœ… Pour permutation_importance_pseudo_oos & SHAP\n",
    "    \"models\":   models,     # liste des modÃ¨les Ridge (un par fenÃªtre)\n",
    "    \"preprocs\": preprocs,   # liste des prÃ©proc (dict) alignÃ©s aux modÃ¨les\n",
    "}\n",
    "\n",
    "# --- Sauvegarde du bundle complet ---\n",
    "with open(RIDGE_PKL, \"wb\") as f:\n",
    "    pickle.dump(bundle, f)\n",
    "\n",
    "# --- Sauvegarde du rÃ©sumÃ© mÃ©ta sÃ©parÃ© (lisible rapidement) ---\n",
    "pd.DataFrame([{\n",
    "    \"model\": \"Ridge\",\n",
    "    \"horizon\": h,\n",
    "    \"min_train_n\": min_train_n,\n",
    "    \"winsor_level\": winsor_level,\n",
    "    \"norm_var\": norm_var,\n",
    "    \"use_bagging\": bool(use_bagging),\n",
    "    \"B_boot\": int(B_boot),\n",
    "    \"L_block\": int(L_block),\n",
    "    \"alpha_mode\": alpha_mode,\n",
    "    \"best_alpha_last\": bundle[\"params\"][\"best_alpha_last\"],\n",
    "    \"trained_until\": bundle[\"meta\"][\"trained_until\"],\n",
    "    \"n_forecasts\": bundle[\"meta\"][\"n_forecasts\"],\n",
    "}]).to_csv(RIDGE_META, index=False)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Bundle sauvegardÃ© â†’ {RIDGE_PKL}\")\n",
    "print(f\"ðŸ’¾ MÃ©ta sauvegardÃ©e â†’ {RIDGE_META}\")\n",
    "print(f\"ðŸ“¦ Contenu du bundle : {list(bundle.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a27eb",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdb0d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# LightGBM + Bagging (pseudo-OOS, h=12) â€” structure \"comme Ridge\"\n",
    "# - Refit annuel, retune hyperparams tous les 36 mois (dÃ¨s 1983)\n",
    "# - hv-block CV (5 folds, gap=12), scoring=MAE\n",
    "# - Winsorisation 1%/99% + normalisation (apprises sur TRAIN)\n",
    "# - Bagging : n_boot bootstrap (proportions/indices), moyenne des prÃ©dictions\n",
    "# - Bundle complet : oos_predictions + models + preprocs + train_fit_dates\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# ---------- ParamÃ¨tres gÃ©nÃ©raux ----------\n",
    "h = 12\n",
    "min_train_n = 36\n",
    "winsor_level = 0.01\n",
    "norm_var = True\n",
    "target_col = \"UNRATE\"\n",
    "\n",
    "# FenÃªtres dâ€™Ã©val / test\n",
    "eval_start = pd.Timestamp(\"1983-01-01\")\n",
    "eval_end   = pd.Timestamp(\"1989-12-31\")\n",
    "test_start = pd.Timestamp(\"1990-01-01\")\n",
    "test_end   = pd.Timestamp(\"2025-12-31\")\n",
    "\n",
    "# ---------- Refit/Retune ----------\n",
    "refit_every_months  = 12     # refit chaque 12 mois\n",
    "retune_every_months = 36     # retune hyperparams chaque 36 mois\n",
    "\n",
    "# ---------- Bagging ----------\n",
    "use_bagging = True\n",
    "n_boot = 30\n",
    "bootstrap_proportion = 1.0\n",
    "rng = np.random.default_rng(12345)\n",
    "\n",
    "# ---------- Fichiers de sortie ----------\n",
    "LGBM_BUNDLE     = \"lightgbm_regression.pkl\"\n",
    "LGBM_META       = \"lightgbm_regression_meta.csv\"\n",
    "LGBM_LAST_PKL   = \"LGBM_last_trained_model.pkl\"\n",
    "LGBM_LAST_META  = \"LGBM_last_trained_model_meta.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93c56304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DonnÃ©es prÃªtes : 1960-01-01 â†’ 2025-08-01 | n=788 | freq=MS\n",
      "Features (10): ['TB3MS', 'RPI', 'INDPRO', 'DPCERA3M086SBEA', 'S&P500', 'BUSLOANS'] ...\n"
     ]
    }
   ],
   "source": [
    "# ---------- PrÃ©paration df_stationary ----------\n",
    "def _ensure_ms_index(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Index mensuel (MS). Si 'date' existe, on l'utilise comme index.\"\"\"\n",
    "    df = df.copy()\n",
    "    if \"date\" in df.columns:\n",
    "        df = df.set_index(\"date\")\n",
    "    idx = pd.to_datetime(df.index)\n",
    "    df.index = idx.to_period(\"M\").to_timestamp(how=\"start\")\n",
    "    return df.asfreq(\"MS\")\n",
    "\n",
    "# âš ï¸ On suppose df_stationary dispo\n",
    "df_all = _ensure_ms_index(df_stationary).sort_index()\n",
    "\n",
    "if target_col not in df_all.columns:\n",
    "    raise ValueError(f\"La cible '{target_col}' est absente de df_stationary.\")\n",
    "\n",
    "y_all = df_all[target_col].astype(float)\n",
    "X_all = df_all.drop(columns=[target_col]).astype(float)\n",
    "\n",
    "# LightGBM : Ã©viter espaces dans noms de colonnes\n",
    "X_all.columns = [str(c).replace(\" \", \"_\") for c in X_all.columns]\n",
    "features = list(X_all.columns)\n",
    "\n",
    "print(f\"âœ… DonnÃ©es prÃªtes : {df_all.index.min().date()} â†’ {df_all.index.max().date()} | n={len(df_all)} | freq=MS\")\n",
    "print(f\"Features ({len(features)}): {features[:6]}{' ...' if len(features)>6 else ''}\")\n",
    "\n",
    "# ---------- PrÃ©proc ----------\n",
    "def fit_preproc(X: pd.DataFrame, wins=0.01, do_norm=True):\n",
    "    lower = X.quantile(wins)\n",
    "    upper = X.quantile(1 - wins)\n",
    "    Xw = X.clip(lower=lower, upper=upper, axis=1)\n",
    "    if do_norm:\n",
    "        mean = Xw.mean()\n",
    "        std  = Xw.std().replace(0, 1)\n",
    "        Xn   = (Xw - mean) / std\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": mean, \"std\": std, \"norm\": True}\n",
    "        return Xn, prep\n",
    "    else:\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": None, \"std\": None, \"norm\": False}\n",
    "        return Xw, prep\n",
    "\n",
    "def apply_preproc(X: pd.DataFrame, prep: dict):\n",
    "    Xp = X.clip(lower=prep[\"lower\"], upper=prep[\"upper\"], axis=1)\n",
    "    if prep[\"norm\"]:\n",
    "        Xp = (Xp - prep[\"mean\"]) / prep[\"std\"].replace(0, 1)\n",
    "    return Xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8a01c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- hv-block CV ----------\n",
    "class HVBlockCV:\n",
    "    def __init__(self, n_splits=5, gap=12):\n",
    "        self.n_splits = n_splits\n",
    "        self.gap = gap\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n = len(X)\n",
    "        fold_sizes = np.full(self.n_splits, n // self.n_splits, dtype=int)\n",
    "        fold_sizes[: n % self.n_splits] += 1\n",
    "        idx = np.arange(n)\n",
    "        cur = 0\n",
    "        for fs in fold_sizes:\n",
    "            start, stop = cur, cur + fs\n",
    "            test_idx = idx[start:stop]\n",
    "            train_mask = np.ones(n, dtype=bool)\n",
    "            left = max(0, start - self.gap)\n",
    "            right = min(n, stop + self.gap)\n",
    "            train_mask[left:right] = False\n",
    "            train_idx = idx[train_mask]\n",
    "            cur = stop\n",
    "            if len(train_idx) == 0 or len(test_idx) == 0:\n",
    "                continue\n",
    "            yield train_idx, test_idx\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21e175d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Grille LightGBM (stabilitÃ©) ----------\n",
    "param_dist = {\n",
    "    \"subsample\":        [0.05,.1,.2,.3,.4,.5,.6,.7,.8,.9,1.0],\n",
    "    \"colsample_bytree\": [.2,.3,.4,.5,.6,.7,1.0],\n",
    "    \"num_leaves\":       [2,3,4,5,8,10,20,40,70,100],\n",
    "    \"n_estimators\":     [5,10,20,30,40,50,75,100],\n",
    "    \"max_depth\":        [1,2,3,5,8,15,-1],          # -1 = illimitÃ©\n",
    "    \"reg_alpha\":        [0, .1, 1, 2, 7, 10, 50, 100],\n",
    "    \"reg_lambda\":       [0, .1, 1, 10, 20, 50, 100],\n",
    "    \"min_child_samples\":[5,10,15],\n",
    "    \"min_split_gain\":   [0.0, 0.01, 0.05],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca6f7f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lgbm(X_tr_p: pd.DataFrame, y_tr: np.ndarray, seed=12345):\n",
    "    cv = HVBlockCV(n_splits=5, gap=12)\n",
    "    base = LGBMRegressor(\n",
    "        boosting_type=\"gbdt\",\n",
    "        objective=\"regression\",\n",
    "        importance_type=\"gain\",\n",
    "        random_state=seed,\n",
    "        n_jobs=1,    # Ã©viter sur-parallÃ©lisation quand RandomizedSearchCV utilise n_jobs=-1\n",
    "        verbose=-1\n",
    "    )\n",
    "    rs = RandomizedSearchCV(\n",
    "        estimator=base,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=100,\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=seed,\n",
    "        refit=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    rs.fit(X_tr_p, y_tr)\n",
    "    return rs.best_params_, float(-rs.best_score_), rs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f79bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Bootstrap utils ----------\n",
    "def bootstrap_indices(n, proportion=1.0, seed=None):\n",
    "    m = int(round(n * proportion))\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    return rng_local.integers(0, n, size=m, endpoint=False)\n",
    "\n",
    "def bagged_predict_lgbm(X_tr_raw, y_tr, x_fore_raw, prep, best_params, B, proportion, seed0):\n",
    "    \"\"\"\n",
    "    Bagging LightGBM :\n",
    "      - prÃ©proc fixe (appris sur TRAIN original)\n",
    "      - bootstrap simple d'indices (proportion)\n",
    "      - fit LGBM(**best_params) et prÃ©diction\n",
    "      - retourne la moyenne + distribution + prÃ©diction base (fit sur tout TRAIN)\n",
    "    \"\"\"\n",
    "    # Base (rÃ©fÃ©rence) : fit sur tout TRAIN prÃ©traitÃ©\n",
    "    X_tr_p = apply_preproc(X_tr_raw, prep)\n",
    "    base = LGBMRegressor(\n",
    "        boosting_type=\"gbdt\",\n",
    "        objective=\"regression\",\n",
    "        importance_type=\"gain\",\n",
    "        n_jobs=1,\n",
    "        random_state=seed0,\n",
    "        verbose=-1,\n",
    "        **best_params\n",
    "    )\n",
    "    base.fit(X_tr_p, y_tr.values)\n",
    "    yhat_base = float(base.predict(apply_preproc(x_fore_raw, prep))[0])\n",
    "\n",
    "    preds = []\n",
    "    n = len(X_tr_raw)\n",
    "    for b in range(B):\n",
    "        ix = bootstrap_indices(n, proportion=proportion, seed=seed0 + b)\n",
    "        Xb = X_tr_raw.iloc[ix]\n",
    "        yb = y_tr.iloc[ix]\n",
    "        Xb_p, _ = fit_preproc(Xb, wins=winsor_level, do_norm=norm_var)  # prÃ©proc appris sur bootstrap\n",
    "        m = LGBMRegressor(\n",
    "            boosting_type=\"gbdt\",\n",
    "            objective=\"regression\",\n",
    "            importance_type=\"gain\",\n",
    "            n_jobs=1,\n",
    "            random_state=seed0 + b,\n",
    "            verbose=-1,\n",
    "            **best_params\n",
    "        )\n",
    "        m.fit(Xb_p, yb.values)\n",
    "        preds.append(float(m.predict(apply_preproc(x_fore_raw, _))[0]))  # appliquer le prep du bootstrap (_)\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    return float(np.mean(preds)), preds, yhat_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8e31984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Validation 83â€“89 â€” n=84 | MAE=0.788 | RMSE=1.040 | RÂ²=-0.385\n",
      "ðŸ“Š Test 90â€“2025 â€” n=428 | MAE=0.777 | RMSE=1.379 | RÂ²=0.183\n"
     ]
    }
   ],
   "source": [
    "# ---------- Boucle pseudo-OOS ----------\n",
    "rows = []                 # (date, y_pred, y_true, y_pred_base, p05, p95)\n",
    "models = []               # modÃ¨les \"base\" LGBM par refit (pour permutation/SHAP)\n",
    "preprocs = []             # prÃ©procs alignÃ©s\n",
    "train_ends = []           # dates de refit\n",
    "cv_mae_history = []       # historique MAE CV lors des retunes\n",
    "best_params_hist = []     # historique des params\n",
    "\n",
    "last_fit_end = None\n",
    "last_t_end   = y_all.index.max() - relativedelta(months=h)\n",
    "last_refit_t = None\n",
    "last_tune_t  = None\n",
    "\n",
    "best_params = {}          # params courants (remplis au 1er retune)\n",
    "base_model = None         # sÃ©curitÃ© si use_bagging=False\n",
    "boot_seed = 12345         # reseed Ã  chaque refit\n",
    "seed0 = 12345\n",
    "\n",
    "for t_end in y_all.index:\n",
    "    if t_end > last_t_end:\n",
    "        break\n",
    "\n",
    "    y_tr = y_all.loc[:t_end]\n",
    "    X_tr = X_all.loc[:t_end]\n",
    "    if len(y_tr) < min_train_n:\n",
    "        continue\n",
    "\n",
    "    # cadence refit\n",
    "    if last_refit_t is None:\n",
    "        need_refit = True\n",
    "    else:\n",
    "        months_since_refit = (t_end.year - last_refit_t.year)*12 + (t_end.month - last_refit_t.month)\n",
    "        need_refit = months_since_refit >= refit_every_months\n",
    "\n",
    "    # cadence retune\n",
    "    need_tune = False\n",
    "    if t_end >= eval_start:\n",
    "        if last_tune_t is None:\n",
    "            need_tune = True\n",
    "        else:\n",
    "            months_since_tune = (t_end.year - last_tune_t.year)*12 + (t_end.month - last_tune_t.month)\n",
    "            need_tune = months_since_tune >= retune_every_months\n",
    "\n",
    "    # PrÃ©proc global (pour tuning/refit)\n",
    "    X_tr_p_global, prep_global = fit_preproc(X_tr, wins=winsor_level, do_norm=norm_var)\n",
    "\n",
    "    if need_tune:\n",
    "        best_params, best_cv_mae, _ = tune_lgbm(X_tr_p_global, y_tr.values, seed=seed0)\n",
    "        last_tune_t = t_end\n",
    "        cv_mae_history.append(best_cv_mae)\n",
    "        best_params_hist.append(best_params.copy())\n",
    "    else:\n",
    "        cv_mae_history.append(np.nan)\n",
    "        best_params_hist.append(best_params.copy() if best_params else {})\n",
    "\n",
    "    # Valeurs sÃ»res si aucun retune n'a encore eu lieu\n",
    "    if not best_params:\n",
    "        best_params = dict(\n",
    "            subsample=0.7, colsample_bytree=0.7, num_leaves=31,\n",
    "            n_estimators=100, max_depth=-1, reg_alpha=0.0, reg_lambda=0.0,\n",
    "            min_child_samples=10, min_split_gain=0.0\n",
    "        )\n",
    "\n",
    "    if need_refit:\n",
    "        # ModÃ¨le \"base\" stockÃ© pour permutation/SHAP\n",
    "        base_model = LGBMRegressor(\n",
    "            boosting_type=\"gbdt\",\n",
    "            objective=\"regression\",\n",
    "            importance_type=\"gain\",\n",
    "            n_jobs=1,\n",
    "            random_state=seed0,\n",
    "            verbose=-1,\n",
    "            **best_params\n",
    "        )\n",
    "        base_model.fit(X_tr_p_global, y_tr.values)\n",
    "        models.append(base_model)\n",
    "        preprocs.append(prep_global)\n",
    "\n",
    "        train_ends.append(t_end)\n",
    "        last_refit_t = t_end\n",
    "        last_fit_end = t_end\n",
    "\n",
    "        # reseed bagging Ã  chaque refit\n",
    "        boot_seed += 9973\n",
    "\n",
    "    # PrÃ©vision h=12\n",
    "    t_fore = t_end + relativedelta(months=h)\n",
    "    if t_fore in y_all.index:\n",
    "        x_fore_raw = X_all.loc[[t_fore]]\n",
    "\n",
    "        if use_bagging:\n",
    "            yhat, dist, yhat_base = bagged_predict_lgbm(\n",
    "                X_tr_raw=X_tr, y_tr=y_tr, x_fore_raw=x_fore_raw,\n",
    "                prep=prep_global, best_params=best_params,\n",
    "                B=n_boot, proportion=bootstrap_proportion, seed0=boot_seed\n",
    "            )\n",
    "            y_p05 = float(np.percentile(dist, 5))\n",
    "            y_p95 = float(np.percentile(dist, 95))\n",
    "        else:\n",
    "            # sans bagging : utiliser / garantir un base_model\n",
    "            if base_model is None:\n",
    "                base_model = LGBMRegressor(\n",
    "                    boosting_type=\"gbdt\",\n",
    "                    objective=\"regression\",\n",
    "                    importance_type=\"gain\",\n",
    "                    n_jobs=1,\n",
    "                    random_state=seed0,\n",
    "                    verbose=-1,\n",
    "                    **best_params\n",
    "                ).fit(X_tr_p_global, y_tr.values)\n",
    "            yhat = float(base_model.predict(apply_preproc(x_fore_raw, prep_global))[0])\n",
    "            yhat_base = yhat\n",
    "            y_p05, y_p95 = (np.nan, np.nan)\n",
    "\n",
    "        rows.append((t_fore, yhat, float(y_all.loc[t_fore]), yhat_base, y_p05, y_p95))\n",
    "\n",
    "# ---------- DataFrame OOS ----------\n",
    "if rows:\n",
    "    df_oos = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\"date\", \"y_pred\", \"y_true\", \"y_pred_base\", \"y_pred_p05\", \"y_pred_p95\"]\n",
    "    )\n",
    "    # âœ… Conversion date â†’ dÃ©but de mois (MS)\n",
    "    df_oos[\"date\"] = pd.to_datetime(df_oos[\"date\"]).dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "    df_oos = df_oos.set_index(\"date\").sort_index()\n",
    "else:\n",
    "    df_oos = pd.DataFrame(columns=[\"y_pred\", \"y_true\", \"y_pred_base\", \"y_pred_p05\", \"y_pred_p95\"])\n",
    "    df_oos.index = pd.to_datetime(pd.Index([]))\n",
    "\n",
    "# ---------- Scores ----------\n",
    "def _scores(df):\n",
    "    if len(df) == 0:\n",
    "        return {\"MAE\": np.nan, \"RMSE\": np.nan, \"R2\": np.nan}\n",
    "    mae  = mean_absolute_error(df[\"y_true\"], df[\"y_pred\"])\n",
    "    rmse = np.sqrt(mean_squared_error(df[\"y_true\"], df[\"y_pred\"]))\n",
    "    ssr  = np.sum((df[\"y_true\"] - df[\"y_pred\"])**2)\n",
    "    sst  = np.sum((df[\"y_true\"] - df[\"y_true\"].mean())**2)\n",
    "    r2   = 1 - ssr/sst if sst > 0 else np.nan\n",
    "    return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2)}\n",
    "\n",
    "df_val  = df_oos.loc[eval_start:eval_end].copy()\n",
    "df_test = df_oos.loc[test_start:test_end].copy()\n",
    "\n",
    "sc_val  = _scores(df_val)\n",
    "sc_test = _scores(df_test)\n",
    "\n",
    "print(f\"\\nðŸ“Š Validation 83â€“89 â€” n={len(df_val)} | MAE={sc_val['MAE']:.3f} | RMSE={sc_val['RMSE']:.3f} | RÂ²={sc_val['R2']:.3f}\")\n",
    "print(f\"ðŸ“Š Test 90â€“{test_end.year} â€” n={len(df_test)} | MAE={sc_test['MAE']:.3f} | RMSE={sc_test['RMSE']:.3f} | RÂ²={sc_test['R2']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90d47e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Bundle OOS sauvegardÃ© â†’ lightgbm_regression.pkl\n",
      "ðŸ’¾ MÃ©ta bundle       â†’ lightgbm_regression_meta.csv\n",
      "ðŸ’¾ Dernier modÃ¨le    â†’ LGBM_last_trained_model.pkl\n",
      "ðŸ’¾ MÃ©ta dernier fit  â†’ LGBM_last_trained_model_meta.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------- Sauvegardes ----------\n",
    "bundle = {\n",
    "    \"oos_predictions\": df_oos.reset_index(),\n",
    "    \"params\": {\n",
    "        \"model\": \"LightGBM + Bagging\",\n",
    "        \"horizon\": h,\n",
    "        \"min_train_n\": min_train_n,\n",
    "        \"winsor_level\": winsor_level,\n",
    "        \"norm_var\": norm_var,\n",
    "        \"features\": features,\n",
    "        \"eval_window\": (str(eval_start.date()), str(eval_end.date())),\n",
    "        \"test_window\": (str(test_start.date()), str(test_end.date())),\n",
    "        # plan refit/retune\n",
    "        \"refit_every_months\": refit_every_months,\n",
    "        \"retune_every_months\": retune_every_months,\n",
    "        # search\n",
    "        \"hyper_search\": \"RandomizedSearchCV (100 iters) + hv-block CV (5 folds, gap=12), scoring=MAE\",\n",
    "        \"best_params_last\": best_params.copy(),\n",
    "        # bagging\n",
    "        \"use_bagging\": bool(use_bagging),\n",
    "        \"n_boot\": int(n_boot),\n",
    "        \"bootstrap_proportion\": float(bootstrap_proportion),\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"trained_until\": str(last_fit_end.date()) if last_fit_end is not None else None,\n",
    "        \"index_freq\": \"MS\",\n",
    "        \"n_obs_all\": int(len(df_all)),\n",
    "        \"n_forecasts\": int(len(df_oos)),\n",
    "        \"cv_mae_history\": cv_mae_history,\n",
    "        \"best_params_history\": best_params_hist\n",
    "    },\n",
    "    \"train_fit_dates\": pd.to_datetime(pd.Index(train_ends)),\n",
    "\n",
    "    # âœ… Pour permutation_importance_pseudo_oos & SHAP\n",
    "    \"models\":   models,     # liste des modÃ¨les \"base\" LGBM (un par refit)\n",
    "    \"preprocs\": preprocs,   # liste des prÃ©proc (dict) alignÃ©s aux modÃ¨les\n",
    "}\n",
    "\n",
    "with open(LGBM_BUNDLE, \"wb\") as f:\n",
    "    pickle.dump(bundle, f)\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"model\": \"LightGBM+Bagging\",\n",
    "    \"horizon\": h,\n",
    "    \"min_train_n\": min_train_n,\n",
    "    \"winsor_level\": winsor_level,\n",
    "    \"norm_var\": norm_var,\n",
    "    \"refit_every_months\": refit_every_months,\n",
    "    \"retune_every_months\": retune_every_months,\n",
    "    \"use_bagging\": bool(use_bagging),\n",
    "    \"n_boot\": int(n_boot),\n",
    "    \"bootstrap_proportion\": float(bootstrap_proportion),\n",
    "    \"trained_until\": bundle[\"meta\"][\"trained_until\"],\n",
    "    \"n_forecasts\": bundle[\"meta\"][\"n_forecasts\"]\n",
    "}]).to_csv(LGBM_META, index=False)\n",
    "\n",
    "# Artefact dernier ensemble (optionnel : pour audit)\n",
    "lgbm_artifact = {\n",
    "    \"trained_until\": bundle[\"meta\"][\"trained_until\"],\n",
    "    \"horizon\": h,\n",
    "    \"features\": features,\n",
    "    \"n_models_base\": len(models),\n",
    "    \"best_params_last\": best_params.copy(),\n",
    "}\n",
    "with open(LGBM_LAST_PKL, \"wb\") as f:\n",
    "    pickle.dump(lgbm_artifact, f)\n",
    "pd.DataFrame([{\n",
    "    \"trained_until\": lgbm_artifact[\"trained_until\"],\n",
    "    \"n_features\": len(features),\n",
    "    \"horizon\": h,\n",
    "    \"n_models_base\": lgbm_artifact[\"n_models_base\"]\n",
    "}]).to_csv(LGBM_LAST_META, index=False)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Bundle OOS sauvegardÃ© â†’ {LGBM_BUNDLE}\")\n",
    "print(f\"ðŸ’¾ MÃ©ta bundle       â†’ {LGBM_META}\")\n",
    "print(f\"ðŸ’¾ Dernier modÃ¨le    â†’ {LGBM_LAST_PKL}\")\n",
    "print(f\"ðŸ’¾ MÃ©ta dernier fit  â†’ {LGBM_LAST_META}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f94243d",
   "metadata": {},
   "source": [
    "# LightGBM avec taux de chÃ´mage en retards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b81b7681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajout feature: UNRATE_lag12 (y_(t-h)) â†’ OK\n",
      "âœ… DonnÃ©es prÃªtes : 1960-01-01 â†’ 2025-08-01 | n=788 | freq=MS\n",
      "Features (11): ['TB3MS', 'RPI', 'INDPRO', 'DPCERA3M086SBEA', 'S&P500', 'BUSLOANS'] ...\n",
      "\n",
      "âœ… Pseudo-OOS (LightGBM, with_UNRATE_lags_h12) â€” n prÃ©visions = 741\n",
      "\n",
      "ðŸ“Š Validation 83â€“89 â€” n=84 | MAE=0.749 | RMSE=1.024 | RÂ²=-0.344\n",
      "ðŸ“Š Test 90â€“2025 â€” n=428 | MAE=0.761 | RMSE=1.436 | RÂ²=0.113\n",
      "\n",
      "ðŸ’¾ Bundle OOS sauvegardÃ© â†’ lightgbm_regression__with_UNRATE_lags_h12.pkl\n",
      "ðŸ’¾ MÃ©ta bundle       â†’ lightgbm_regression_meta__with_UNRATE_lags_h12.csv\n",
      "ðŸ’¾ Dernier modÃ¨le    â†’ LGBM_last_trained_model__with_UNRATE_lags_h12.pkl\n",
      "ðŸ’¾ MÃ©ta dernier fit  â†’ LGBM_last_trained_model_meta__with_UNRATE_lags_h12.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# ðŸ”¹ LightGBM + Bagging en blocs (pseudo-OOS, h=12)\n",
    "#    â†’ variante AVEC RETARDS (ajout y_{t-h})\n",
    "#    â†’ artefacts taggÃ©s: with_<target>_lags_h<h>\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# ---------- ParamÃ¨tres gÃ©nÃ©raux ----------\n",
    "h = 12\n",
    "min_train_n = 36\n",
    "winsor_level = 0.01\n",
    "norm_var = True\n",
    "target_col = \"UNRATE\"\n",
    "\n",
    "# FenÃªtres dâ€™Ã©val / test\n",
    "eval_start = pd.Timestamp(\"1983-01-01\")\n",
    "eval_end   = pd.Timestamp(\"1989-12-31\")\n",
    "test_start = pd.Timestamp(\"1990-01-01\")\n",
    "test_end   = pd.Timestamp(\"2025-12-31\")\n",
    "\n",
    "# ---------- Refit/Retune ----------\n",
    "refit_every_months  = 12     # refit chaque 12 mois\n",
    "retune_every_months = 36     # retune hyperparams chaque 36 mois\n",
    "\n",
    "# ---------- Bagging (moving-block bootstrap, comme Ridge) ----------\n",
    "use_bagging = True\n",
    "B_boot = 30                 # nb. de bootstraps\n",
    "L_block = 12                # longueur de bloc (mois)\n",
    "rng = np.random.default_rng(12345)\n",
    "\n",
    "# ---------- Fichiers de sortie (VARIANTE AVEC RETARDS) ----------\n",
    "NAME_TAG = f\"with_{target_col}_lags_h{h}\"   # ex: with_UNRATE_lags_h12\n",
    "LGBM_LAGS_BUNDLE    = f\"lightgbm_regression__{NAME_TAG}.pkl\"\n",
    "LGBM_LAGS_META      = f\"lightgbm_regression_meta__{NAME_TAG}.csv\"\n",
    "LGBM_LAGS_LAST_PKL  = f\"LGBM_last_trained_model__{NAME_TAG}.pkl\"\n",
    "LGBM_LAGS_LAST_META = f\"LGBM_last_trained_model_meta__{NAME_TAG}.csv\"\n",
    "\n",
    "# ---------- PrÃ©paration df_stationary ----------\n",
    "def _ensure_ms_index(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Index mensuel (MS). Si 'date' existe, on l'utilise comme index.\"\"\"\n",
    "    df = df.copy()\n",
    "    if \"date\" in df.columns:\n",
    "        df = df.set_index(\"date\")\n",
    "    idx = pd.to_datetime(df.index)\n",
    "    df.index = idx.to_period(\"M\").to_timestamp(how=\"start\")\n",
    "    return df.asfreq(\"MS\")\n",
    "\n",
    "# âš ï¸ On suppose df_stationary dispo\n",
    "df_all = _ensure_ms_index(df_stationary).sort_index()\n",
    "\n",
    "if target_col not in df_all.columns:\n",
    "    raise ValueError(f\"La cible '{target_col}' est absente de df_stationary.\")\n",
    "\n",
    "y_all = df_all[target_col].astype(float)\n",
    "X_all = df_all.drop(columns=[target_col]).astype(float)\n",
    "\n",
    "# --- Ajout du lag h (=12) de la cible comme variable explicative ---\n",
    "lag_feat_name = f\"{target_col}_lag{h}\"   # ex: 'UNRATE_lag12'\n",
    "X_all[lag_feat_name] = y_all.shift(h)\n",
    "\n",
    "# LightGBM : Ã©viter espaces dans noms de colonnes\n",
    "X_all.columns = [str(c).replace(\" \", \"_\") for c in X_all.columns]\n",
    "features = list(X_all.columns)\n",
    "\n",
    "print(f\"Ajout feature: {lag_feat_name} (y_(t-h)) â†’ OK\")\n",
    "print(f\"âœ… DonnÃ©es prÃªtes : {df_all.index.min().date()} â†’ {df_all.index.max().date()} | n={len(df_all)} | freq=MS\")\n",
    "print(f\"Features ({len(features)}): {features[:6]}{' ...' if len(features)>6 else ''}\")\n",
    "\n",
    "# ---------- PrÃ©proc ----------\n",
    "def fit_preproc(X: pd.DataFrame, wins=0.01, do_norm=True):\n",
    "    lower = X.quantile(wins)\n",
    "    upper = X.quantile(1 - wins)\n",
    "    Xw = X.clip(lower=lower, upper=upper, axis=1)\n",
    "    if do_norm:\n",
    "        mean = Xw.mean()\n",
    "        std  = Xw.std().replace(0, 1)\n",
    "        Xn   = (Xw - mean) / std\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": mean, \"std\": std, \"norm\": True}\n",
    "        return Xn, prep\n",
    "    else:\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": None, \"std\": None, \"norm\": False}\n",
    "        return Xw, prep\n",
    "\n",
    "def apply_preproc(X: pd.DataFrame, prep: dict):\n",
    "    Xp = X.clip(lower=prep[\"lower\"], upper=prep[\"upper\"], axis=1)\n",
    "    if prep[\"norm\"]:\n",
    "        Xp = (Xp - prep[\"mean\"]) / prep[\"std\"].replace(0, 1)\n",
    "    return Xp\n",
    "\n",
    "# ---------- hv-block CV ----------\n",
    "class HVBlockCV:\n",
    "    def __init__(self, n_splits=5, gap=12):\n",
    "        self.n_splits = n_splits\n",
    "        self.gap = gap\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n = len(X)\n",
    "        fold_sizes = np.full(self.n_splits, n // self.n_splits, dtype=int)\n",
    "        fold_sizes[: n % self.n_splits] += 1\n",
    "        idx = np.arange(n)\n",
    "        cur = 0\n",
    "        for fs in fold_sizes:\n",
    "            start, stop = cur, cur + fs\n",
    "            test_idx = idx[start:stop]\n",
    "            train_mask = np.ones(n, dtype=bool)\n",
    "            left = max(0, start - self.gap)\n",
    "            right = min(n, stop + self.gap)\n",
    "            train_mask[left:right] = False\n",
    "            train_idx = idx[train_mask]\n",
    "            cur = stop\n",
    "            if len(train_idx) == 0 or len(test_idx) == 0:\n",
    "                continue\n",
    "            yield train_idx, test_idx\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "# ---------- Grille LightGBM ----------\n",
    "param_dist = {\n",
    "    \"subsample\":        [0.05,.1,.2,.3,.4,.5,.6,.7,.8,.9,1.0],\n",
    "    \"colsample_bytree\": [.2,.3,.4,.5,.6,.7,1.0],\n",
    "    \"num_leaves\":       [2,3,4,5,8,10,20,40,70,100],\n",
    "    \"n_estimators\":     [5,10,20,30,40,50,75,100],\n",
    "    \"max_depth\":        [1,2,3,5,8,15,-1],\n",
    "    \"reg_alpha\":        [0, .1, 1, 2, 7, 10, 50, 100],\n",
    "    \"reg_lambda\":       [0, .1, 1, 10, 20, 50, 100],\n",
    "    \"min_child_samples\":[5,10,15],\n",
    "    \"min_split_gain\":   [0.0, 0.01, 0.05],\n",
    "}\n",
    "\n",
    "def tune_lgbm(X_tr_p: pd.DataFrame, y_tr: np.ndarray, seed=12345):\n",
    "    cv = HVBlockCV(n_splits=5, gap=12)\n",
    "    base = LGBMRegressor(\n",
    "        boosting_type=\"gbdt\",\n",
    "        objective=\"regression\",\n",
    "        importance_type=\"gain\",\n",
    "        random_state=seed,\n",
    "        n_jobs=1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    rs = RandomizedSearchCV(\n",
    "        estimator=base,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=100,\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=seed,\n",
    "        refit=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    rs.fit(X_tr_p, y_tr)\n",
    "    return rs.best_params_, float(-rs.best_score_), rs.best_estimator_\n",
    "\n",
    "# ---------- Bootstrap utils (moving-block comme Ridge) ----------\n",
    "def block_bootstrap_rows(index, L, rng):\n",
    "    n = len(index)\n",
    "    if n < 3:\n",
    "        return np.arange(n)\n",
    "    L = max(2, min(int(L), n-1))\n",
    "    nb = int(np.ceil(n / L))\n",
    "    starts = rng.integers(0, n - L + 1, size=nb)\n",
    "    ix = np.concatenate([np.arange(s, s+L) for s in starts])[:n]\n",
    "    return ix\n",
    "\n",
    "def bagged_predict_lgbm(X_tr_raw, y_tr, x_fore_raw, prep, best_params, B, L, rng):\n",
    "    \"\"\"\n",
    "    Bagging en blocs pour LightGBM (prÃ©proc fixÃ© comme Ridge).\n",
    "    \"\"\"\n",
    "    # Base fit (rÃ©fÃ©rence) sur TRAIN complet prÃ©traitÃ©\n",
    "    X_tr_p = apply_preproc(X_tr_raw, prep)\n",
    "    base = LGBMRegressor(\n",
    "        boosting_type=\"gbdt\",\n",
    "        objective=\"regression\",\n",
    "        importance_type=\"gain\",\n",
    "        n_jobs=1,\n",
    "        random_state=123,\n",
    "        verbose=-1,\n",
    "        **best_params\n",
    "    )\n",
    "    base.fit(X_tr_p, y_tr.values)\n",
    "    yhat_base = float(base.predict(apply_preproc(x_fore_raw, prep))[0])\n",
    "\n",
    "    preds = []\n",
    "    for b in range(B):\n",
    "        ix = block_bootstrap_rows(X_tr_raw.index, L, rng)\n",
    "        Xb = X_tr_raw.iloc[ix]\n",
    "        yb = y_tr.iloc[ix]\n",
    "        Xb_p = apply_preproc(Xb, prep)   # âš ï¸ mÃªme prep\n",
    "        m = LGBMRegressor(\n",
    "            boosting_type=\"gbdt\",\n",
    "            objective=\"regression\",\n",
    "            importance_type=\"gain\",\n",
    "            n_jobs=1,\n",
    "            random_state=123 + b,\n",
    "            verbose=-1,\n",
    "            **best_params\n",
    "        )\n",
    "        m.fit(Xb_p, yb.values)\n",
    "        preds.append(float(m.predict(apply_preproc(x_fore_raw, prep))[0]))\n",
    "    return float(np.mean(preds)), np.array(preds), yhat_base\n",
    "\n",
    "# ---------- Boucle pseudo-OOS ----------\n",
    "def _months_between(a: pd.Timestamp, b: pd.Timestamp) -> int:\n",
    "    return (b.year - a.year) * 12 + (b.month - a.month)\n",
    "\n",
    "rows = []\n",
    "models = []               # modÃ¨les \"base\" LGBM par refit (pour permutation/SHAP)\n",
    "preprocs = []             # prÃ©procs alignÃ©s\n",
    "train_ends = []           # dates de refit\n",
    "cv_mae_history = []       # historique MAE CV lors des retunes\n",
    "best_params_hist = []     # historique des params\n",
    "\n",
    "last_fit_end = None\n",
    "last_t_end   = y_all.index.max() - relativedelta(months=h)\n",
    "last_refit_t = None\n",
    "last_tune_t  = None\n",
    "\n",
    "best_params = {}          # params courants (remplis au 1er retune)\n",
    "base_model = None\n",
    "\n",
    "for t_end in y_all.index:\n",
    "    if t_end > last_t_end:\n",
    "        break\n",
    "\n",
    "    y_tr = y_all.loc[:t_end]\n",
    "    X_tr = X_all.loc[:t_end]\n",
    "    if len(y_tr) < min_train_n:\n",
    "        continue\n",
    "\n",
    "    # cadence refit / retune\n",
    "    need_refit = (last_refit_t is None) or (_months_between(last_refit_t, t_end) >= refit_every_months)\n",
    "    need_tune  = (t_end >= eval_start) and (last_tune_t is None or _months_between(last_tune_t, t_end) >= retune_every_months)\n",
    "\n",
    "    # PrÃ©proc global (fixÃ© pour ce t_end)\n",
    "    X_tr_p_global, prep_global = fit_preproc(X_tr, wins=winsor_level, do_norm=norm_var)\n",
    "\n",
    "    # Tuning si nÃ©cessaire\n",
    "    if need_tune:\n",
    "        best_params, best_cv_mae, _ = tune_lgbm(X_tr_p_global, y_tr.values, seed=12345)\n",
    "        last_tune_t = t_end\n",
    "        cv_mae_history.append(best_cv_mae)\n",
    "        best_params_hist.append(best_params.copy())\n",
    "    else:\n",
    "        cv_mae_history.append(np.nan)\n",
    "        best_params_hist.append(best_params.copy() if best_params else {})\n",
    "\n",
    "    # Valeurs sÃ»res si aucun retune n'a encore eu lieu\n",
    "    if not best_params:\n",
    "        best_params = dict(\n",
    "            subsample=0.7, colsample_bytree=0.7, num_leaves=31,\n",
    "            n_estimators=100, max_depth=-1, reg_alpha=0.0, reg_lambda=0.0,\n",
    "            min_child_samples=10, min_split_gain=0.0\n",
    "        )\n",
    "\n",
    "    # Refit si nÃ©cessaire (stockage du modÃ¨le base + prep alignÃ©s)\n",
    "    if need_refit:\n",
    "        base_model = LGBMRegressor(\n",
    "            boosting_type=\"gbdt\",\n",
    "            objective=\"regression\",\n",
    "            importance_type=\"gain\",\n",
    "            n_jobs=1,\n",
    "            random_state=12345,\n",
    "            verbose=-1,\n",
    "            **best_params\n",
    "        )\n",
    "        base_model.fit(X_tr_p_global, y_tr.values)\n",
    "        models.append(base_model)\n",
    "        preprocs.append(prep_global)\n",
    "        train_ends.append(t_end)\n",
    "        last_refit_t = t_end\n",
    "        last_fit_end = t_end\n",
    "\n",
    "    # PrÃ©vision h=12\n",
    "    t_fore = t_end + relativedelta(months=h)\n",
    "    if t_fore in y_all.index:\n",
    "        x_fore_raw = X_all.loc[[t_fore]]\n",
    "        if use_bagging:\n",
    "            yhat, dist, yhat_base = bagged_predict_lgbm(\n",
    "                X_tr_raw=X_tr, y_tr=y_tr, x_fore_raw=x_fore_raw,\n",
    "                prep=prep_global, best_params=best_params,\n",
    "                B=B_boot, L=L_block, rng=rng\n",
    "            )\n",
    "            y_p05 = float(np.percentile(dist, 5))\n",
    "            y_p95 = float(np.percentile(dist, 95))\n",
    "        else:\n",
    "            if base_model is None:\n",
    "                base_model = LGBMRegressor(\n",
    "                    boosting_type=\"gbdt\",\n",
    "                    objective=\"regression\",\n",
    "                    importance_type=\"gain\",\n",
    "                    n_jobs=1,\n",
    "                    random_state=12345,\n",
    "                    verbose=-1,\n",
    "                    **best_params\n",
    "                ).fit(X_tr_p_global, y_tr.values)\n",
    "            yhat = float(base_model.predict(apply_preproc(x_fore_raw, prep_global))[0])\n",
    "            yhat_base = yhat\n",
    "            y_p05, y_p95 = (np.nan, np.nan)\n",
    "\n",
    "        rows.append((t_fore, yhat, float(y_all.loc[t_fore]), yhat_base, y_p05, y_p95))\n",
    "\n",
    "# ---------- DataFrame OOS ----------\n",
    "if rows:\n",
    "    df_oos = (\n",
    "        pd.DataFrame(rows, columns=[\"date\", \"y_pred\", \"y_true\", \"y_pred_base\", \"y_pred_p05\", \"y_pred_p95\"])\n",
    "          .assign(date=lambda d: pd.to_datetime(d[\"date\"]).dt.to_period(\"M\").dt.to_timestamp(how=\"start\"))\n",
    "          .set_index(\"date\").sort_index()\n",
    "    )\n",
    "else:\n",
    "    df_oos = pd.DataFrame(columns=[\"y_pred\",\"y_true\",\"y_pred_base\",\"y_pred_p05\",\"y_pred_p95\"])\n",
    "    df_oos.index = pd.to_datetime(pd.Index([]))\n",
    "\n",
    "print(f\"\\nâœ… Pseudo-OOS (LightGBM, {NAME_TAG}) â€” n prÃ©visions = {len(df_oos)}\")\n",
    "\n",
    "# ---------- Scores ----------\n",
    "def _scores(df):\n",
    "    if len(df) == 0:\n",
    "        return {\"MAE\": np.nan, \"RMSE\": np.nan, \"R2\": np.nan}\n",
    "    mae  = mean_absolute_error(df[\"y_true\"], df[\"y_pred\"])\n",
    "    rmse = np.sqrt(mean_squared_error(df[\"y_true\"], df[\"y_pred\"]))\n",
    "    r2   = r2_score(df[\"y_true\"], df[\"y_pred\"]) if len(df) > 1 else np.nan\n",
    "    return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2)}\n",
    "\n",
    "df_val  = df_oos.loc[eval_start:eval_end].copy()\n",
    "df_test = df_oos.loc[test_start:test_end].copy()\n",
    "sc_val  = _scores(df_val)\n",
    "sc_test = _scores(df_test)\n",
    "\n",
    "print(f\"\\nðŸ“Š Validation 83â€“89 â€” n={len(df_val)} | MAE={sc_val['MAE']:.3f} | RMSE={sc_val['RMSE']:.3f} | RÂ²={sc_val['R2']:.3f}\")\n",
    "print(f\"ðŸ“Š Test 90â€“{test_end.year} â€” n={len(df_test)} | MAE={sc_test['MAE']:.3f} | RMSE={sc_test['RMSE']:.3f} | RÂ²={sc_test['R2']:.3f}\")\n",
    "\n",
    "# ---------- Sauvegardes ----------\n",
    "bundle = {\n",
    "    \"oos_predictions\": df_oos.reset_index(),\n",
    "    \"params\": {\n",
    "        \"model\": f\"LightGBM + Bagging ({NAME_TAG})\",\n",
    "        \"horizon\": h,\n",
    "        \"min_train_n\": min_train_n,\n",
    "        \"winsor_level\": winsor_level,\n",
    "        \"norm_var\": norm_var,\n",
    "        \"features\": features,\n",
    "        \"eval_window\": (str(eval_start.date()), str(eval_end.date())),\n",
    "        \"test_window\": (str(test_start.date()), str(test_end.date())),\n",
    "        # plan refit/retune\n",
    "        \"refit_every_months\": int(refit_every_months),\n",
    "        \"retune_every_months\": int(retune_every_months),\n",
    "        # search\n",
    "        \"hyper_search\": \"RandomizedSearchCV (100 iters) + hv-block CV (5 folds, gap=12), scoring=MAE\",\n",
    "        \"best_params_last\": best_params.copy(),\n",
    "        # bagging (mÃªmes noms que Ridge)\n",
    "        \"use_bagging\": bool(use_bagging),\n",
    "        \"B_boot\": int(B_boot),\n",
    "        \"L_block\": int(L_block),\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"trained_until\": str(last_fit_end.date()) if last_fit_end is not None else None,\n",
    "        \"index_freq\": \"MS\",\n",
    "        \"n_obs_all\": int(len(df_all)),\n",
    "        \"n_forecasts\": int(len(df_oos)),\n",
    "        \"cv_mae_history\": cv_mae_history,\n",
    "        \"best_params_history\": best_params_hist\n",
    "    },\n",
    "    \"train_fit_dates\": pd.to_datetime(pd.Index(train_ends)),\n",
    "\n",
    "    # âœ… Pour permutation_importance_pseudo_oos & SHAP\n",
    "    \"models\":   models,\n",
    "    \"preprocs\": preprocs,\n",
    "}\n",
    "\n",
    "with open(LGBM_LAGS_BUNDLE, \"wb\") as f:\n",
    "    pickle.dump(bundle, f)\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"model\": f\"LightGBM+Bagging ({NAME_TAG})\",\n",
    "    \"horizon\": h,\n",
    "    \"min_train_n\": min_train_n,\n",
    "    \"winsor_level\": winsor_level,\n",
    "    \"norm_var\": norm_var,\n",
    "    \"refit_every_months\": int(refit_every_months),\n",
    "    \"retune_every_months\": int(retune_every_months),\n",
    "    \"use_bagging\": bool(use_bagging),\n",
    "    \"B_boot\": int(B_boot),\n",
    "    \"L_block\": int(L_block),\n",
    "    \"trained_until\": bundle[\"meta\"][\"trained_until\"],\n",
    "    \"n_forecasts\": bundle[\"meta\"][\"n_forecasts\"]\n",
    "}]).to_csv(LGBM_LAGS_META, index=False)\n",
    "\n",
    "# Artefact dernier ensemble (optionnel : pour audit)\n",
    "lgbm_artifact = {\n",
    "    \"trained_until\": bundle[\"meta\"][\"trained_until\"],\n",
    "    \"horizon\": h,\n",
    "    \"features\": features,\n",
    "    \"n_models_base\": len(models),\n",
    "    \"best_params_last\": best_params.copy(),\n",
    "}\n",
    "\n",
    "with open(LGBM_LAGS_LAST_PKL, \"wb\") as f:\n",
    "    pickle.dump(lgbm_artifact, f)\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"trained_until\": lgbm_artifact[\"trained_until\"],\n",
    "    \"n_features\": len(features),\n",
    "    \"horizon\": h,\n",
    "    \"n_models_base\": lgbm_artifact[\"n_models_base\"],\n",
    "    \"model\": f\"LightGBM+Bagging ({NAME_TAG})\"\n",
    "}]).to_csv(LGBM_LAGS_LAST_META, index=False)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Bundle OOS sauvegardÃ© â†’ {LGBM_LAGS_BUNDLE}\")\n",
    "print(f\"ðŸ’¾ MÃ©ta bundle       â†’ {LGBM_LAGS_META}\")\n",
    "print(f\"ðŸ’¾ Dernier modÃ¨le    â†’ {LGBM_LAGS_LAST_PKL}\")\n",
    "print(f\"ðŸ’¾ MÃ©ta dernier fit  â†’ {LGBM_LAGS_LAST_META}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5732408f",
   "metadata": {},
   "source": [
    "# RIDGE avec lags = 12 du taux de chÃ´mage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ea5035f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DonnÃ©es prÃªtes : 1960-01-01 â†’ 2025-08-01 | n=788 | freq=MS\n",
      "Ajout feature: UNRATE_lag12 (y_(t-h)) â†’ OK\n",
      "Features (11): ['TB3MS', 'RPI', 'INDPRO', 'DPCERA3M086SBEA', 'S&P500', 'BUSLOANS'] ...\n",
      "\n",
      "âœ… Pseudo-OOS (Ridge, with_UNRATE_lags_h12) â€” n prÃ©visions = 741\n",
      "              y_pred  y_true  y_pred_base  y_pred_p05  y_pred_p95\n",
      "date                                                             \n",
      "1963-12-01  1.040464     0.0     0.281075   -0.155252    1.481465\n",
      "1964-01-01  0.728928    -0.1    -0.028176    0.076145    1.154382\n",
      "1964-02-01  0.264863    -0.5     0.307588   -0.430290    0.729122\n",
      "\n",
      "ðŸ“Š Validation 83â€“89 â€” n=84 | MAE=0.808 | RMSE=1.023 | RÂ²=-0.340\n",
      "ðŸ“Š Test 90â€“2025 â€” n=428 | MAE=0.794 | RMSE=1.486 | RÂ²=0.051\n",
      "âž¡ï¸  Gain bagging (Î”MAE) = 0.005\n",
      "\n",
      "ðŸ’¾ Bundle sauvegardÃ© â†’ ridge_regression__with_UNRATE_lags_h12.pkl\n",
      "ðŸ’¾ MÃ©ta sauvegardÃ©e â†’ ridge_regression_meta__with_UNRATE_lags_h12.csv\n",
      "ðŸ“¦ Contenu du bundle : ['oos_predictions', 'params', 'meta', 'train_fit_dates', 'models', 'preprocs']\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# ðŸ”¹ Ridge Regression + Bagging (pseudo-OOS, h=12)\n",
    "#    â†’ ajoute UNRATE_lag12 comme variable explicative\n",
    "#    â†’ gestion robuste des NaN (CV/fit/pred)\n",
    "#    â†’ artefacts taggÃ©s: with_UNRATE_lags_h12\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# ---------- ParamÃ¨tres gÃ©nÃ©raux ----------\n",
    "h = 12\n",
    "min_train_n = 36           # â‰¥ 3 ans avant de commencer Ã  prÃ©voir\n",
    "winsor_level = 0.01        # winsorisation (1er/99e percentiles)\n",
    "norm_var = True            # normaliser ou non\n",
    "target_col = \"UNRATE\"      # cible dans df_stationary\n",
    "\n",
    "# SÃ©lection d'alpha : \"cv\" (CV 5-fold) OU une valeur float (ex: 1.0)\n",
    "alpha_mode = \"cv\"\n",
    "alpha_grid = np.logspace(-4, 4, 30)   # utilisÃ© si alpha_mode=\"cv\"\n",
    "\n",
    "# FenÃªtres d'Ã©valuation / test\n",
    "eval_start = pd.Timestamp(\"1983-01-01\")\n",
    "eval_end   = pd.Timestamp(\"1989-12-31\")\n",
    "test_start = pd.Timestamp(\"1990-01-01\")\n",
    "test_end   = pd.Timestamp(\"2025-12-31\")\n",
    "\n",
    "# ---------- Bagging (bootstrap en blocs) ----------\n",
    "use_bagging = True\n",
    "B_boot = 30               # comme les auteurs\n",
    "L_block = 12              # blocs annuels (12 mois)\n",
    "rng = np.random.default_rng(123)  # seed bootstrap\n",
    "\n",
    "# ---------- Fichiers de sortie (VARIANTE AVEC RETARDS) ----------\n",
    "NAME_TAG = f\"with_{target_col}_lags_h{h}\"   # ex: with_UNRATE_lags_h12\n",
    "RIDGE_LAGS_PKL  = f\"ridge_regression__{NAME_TAG}.pkl\"        # bundle (dict)\n",
    "RIDGE_LAGS_META = f\"ridge_regression_meta__{NAME_TAG}.csv\"   # mÃ©ta rÃ©sumÃ©\n",
    "\n",
    "# ---------- PrÃ©paration df_stationary ----------\n",
    "def _ensure_ms_index(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Force un index DatetimeIndex en dÃ©but de mois (MS).\"\"\"\n",
    "    if \"date\" in df.columns:\n",
    "        df = df.set_index(\"date\")\n",
    "    idx = pd.to_datetime(df.index)\n",
    "    df = df.copy()\n",
    "    df.index = idx.to_period(\"M\").to_timestamp(how=\"start\")\n",
    "    return df.asfreq(\"MS\")\n",
    "\n",
    "# âš ï¸ On suppose df_stationary dÃ©jÃ  chargÃ© en mÃ©moire\n",
    "df_all = _ensure_ms_index(df_stationary).sort_index()\n",
    "\n",
    "if target_col not in df_all.columns:\n",
    "    raise ValueError(f\"La colonne cible '{target_col}' est absente de df_stationary.\")\n",
    "\n",
    "y_all = df_all[target_col].astype(float)\n",
    "X_all = df_all.drop(columns=[target_col]).astype(float)\n",
    "\n",
    "# --- Ajout du lag h (=12) de la cible comme variable explicative ---\n",
    "lag_feat_name = f\"{target_col}_lag{h}\"  # 'UNRATE_lag12'\n",
    "X_all[lag_feat_name] = y_all.shift(h)\n",
    "\n",
    "features = list(X_all.columns)\n",
    "\n",
    "print(f\"âœ… DonnÃ©es prÃªtes : {df_all.index.min().date()} â†’ {df_all.index.max().date()} | n={len(df_all)} | freq=MS\")\n",
    "print(f\"Ajout feature: {lag_feat_name} (y_(t-h)) â†’ OK\")\n",
    "print(f\"Features ({len(features)}): {features[:6]}{' ...' if len(features)>6 else ''}\")\n",
    "\n",
    "# ---------- PrÃ©proc ----------\n",
    "def fit_preproc(X: pd.DataFrame, wins=0.01, do_norm=True):\n",
    "    \"\"\"Apprend winsor + normalisation sur TRAIN et renvoie (X_trans, prep).\"\"\"\n",
    "    lower = X.quantile(wins)\n",
    "    upper = X.quantile(1 - wins)\n",
    "    Xw = X.clip(lower=lower, upper=upper, axis=1)\n",
    "    if do_norm:\n",
    "        mean = Xw.mean()\n",
    "        std  = Xw.std().replace(0, 1)\n",
    "        Xn   = (Xw - mean) / std\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": mean, \"std\": std, \"norm\": True}\n",
    "        return Xn, prep\n",
    "    else:\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": None, \"std\": None, \"norm\": False}\n",
    "        return Xw, prep\n",
    "\n",
    "def apply_preproc(X: pd.DataFrame, prep: dict):\n",
    "    \"\"\"Applique le prÃ©proc appris (pas de fuite).\"\"\"\n",
    "    Xp = X.clip(lower=prep[\"lower\"], upper=prep[\"upper\"], axis=1)\n",
    "    if prep[\"norm\"]:\n",
    "        Xp = (Xp - prep[\"mean\"]) / prep[\"std\"].replace(0, 1)\n",
    "    return Xp\n",
    "\n",
    "# ---------- SÃ©lection d'alpha ----------\n",
    "def select_alpha(X_tr_p: pd.DataFrame, y_tr: pd.Series, mode=\"cv\"):\n",
    "    \"\"\"Renvoie (alpha, cv_mae) si mode='cv', sinon (alpha, np.nan).\"\"\"\n",
    "    if mode == \"cv\":\n",
    "        model = Ridge(fit_intercept=True)\n",
    "        grid = GridSearchCV(\n",
    "            model,\n",
    "            {\"alpha\": alpha_grid},\n",
    "            scoring=\"neg_mean_absolute_error\",\n",
    "            cv=5,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        grid.fit(X_tr_p, y_tr.values)\n",
    "        best_alpha = float(grid.best_estimator_.alpha)\n",
    "        cv_mae = float(-grid.best_score_)\n",
    "        return best_alpha, cv_mae\n",
    "    else:\n",
    "        try:\n",
    "            a = float(mode)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"alpha_mode doit Ãªtre 'cv' ou un float. ReÃ§u: {mode}\") from e\n",
    "        return a, np.nan\n",
    "\n",
    "# ---------- Bootstrap utils ----------\n",
    "def block_bootstrap_rows(index, L, rng):\n",
    "    \"\"\"Moving-block bootstrap sur index (positions).\"\"\"\n",
    "    n = len(index)\n",
    "    if n < 3:\n",
    "        return np.arange(n)  # fallback\n",
    "    L = max(2, min(int(L), n-1))\n",
    "    nb = int(np.ceil(n / L))\n",
    "    starts = rng.integers(0, n - L + 1, size=nb)\n",
    "    ix = np.concatenate([np.arange(s, s+L) for s in starts])[:n]\n",
    "    return ix\n",
    "\n",
    "def bagged_predict_ridge(X_tr_raw: pd.DataFrame, y_tr: pd.Series, x_fore_raw: pd.DataFrame,\n",
    "                         prep: dict, alpha: float, B: int, L: int, rng):\n",
    "    \"\"\"\n",
    "    Bagging (moving-block bootstrap) pour Ridge :\n",
    "      - prÃ©proc fixÃ© sur TRAIN original (pas rÃ©-appris)\n",
    "      - on droppe les lignes NaN aprÃ¨s prÃ©proc (Ridge n'accepte pas NaN)\n",
    "      - bootstrap sur l'index des lignes propres\n",
    "      - prÃ©diction h ; renvoie (moyenne, distribution, base_pred)\n",
    "    \"\"\"\n",
    "    # PrÃ©proc du TRAIN + nettoyage\n",
    "    X_tr_p = apply_preproc(X_tr_raw, prep)\n",
    "    mask_clean = X_tr_p.notna().all(axis=1) & y_tr.notna()\n",
    "    Xc = X_tr_p.loc[mask_clean]\n",
    "    yc = y_tr.loc[mask_clean]\n",
    "\n",
    "    if len(Xc) < 5:\n",
    "        raise ValueError(\"Trop peu d'observations propres pour bagging Ridge.\")\n",
    "\n",
    "    # PrÃ©proc de x_fore + imputation 0 (aprÃ¨s normalisation 0 = moyenne)\n",
    "    x_fore_p = apply_preproc(x_fore_raw, prep).fillna(0.0)\n",
    "\n",
    "    # Base fit (rÃ©fÃ©rence) sur TRAIN propre\n",
    "    base = Ridge(alpha=alpha, fit_intercept=True)\n",
    "    base.fit(Xc.values, yc.values)\n",
    "    yhat_base = float(base.predict(x_fore_p.values)[0])\n",
    "\n",
    "    # Bootstrap sur l'index propre\n",
    "    def _block_bootstrap_rows_from_clean(n_clean, L, rng):\n",
    "        if n_clean < 3:\n",
    "            return np.arange(n_clean)\n",
    "        L_eff = max(2, min(int(L), n_clean - 1))\n",
    "        nb = int(np.ceil(n_clean / L_eff))\n",
    "        starts = rng.integers(0, n_clean - L_eff + 1, size=nb)\n",
    "        ix_pos = np.concatenate([np.arange(s, s + L_eff) for s in starts])[:n_clean]\n",
    "        return ix_pos\n",
    "\n",
    "    preds = []\n",
    "    n_clean = len(Xc)\n",
    "    for b in range(B):\n",
    "        ix_pos = _block_bootstrap_rows_from_clean(n_clean, L, rng)\n",
    "        Xb = Xc.iloc[ix_pos]\n",
    "        yb = yc.iloc[ix_pos]\n",
    "        m = Ridge(alpha=alpha, fit_intercept=True)\n",
    "        m.fit(Xb.values, yb.values)\n",
    "        preds.append(float(m.predict(x_fore_p.values)[0]))\n",
    "\n",
    "    return float(np.mean(preds)), np.array(preds), yhat_base\n",
    "\n",
    "# ---------- Boucle pseudo-OOS ----------\n",
    "rows = []                 # (date, y_pred, y_true, y_pred_base, p05, p95)\n",
    "models = []               # stockage des modÃ¨les (un par fenÃªtre)\n",
    "preprocs = []             # stockage prep (alignÃ©s aux modÃ¨les)\n",
    "train_ends = []           # dates de fin train (pour trace)\n",
    "alpha_history = []        # alpha utilisÃ© par fenÃªtre\n",
    "cv_mae_history = []       # MAE CV (si mode=\"cv\"), sinon NaN\n",
    "\n",
    "last_t_end = y_all.index.max() - relativedelta(months=h)\n",
    "last_model = None\n",
    "last_fit_end = None\n",
    "\n",
    "for t_end in y_all.index:\n",
    "    if t_end > last_t_end:\n",
    "        break\n",
    "\n",
    "    y_tr = y_all.loc[:t_end]\n",
    "    X_tr = X_all.loc[:t_end]\n",
    "    if len(y_tr) < min_train_n:\n",
    "        continue\n",
    "\n",
    "    # PrÃ©proc appris sur TRAIN courant\n",
    "    X_tr_p, prep = fit_preproc(X_tr, wins=winsor_level, do_norm=norm_var)\n",
    "\n",
    "    # ðŸ”§ Nettoyage TRAIN (Ridge / GridSearchCV n'acceptent pas les NaN)\n",
    "    mask_clean = X_tr_p.notna().all(axis=1) & y_tr.notna()\n",
    "    X_tr_p_clean = X_tr_p.loc[mask_clean]\n",
    "    y_tr_clean   = y_tr.loc[mask_clean]\n",
    "\n",
    "    # SÃ©curitÃ© : si trop peu d'observations propres, on saute cette itÃ©ration\n",
    "    if len(X_tr_p_clean) < 10:\n",
    "        continue\n",
    "\n",
    "    # Choix alpha (CV ou fixe) sur TRAIN propre\n",
    "    if alpha_mode == \"cv\" and len(X_tr_p_clean) < 25:  # ~5 obs/fold min\n",
    "        alpha, cv_mae = 1.0, np.nan\n",
    "    else:\n",
    "        alpha, cv_mae = select_alpha(X_tr_p_clean, y_tr_clean, mode=alpha_mode)\n",
    "    alpha_history.append(alpha)\n",
    "    cv_mae_history.append(cv_mae)\n",
    "\n",
    "    # Horizon ciblÃ©\n",
    "    t_fore = t_end + relativedelta(months=h)\n",
    "    if t_fore in y_all.index:\n",
    "        x_fore_raw = X_all.loc[[t_fore]]\n",
    "        # ðŸ”§ Pas de NaN Ã  la prÃ©diction (aprÃ¨s normalisation, 0 = moyenne)\n",
    "        x_fore_p = apply_preproc(x_fore_raw, prep).fillna(0.0)\n",
    "\n",
    "        if use_bagging:\n",
    "            yhat_h, dist, yhat_base = bagged_predict_ridge(\n",
    "                X_tr_raw=X_tr, y_tr=y_tr, x_fore_raw=x_fore_raw,\n",
    "                prep=prep, alpha=alpha, B=B_boot, L=L_block, rng=rng\n",
    "            )\n",
    "            y_p05 = float(np.percentile(dist, 5))\n",
    "            y_p95 = float(np.percentile(dist, 95))\n",
    "        else:\n",
    "            model_tmp = Ridge(alpha=alpha, fit_intercept=True)\n",
    "            model_tmp.fit(X_tr_p_clean.values, y_tr_clean.values)\n",
    "            yhat_h = float(model_tmp.predict(x_fore_p.values)[0])\n",
    "            yhat_base = yhat_h\n",
    "            y_p05, y_p95 = (np.nan, np.nan)\n",
    "\n",
    "        rows.append((t_fore, yhat_h, float(y_all.loc[t_fore]), yhat_base, y_p05, y_p95))\n",
    "\n",
    "    # trace / dernier modÃ¨le base (utile pour sauvegarde) â€” entraÃ®nÃ© sur TRAIN propre\n",
    "    last_model = Ridge(alpha=alpha, fit_intercept=True).fit(X_tr_p_clean.values, y_tr_clean.values)\n",
    "    last_fit_end = t_end\n",
    "    models.append(last_model)\n",
    "    preprocs.append(prep)\n",
    "    train_ends.append(t_end)\n",
    "\n",
    "# ---------- DataFrame OOS ----------\n",
    "if rows:\n",
    "    df_oos = (\n",
    "        pd.DataFrame(rows, columns=[\"date\", \"y_pred\", \"y_true\", \"y_pred_base\", \"y_pred_p05\", \"y_pred_p95\"])\n",
    "          .assign(date=lambda d: pd.to_datetime(d[\"date\"]).dt.to_period(\"M\").dt.to_timestamp(how=\"start\"))\n",
    "          .set_index(\"date\").sort_index()\n",
    "    )\n",
    "else:\n",
    "    df_oos = pd.DataFrame(columns=[\"y_pred\", \"y_true\", \"y_pred_base\", \"y_pred_p05\", \"y_pred_p95\"])\n",
    "    df_oos.index = pd.to_datetime(pd.Index([]))\n",
    "\n",
    "print(f\"\\nâœ… Pseudo-OOS (Ridge, {NAME_TAG}) â€” n prÃ©visions = {len(df_oos)}\")\n",
    "print(df_oos.head(3))\n",
    "\n",
    "# ---------- Scores ----------\n",
    "def _scores(df: pd.DataFrame):\n",
    "    if len(df) == 0:\n",
    "        return {\"MAE\": np.nan, \"RMSE\": np.nan, \"R2\": np.nan}\n",
    "    mae  = mean_absolute_error(df[\"y_true\"], df[\"y_pred\"])\n",
    "    rmse = np.sqrt(mean_squared_error(df[\"y_true\"], df[\"y_pred\"]))\n",
    "    r2   = r2_score(df[\"y_true\"], df[\"y_pred\"]) if len(df) > 1 else np.nan\n",
    "    return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2)}\n",
    "\n",
    "df_val  = df_oos.loc[eval_start:eval_end].copy()\n",
    "df_test = df_oos.loc[test_start:test_end].copy()\n",
    "\n",
    "sc_val  = _scores(df_val)\n",
    "sc_test = _scores(df_test)\n",
    "\n",
    "print(f\"\\nðŸ“Š Validation 83â€“89 â€” n={len(df_val)} | MAE={sc_val['MAE']:.3f} | RMSE={sc_val['RMSE']:.3f} | RÂ²={sc_val['R2']:.3f}\")\n",
    "print(f\"ðŸ“Š Test 90â€“{test_end.year} â€” n={len(df_test)} | MAE={sc_test['MAE']:.3f} | RMSE={sc_test['RMSE']:.3f} | RÂ²={sc_test['R2']:.3f}\")\n",
    "\n",
    "# (option) Comparaison bagging vs base\n",
    "if \"y_pred_base\" in df_oos and df_oos[\"y_pred_base\"].notna().any():\n",
    "    mae_bag  = mean_absolute_error(df_oos[\"y_true\"], df_oos[\"y_pred\"])\n",
    "    mae_base = mean_absolute_error(df_oos[\"y_true\"], df_oos[\"y_pred_base\"])\n",
    "    print(f\"âž¡ï¸  Gain bagging (Î”MAE) = {mae_base - mae_bag:.3f}\")\n",
    "\n",
    "# ---------- Sauvegardes ----------\n",
    "bundle = {\n",
    "    \"oos_predictions\": df_oos.reset_index(),     # (date, y_pred, y_true, y_pred_base, y_pred_p05, y_pred_p95)\n",
    "    \"params\": {\n",
    "        \"model\": f\"Ridge ({NAME_TAG})\",\n",
    "        \"horizon\": h,\n",
    "        \"min_train_n\": min_train_n,\n",
    "        \"winsor_level\": winsor_level,\n",
    "        \"norm_var\": norm_var,\n",
    "        \"features\": features,\n",
    "        \"eval_window\": (str(eval_start.date()), str(eval_end.date())),\n",
    "        \"test_window\": (str(test_start.date()), str(test_end.date())),\n",
    "        # ---- bagging ----\n",
    "        \"use_bagging\": bool(use_bagging),\n",
    "        \"B_boot\": int(B_boot),\n",
    "        \"L_block\": int(L_block),\n",
    "        # ---- alpha ----\n",
    "        \"alpha_mode\": alpha_mode,\n",
    "        \"alpha_grid\": list(alpha_grid) if alpha_mode == \"cv\" else None,\n",
    "        \"best_alpha_last\": (alpha_history[-1] if len(alpha_history) else None),\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"trained_until\": str(last_fit_end.date()) if last_fit_end is not None else None,\n",
    "        \"index_freq\": \"MS\",\n",
    "        \"n_obs_all\": int(len(df_all)),\n",
    "        \"n_forecasts\": int(len(df_oos)),\n",
    "        \"alpha_history\": alpha_history,\n",
    "        \"cv_mae_history\": cv_mae_history,\n",
    "    },\n",
    "    \"train_fit_dates\": pd.to_datetime(pd.Index(train_ends)),\n",
    "\n",
    "    # âœ… Pour permutation_importance_pseudo_oos & SHAP\n",
    "    \"models\":   models,     # liste des modÃ¨les Ridge (un par fenÃªtre)\n",
    "    \"preprocs\": preprocs,   # liste des prÃ©proc (dict) alignÃ©s aux modÃ¨les\n",
    "}\n",
    "\n",
    "# --- Sauvegarde du bundle complet ---\n",
    "with open(RIDGE_LAGS_PKL, \"wb\") as f:\n",
    "    pickle.dump(bundle, f)\n",
    "\n",
    "# --- Sauvegarde du rÃ©sumÃ© mÃ©ta sÃ©parÃ© (lisible rapidement) ---\n",
    "pd.DataFrame([{\n",
    "    \"model\": f\"Ridge ({NAME_TAG})\",\n",
    "    \"horizon\": h,\n",
    "    \"min_train_n\": min_train_n,\n",
    "    \"winsor_level\": winsor_level,\n",
    "    \"norm_var\": norm_var,\n",
    "    \"use_bagging\": bool(use_bagging),\n",
    "    \"B_boot\": int(B_boot),\n",
    "    \"L_block\": int(L_block),\n",
    "    \"alpha_mode\": alpha_mode,\n",
    "    \"best_alpha_last\": bundle[\"params\"][\"best_alpha_last\"],\n",
    "    \"trained_until\": bundle[\"meta\"][\"trained_until\"],\n",
    "    \"n_forecasts\": bundle[\"meta\"][\"n_forecasts\"],\n",
    "}]).to_csv(RIDGE_LAGS_META, index=False)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Bundle sauvegardÃ© â†’ {RIDGE_LAGS_PKL}\")\n",
    "print(f\"ðŸ’¾ MÃ©ta sauvegardÃ©e â†’ {RIDGE_LAGS_META}\")\n",
    "print(f\"ðŸ“¦ Contenu du bundle : {list(bundle.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecf20d0",
   "metadata": {},
   "source": [
    "# LightGBMnoUSREC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e66155b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Colonnes USREC exclues: ['USREC']\n",
      "Ajout feature: UNRATE_lag12 (y_(t-h)) â†’ OK\n",
      "âœ… DonnÃ©es prÃªtes : 1960-01-01 â†’ 2025-08-01 | n=788 | freq=MS\n",
      "Features (10): ['TB3MS', 'RPI', 'INDPRO', 'DPCERA3M086SBEA', 'S&P500', 'BUSLOANS'] ...\n",
      "\n",
      "âœ… Pseudo-OOS (LightGBM, with_UNRATE_lags_h12__noUSREC) â€” n prÃ©visions = 741\n",
      "\n",
      "ðŸ“Š Validation 83â€“89 â€” n=84 | MAE=0.827 | RMSE=1.076 | RÂ²=-0.482\n",
      "ðŸ“Š Test 90â€“2025 â€” n=428 | MAE=0.767 | RMSE=1.401 | RÂ²=0.157\n",
      "\n",
      "ðŸ’¾ Bundle OOS sauvegardÃ© â†’ lightgbm_regression__with_UNRATE_lags_h12__noUSREC.pkl\n",
      "ðŸ’¾ MÃ©ta bundle       â†’ lightgbm_regression_meta__with_UNRATE_lags_h12__noUSREC.csv\n",
      "ðŸ’¾ Dernier modÃ¨le    â†’ LGBM_last_trained_model__with_UNRATE_lags_h12__noUSREC.pkl\n",
      "ðŸ’¾ MÃ©ta dernier fit  â†’ LGBM_last_trained_model_meta__with_UNRATE_lags_h12__noUSREC.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# ðŸ”¹ LightGBM + Bagging (pseudo-OOS, h=12)\n",
    "#    â†’ VARIANTE SANS USREC (mais AVEC y_{t-h})\n",
    "#    â†’ artefacts taggÃ©s: with_<target>_lags_h<h>__noUSREC\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# ---------- ParamÃ¨tres gÃ©nÃ©raux ----------\n",
    "h = 12\n",
    "min_train_n = 36\n",
    "winsor_level = 0.01\n",
    "norm_var = True\n",
    "target_col = \"UNRATE\"\n",
    "\n",
    "# FenÃªtres dâ€™Ã©val / test\n",
    "eval_start = pd.Timestamp(\"1983-01-01\")\n",
    "eval_end   = pd.Timestamp(\"1989-12-31\")\n",
    "test_start = pd.Timestamp(\"1990-01-01\")\n",
    "test_end   = pd.Timestamp(\"2025-12-31\")\n",
    "\n",
    "# ---------- Refit/Retune ----------\n",
    "refit_every_months  = 12\n",
    "retune_every_months = 36\n",
    "\n",
    "# ---------- Bagging (moving-block bootstrap) ----------\n",
    "use_bagging = True\n",
    "B_boot = 30\n",
    "L_block = 12\n",
    "rng = np.random.default_rng(12345)\n",
    "\n",
    "# ---------- Fichiers de sortie (VARIANTE SANS USREC) ----------\n",
    "NAME_TAG = f\"with_{target_col}_lags_h{h}__noUSREC\"   # â† clair et explicite\n",
    "LGBM_NOUSREC_LAGS_BUNDLE    = f\"lightgbm_regression__{NAME_TAG}.pkl\"\n",
    "LGBM_NOUSREC_LAGS_META      = f\"lightgbm_regression_meta__{NAME_TAG}.csv\"\n",
    "LGBM_NOUSREC_LAGS_LAST_PKL  = f\"LGBM_last_trained_model__{NAME_TAG}.pkl\"\n",
    "LGBM_NOUSREC_LAGS_LAST_META = f\"LGBM_last_trained_model_meta__{NAME_TAG}.csv\"\n",
    "\n",
    "# ---------- PrÃ©paration df_stationary ----------\n",
    "def _ensure_ms_index(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"date\" in df.columns:\n",
    "        df = df.set_index(\"date\")\n",
    "    idx = pd.to_datetime(df.index)\n",
    "    df.index = idx.to_period(\"M\").to_timestamp(how=\"start\")\n",
    "    return df.asfreq(\"MS\")\n",
    "\n",
    "# âš ï¸ On suppose df_stationary dispo\n",
    "df_all = _ensure_ms_index(df_stationary).sort_index()\n",
    "\n",
    "if target_col not in df_all.columns:\n",
    "    raise ValueError(f\"La cible '{target_col}' est absente de df_stationary.\")\n",
    "\n",
    "y_all = df_all[target_col].astype(float)\n",
    "X_all = df_all.drop(columns=[target_col]).astype(float)\n",
    "\n",
    "# --- âŒ Supprimer toute colonne USREC (USREC, USREC_lag*, etc.) ---\n",
    "usrec_cols = [c for c in X_all.columns if \"USREC\" in str(c).upper()]\n",
    "if usrec_cols:\n",
    "    X_all = X_all.drop(columns=usrec_cols)\n",
    "    print(f\"ðŸ§¹ Colonnes USREC exclues: {usrec_cols}\")\n",
    "\n",
    "# --- âœ… Ajout du lag h (=12) de la cible comme variable explicative ---\n",
    "lag_feat_name = f\"{target_col}_lag{h}\"   # ex: 'UNRATE_lag12'\n",
    "X_all[lag_feat_name] = y_all.shift(h)\n",
    "\n",
    "# LightGBM : Ã©viter espaces dans noms de colonnes\n",
    "X_all.columns = [str(c).replace(\" \", \"_\") for c in X_all.columns]\n",
    "features = list(X_all.columns)\n",
    "\n",
    "print(f\"Ajout feature: {lag_feat_name} (y_(t-h)) â†’ OK\")\n",
    "print(f\"âœ… DonnÃ©es prÃªtes : {df_all.index.min().date()} â†’ {df_all.index.max().date()} | n={len(df_all)} | freq=MS\")\n",
    "print(f\"Features ({len(features)}): {features[:6]}{' ...' if len(features)>6 else ''}\")\n",
    "\n",
    "# ---------- PrÃ©proc ----------\n",
    "def fit_preproc(X: pd.DataFrame, wins=0.01, do_norm=True):\n",
    "    lower = X.quantile(wins)\n",
    "    upper = X.quantile(1 - wins)\n",
    "    Xw = X.clip(lower=lower, upper=upper, axis=1)\n",
    "    if do_norm:\n",
    "        mean = Xw.mean()\n",
    "        std  = Xw.std().replace(0, 1)\n",
    "        Xn   = (Xw - mean) / std\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": mean, \"std\": std, \"norm\": True}\n",
    "        return Xn, prep\n",
    "    else:\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": None, \"std\": None, \"norm\": False}\n",
    "        return Xw, prep\n",
    "\n",
    "def apply_preproc(X: pd.DataFrame, prep: dict):\n",
    "    Xp = X.clip(lower=prep[\"lower\"], upper=prep[\"upper\"], axis=1)\n",
    "    if prep[\"norm\"]:\n",
    "        Xp = (Xp - prep[\"mean\"]) / prep[\"std\"].replace(0, 1)\n",
    "    return Xp\n",
    "\n",
    "# ---------- hv-block CV ----------\n",
    "class HVBlockCV:\n",
    "    def __init__(self, n_splits=5, gap=12):\n",
    "        self.n_splits = n_splits\n",
    "        self.gap = 12 if gap is None else int(gap)\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n = len(X)\n",
    "        fold_sizes = np.full(self.n_splits, n // self.n_splits, dtype=int)\n",
    "        fold_sizes[: n % self.n_splits] += 1\n",
    "        idx = np.arange(n)\n",
    "        cur = 0\n",
    "        for fs in fold_sizes:\n",
    "            start, stop = cur, cur + fs\n",
    "            test_idx = idx[start:stop]\n",
    "            train_mask = np.ones(n, dtype=bool)\n",
    "            left = max(0, start - self.gap)\n",
    "            right = min(n, stop + self.gap)\n",
    "            train_mask[left:right] = False\n",
    "            train_idx = idx[train_mask]\n",
    "            cur = stop\n",
    "            if len(train_idx) == 0 or len(test_idx) == 0:\n",
    "                continue\n",
    "            yield train_idx, test_idx\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "# ---------- Grille LightGBM ----------\n",
    "param_dist = {\n",
    "    \"subsample\":        [0.05,.1,.2,.3,.4,.5,.6,.7,.8,.9,1.0],\n",
    "    \"colsample_bytree\": [.2,.3,.4,.5,.6,.7,1.0],\n",
    "    \"num_leaves\":       [2,3,4,5,8,10,20,40,70,100],\n",
    "    \"n_estimators\":     [5,10,20,30,40,50,75,100],\n",
    "    \"max_depth\":        [1,2,3,5,8,15,-1],\n",
    "    \"reg_alpha\":        [0, .1, 1, 2, 7, 10, 50, 100],\n",
    "    \"reg_lambda\":       [0, .1, 1, 10, 20, 50, 100],\n",
    "    \"min_child_samples\":[5,10,15],\n",
    "    \"min_split_gain\":   [0.0, 0.01, 0.05],\n",
    "}\n",
    "\n",
    "def tune_lgbm(X_tr_p: pd.DataFrame, y_tr: np.ndarray, seed=12345):\n",
    "    cv = HVBlockCV(n_splits=5, gap=12)\n",
    "    base = LGBMRegressor(\n",
    "        boosting_type=\"gbdt\",\n",
    "        objective=\"regression\",\n",
    "        importance_type=\"gain\",\n",
    "        random_state=seed,\n",
    "        n_jobs=1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    rs = RandomizedSearchCV(\n",
    "        estimator=base,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=100,\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=seed,\n",
    "        refit=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    rs.fit(X_tr_p, y_tr)\n",
    "    return rs.best_params_, float(-rs.best_score_), rs.best_estimator_\n",
    "\n",
    "# ---------- Bootstrap utils ----------\n",
    "def block_bootstrap_rows(index, L, rng):\n",
    "    n = len(index)\n",
    "    if n < 3:\n",
    "        return np.arange(n)\n",
    "    L = max(2, min(int(L), n-1))\n",
    "    nb = int(np.ceil(n / L))\n",
    "    starts = rng.integers(0, n - L + 1, size=nb)\n",
    "    ix = np.concatenate([np.arange(s, s+L) for s in starts])[:n]\n",
    "    return ix\n",
    "\n",
    "def bagged_predict_lgbm(X_tr_raw, y_tr, x_fore_raw, prep, best_params, B, L, rng):\n",
    "    X_tr_p = apply_preproc(X_tr_raw, prep)\n",
    "    base = LGBMRegressor(\n",
    "        boosting_type=\"gbdt\",\n",
    "        objective=\"regression\",\n",
    "        importance_type=\"gain\",\n",
    "        n_jobs=1,\n",
    "        random_state=123,\n",
    "        verbose=-1,\n",
    "        **best_params\n",
    "    )\n",
    "    base.fit(X_tr_p, y_tr.values)\n",
    "    yhat_base = float(base.predict(apply_preproc(x_fore_raw, prep))[0])\n",
    "\n",
    "    preds = []\n",
    "    for b in range(B):\n",
    "        ix = block_bootstrap_rows(X_tr_raw.index, L, rng)\n",
    "        Xb = X_tr_raw.iloc[ix]\n",
    "        yb = y_tr.iloc[ix]\n",
    "        Xb_p = apply_preproc(Xb, prep)\n",
    "        m = LGBMRegressor(\n",
    "            boosting_type=\"gbdt\",\n",
    "            objective=\"regression\",\n",
    "            importance_type=\"gain\",\n",
    "            n_jobs=1,\n",
    "            random_state=123 + b,\n",
    "            verbose=-1,\n",
    "            **best_params\n",
    "        )\n",
    "        m.fit(Xb_p, yb.values)\n",
    "        preds.append(float(m.predict(apply_preproc(x_fore_raw, prep))[0]))\n",
    "    return float(np.mean(preds)), np.array(preds), yhat_base\n",
    "\n",
    "# ---------- Boucle pseudo-OOS ----------\n",
    "def _months_between(a: pd.Timestamp, b: pd.Timestamp) -> int:\n",
    "    return (b.year - a.year) * 12 + (b.month - a.month)\n",
    "\n",
    "rows = []\n",
    "models = []\n",
    "preprocs = []\n",
    "train_ends = []\n",
    "cv_mae_history = []\n",
    "best_params_hist = []\n",
    "\n",
    "last_fit_end = None\n",
    "last_t_end   = y_all.index.max() - relativedelta(months=h)\n",
    "last_refit_t = None\n",
    "last_tune_t  = None\n",
    "\n",
    "best_params = {}\n",
    "base_model = None\n",
    "\n",
    "for t_end in y_all.index:\n",
    "    if t_end > last_t_end:\n",
    "        break\n",
    "\n",
    "    y_tr = y_all.loc[:t_end]\n",
    "    X_tr = X_all.loc[:t_end]\n",
    "    if len(y_tr) < min_train_n:\n",
    "        continue\n",
    "\n",
    "    need_refit = (last_refit_t is None) or (_months_between(last_refit_t, t_end) >= refit_every_months)\n",
    "    need_tune  = (t_end >= eval_start) and (last_tune_t is None or _months_between(last_tune_t, t_end) >= retune_every_months)\n",
    "\n",
    "    X_tr_p_global, prep_global = fit_preproc(X_tr, wins=winsor_level, do_norm=norm_var)\n",
    "\n",
    "    if need_tune:\n",
    "        best_params, best_cv_mae, _ = tune_lgbm(X_tr_p_global, y_tr.values, seed=12345)\n",
    "        last_tune_t = t_end\n",
    "        cv_mae_history.append(best_cv_mae)\n",
    "        best_params_hist.append(best_params.copy())\n",
    "    else:\n",
    "        cv_mae_history.append(np.nan)\n",
    "        best_params_hist.append(best_params.copy() if best_params else {})\n",
    "\n",
    "    if not best_params:\n",
    "        best_params = dict(\n",
    "            subsample=0.7, colsample_bytree=0.7, num_leaves=31,\n",
    "            n_estimators=100, max_depth=-1, reg_alpha=0.0, reg_lambda=0.0,\n",
    "            min_child_samples=10, min_split_gain=0.0\n",
    "        )\n",
    "\n",
    "    if need_refit:\n",
    "        base_model = LGBMRegressor(\n",
    "            boosting_type=\"gbdt\",\n",
    "            objective=\"regression\",\n",
    "            importance_type=\"gain\",\n",
    "            n_jobs=1,\n",
    "            random_state=12345,\n",
    "            verbose=-1,\n",
    "            **best_params\n",
    "        )\n",
    "        base_model.fit(X_tr_p_global, y_tr.values)\n",
    "        models.append(base_model)\n",
    "        preprocs.append(prep_global)\n",
    "        train_ends.append(t_end)\n",
    "        last_refit_t = t_end\n",
    "        last_fit_end = t_end\n",
    "\n",
    "    t_fore = t_end + relativedelta(months=h)\n",
    "    if t_fore in y_all.index:\n",
    "        x_fore_raw = X_all.loc[[t_fore]]\n",
    "        if use_bagging:\n",
    "            yhat, dist, yhat_base = bagged_predict_lgbm(\n",
    "                X_tr_raw=X_tr, y_tr=y_tr, x_fore_raw=x_fore_raw,\n",
    "                prep=prep_global, best_params=best_params,\n",
    "                B=B_boot, L=L_block, rng=rng\n",
    "            )\n",
    "            y_p05 = float(np.percentile(dist, 5))\n",
    "            y_p95 = float(np.percentile(dist, 95))\n",
    "        else:\n",
    "            if base_model is None:\n",
    "                base_model = LGBMRegressor(\n",
    "                    boosting_type=\"gbdt\",\n",
    "                    objective=\"regression\",\n",
    "                    importance_type=\"gain\",\n",
    "                    n_jobs=1,\n",
    "                    random_state=12345,\n",
    "                    verbose=-1,\n",
    "                    **best_params\n",
    "                ).fit(X_tr_p_global, y_tr.values)\n",
    "            yhat = float(base_model.predict(apply_preproc(x_fore_raw, prep_global))[0])\n",
    "            yhat_base = yhat\n",
    "            y_p05, y_p95 = (np.nan, np.nan)\n",
    "\n",
    "        rows.append((t_fore, yhat, float(y_all.loc[t_fore]), yhat_base, y_p05, y_p95))\n",
    "\n",
    "# ---------- DataFrame OOS ----------\n",
    "if rows:\n",
    "    df_oos = (\n",
    "        pd.DataFrame(rows, columns=[\"date\", \"y_pred\", \"y_true\", \"y_pred_base\", \"y_pred_p05\", \"y_pred_p95\"])\n",
    "          .assign(date=lambda d: pd.to_datetime(d[\"date\"]).dt.to_period(\"M\").dt.to_timestamp(how=\"start\"))\n",
    "          .set_index(\"date\").sort_index()\n",
    "    )\n",
    "else:\n",
    "    df_oos = pd.DataFrame(columns=[\"y_pred\",\"y_true\",\"y_pred_base\",\"y_pred_p05\",\"y_pred_p95\"])\n",
    "    df_oos.index = pd.to_datetime(pd.Index([]))\n",
    "\n",
    "print(f\"\\nâœ… Pseudo-OOS (LightGBM, {NAME_TAG}) â€” n prÃ©visions = {len(df_oos)}\")\n",
    "\n",
    "# ---------- Scores ----------\n",
    "def _scores(df):\n",
    "    if len(df) == 0:\n",
    "        return {\"MAE\": np.nan, \"RMSE\": np.nan, \"R2\": np.nan}\n",
    "    mae  = mean_absolute_error(df[\"y_true\"], df[\"y_pred\"])\n",
    "    rmse = np.sqrt(mean_squared_error(df[\"y_true\"], df[\"y_pred\"]))\n",
    "    r2   = r2_score(df[\"y_true\"], df[\"y_pred\"]) if len(df) > 1 else np.nan\n",
    "    return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2)}\n",
    "\n",
    "df_val  = df_oos.loc[eval_start:eval_end].copy()\n",
    "df_test = df_oos.loc[test_start:test_end].copy()\n",
    "sc_val  = _scores(df_val)\n",
    "sc_test = _scores(df_test)\n",
    "\n",
    "print(f\"\\nðŸ“Š Validation 83â€“89 â€” n={len(df_val)} | MAE={sc_val['MAE']:.3f} | RMSE={sc_val['RMSE']:.3f} | RÂ²={sc_val['R2']:.3f}\")\n",
    "print(f\"ðŸ“Š Test 90â€“{test_end.year} â€” n={len(df_test)} | MAE={sc_test['MAE']:.3f} | RMSE={sc_test['RMSE']:.3f} | RÂ²={sc_test['R2']:.3f}\")\n",
    "\n",
    "# ---------- Sauvegardes ----------\n",
    "bundle = {\n",
    "    \"oos_predictions\": df_oos.reset_index(),\n",
    "    \"params\": {\n",
    "        \"model\": f\"LightGBM + Bagging ({NAME_TAG})\",\n",
    "        \"horizon\": h,\n",
    "        \"min_train_n\": min_train_n,\n",
    "        \"winsor_level\": winsor_level,\n",
    "        \"norm_var\": norm_var,\n",
    "        \"features\": features,   # â† SANS aucune USREC\n",
    "        \"eval_window\": (str(eval_start.date()), str(eval_end.date())),\n",
    "        \"test_window\": (str(test_start.date()), str(test_end.date())),\n",
    "        \"refit_every_months\": int(refit_every_months),\n",
    "        \"retune_every_months\": int(retune_every_months),\n",
    "        \"hyper_search\": \"RandomizedSearchCV (100 iters) + hv-block CV (5 folds, gap=12), scoring=MAE\",\n",
    "        \"best_params_last\": best_params.copy(),\n",
    "        \"use_bagging\": bool(use_bagging),\n",
    "        \"B_boot\": int(B_boot),\n",
    "        \"L_block\": int(L_block),\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"trained_until\": str(last_fit_end.date()) if last_fit_end is not None else None,\n",
    "        \"index_freq\": \"MS\",\n",
    "        \"n_obs_all\": int(len(df_all)),\n",
    "        \"n_forecasts\": int(len(df_oos)),\n",
    "        \"cv_mae_history\": cv_mae_history,\n",
    "        \"best_params_history\": best_params_hist\n",
    "    },\n",
    "    \"train_fit_dates\": pd.to_datetime(pd.Index(train_ends)),\n",
    "    \"models\":   models,\n",
    "    \"preprocs\": preprocs,\n",
    "}\n",
    "\n",
    "with open(LGBM_NOUSREC_LAGS_BUNDLE, \"wb\") as f:\n",
    "    pickle.dump(bundle, f)\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"model\": f\"LightGBM+Bagging ({NAME_TAG})\",\n",
    "    \"horizon\": h,\n",
    "    \"min_train_n\": min_train_n,\n",
    "    \"winsor_level\": winsor_level,\n",
    "    \"norm_var\": norm_var,\n",
    "    \"refit_every_months\": int(refit_every_months),\n",
    "    \"retune_every_months\": int(retune_every_months),\n",
    "    \"use_bagging\": bool(use_bagging),\n",
    "    \"B_boot\": int(B_boot),\n",
    "    \"L_block\": int(L_block),\n",
    "    \"trained_until\": bundle[\"meta\"][\"trained_until\"],\n",
    "    \"n_forecasts\": bundle[\"meta\"][\"n_forecasts\"]\n",
    "}]).to_csv(LGBM_NOUSREC_LAGS_META, index=False)\n",
    "\n",
    "lgbm_artifact = {\n",
    "    \"trained_until\": bundle[\"meta\"][\"trained_until\"],\n",
    "    \"horizon\": h,\n",
    "    \"features\": features,\n",
    "    \"n_models_base\": len(models),\n",
    "    \"best_params_last\": best_params.copy(),\n",
    "}\n",
    "\n",
    "with open(LGBM_NOUSREC_LAGS_LAST_PKL, \"wb\") as f:\n",
    "    pickle.dump(lgbm_artifact, f)\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"trained_until\": lgbm_artifact[\"trained_until\"],\n",
    "    \"n_features\": len(features),\n",
    "    \"horizon\": h,\n",
    "    \"n_models_base\": lgbm_artifact[\"n_models_base\"],\n",
    "    \"model\": f\"LightGBM+Bagging ({NAME_TAG})\"\n",
    "}]).to_csv(LGBM_NOUSREC_LAGS_LAST_META, index=False)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Bundle OOS sauvegardÃ© â†’ {LGBM_NOUSREC_LAGS_BUNDLE}\")\n",
    "print(f\"ðŸ’¾ MÃ©ta bundle       â†’ {LGBM_NOUSREC_LAGS_META}\")\n",
    "print(f\"ðŸ’¾ Dernier modÃ¨le    â†’ {LGBM_NOUSREC_LAGS_LAST_PKL}\")\n",
    "print(f\"ðŸ’¾ MÃ©ta dernier fit  â†’ {LGBM_NOUSREC_LAGS_LAST_META}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
