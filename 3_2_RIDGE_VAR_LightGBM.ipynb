{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c37aae7",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b8aea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d8d201",
   "metadata": {},
   "source": [
    "# Importation des donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78c5c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stationary = pd.read_csv(\"df_stationary.csv\", index_col=\"date\")\n",
    "df_stationary_test = pd.read_csv(\"df_stationary_test.csv\", index_col=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c150f38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['UNRATE', 'TB3MS', 'RPI', 'INDPRO', 'DPCERA3M086SBEA', 'S&P 500',\n",
       "       'BUSLOANS', 'CPIAUCSL', 'OILPRICEx', 'M2SL', 'USREC'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stationary.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91cc0a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['UNRATE', 'TB3MS', 'RPI', 'INDPRO', 'DPCERA3M086SBEA', 'S&P 500',\n",
       "       'BUSLOANS', 'CPIAUCSL', 'OILPRICEx', 'M2SL', 'USREC'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stationary_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944583e",
   "metadata": {},
   "source": [
    "# RIDGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37bbdc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DonnÃ©es prÃªtes : 1960-01-01 â†’ 2025-08-01 | n=788 | freq=MS\n",
      "Features (10): ['TB3MS', 'RPI', 'INDPRO', 'DPCERA3M086SBEA', 'S&P 500', 'BUSLOANS'] ...\n",
      "\n",
      "âœ… Pseudo-OOS Ridge terminÃ© â€” n prÃ©visions = 741\n",
      "              y_pred  y_true  y_pred_base  y_pred_p05  y_pred_p95\n",
      "date                                                             \n",
      "1963-12-01  0.093785     0.0    -0.181263   -0.455216    1.220200\n",
      "1964-01-01  0.348080    -0.1    -0.071449   -0.302035    1.165286\n",
      "1964-02-01  0.390935    -0.5     0.058525   -0.508924    1.117637\n",
      "\n",
      "ðŸ“Š Validation 83â€“89 | n=84 | MAE=0.819 | RMSE=1.025 | RÂ²=-0.346\n",
      "ðŸ“Š Test 90â€“2025     | n=428 | MAE=0.821 | RMSE=1.464 | RÂ²=0.080\n",
      "\n",
      "ðŸ’¾ Bundle OOS sauvegardÃ© â†’ ridge_regression.pkl\n",
      "ðŸ’¾ MÃ©ta bundle       â†’ ridge_regression_meta.csv\n",
      "ðŸ’¾ Dernier modÃ¨le    â†’ ridge_last_model.pkl\n",
      "ðŸ’¾ MÃ©ta dernier fit  â†’ ridge_last_model_meta.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# ðŸ”¹ Ridge Regression + Bagging (pseudo-OOS)\n",
    "#    â†’ mÃªme structure que LinearRegression\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# ---------- ParamÃ¨tres gÃ©nÃ©raux ----------\n",
    "h = 12\n",
    "min_train_n = 36\n",
    "winsor_level = 0.01\n",
    "norm_var = True\n",
    "target_col = \"UNRATE\"\n",
    "\n",
    "# FenÃªtres dâ€™Ã©valuation / test\n",
    "eval_start = pd.Timestamp(\"1983-01-01\")\n",
    "eval_end   = pd.Timestamp(\"1989-12-31\")\n",
    "test_start = pd.Timestamp(\"1990-01-01\")\n",
    "test_end   = pd.Timestamp(\"2025-12-31\")\n",
    "\n",
    "# Bagging (moving-block bootstrap)\n",
    "use_bagging = True\n",
    "B_boot = 30\n",
    "L_block = 12\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "# Grille de recherche pour alpha\n",
    "alpha_grid = np.logspace(-4, 4, 30)\n",
    "\n",
    "# ---------- Fichiers ----------\n",
    "RIDGE_PKL  = \"ridge_regression.pkl\"\n",
    "RIDGE_META = \"ridge_regression_meta.csv\"\n",
    "\n",
    "# ---------- PrÃ©paration des donnÃ©es ----------\n",
    "def _ensure_ms_index(df):\n",
    "    if \"date\" in df.columns:\n",
    "        df = df.set_index(\"date\")\n",
    "    idx = pd.to_datetime(df.index)\n",
    "    df.index = idx.to_period(\"M\").to_timestamp(how=\"start\")\n",
    "    return df.asfreq(\"MS\")\n",
    "\n",
    "df_all = _ensure_ms_index(df_stationary).sort_index()\n",
    "if target_col not in df_all.columns:\n",
    "    raise ValueError(f\"La colonne cible '{target_col}' est absente de df_stationary.\")\n",
    "\n",
    "y_all = df_all[target_col].astype(float)\n",
    "X_all = df_all.drop(columns=[target_col]).astype(float)\n",
    "features = list(X_all.columns)\n",
    "\n",
    "print(f\"âœ… DonnÃ©es prÃªtes : {df_all.index.min().date()} â†’ {df_all.index.max().date()} | n={len(df_all)} | freq=MS\")\n",
    "print(f\"Features ({len(features)}): {features[:6]}{' ...' if len(features)>6 else ''}\")\n",
    "\n",
    "# ---------- PrÃ©proc ----------\n",
    "def fit_preproc(X, wins=0.01, do_norm=True):\n",
    "    lower = X.quantile(wins)\n",
    "    upper = X.quantile(1 - wins)\n",
    "    Xw = X.clip(lower=lower, upper=upper, axis=1)\n",
    "    if do_norm:\n",
    "        mean = Xw.mean()\n",
    "        std = Xw.std().replace(0, 1)\n",
    "        Xn = (Xw - mean) / std\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": mean, \"std\": std, \"norm\": True}\n",
    "        return Xn, prep\n",
    "    else:\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": None, \"std\": None, \"norm\": False}\n",
    "        return Xw, prep\n",
    "\n",
    "def apply_preproc(X, prep):\n",
    "    Xp = X.clip(lower=prep[\"lower\"], upper=prep[\"upper\"], axis=1)\n",
    "    if prep[\"norm\"]:\n",
    "        Xp = (Xp - prep[\"mean\"]) / prep[\"std\"].replace(0, 1)\n",
    "    return Xp\n",
    "\n",
    "# ---------- Bootstrap utils ----------\n",
    "def block_bootstrap_rows(index, L, rng):\n",
    "    n = len(index)\n",
    "    if n < 3:\n",
    "        return np.arange(n)\n",
    "    L = max(2, min(int(L), n - 1))\n",
    "    nb = int(np.ceil(n / L))\n",
    "    starts = rng.integers(0, n - L + 1, size=nb)\n",
    "    ix = np.concatenate([np.arange(s, s + L) for s in starts])[:n]\n",
    "    return ix\n",
    "\n",
    "def bagged_predict_ridge(X_tr_raw, y_tr, x_fore_raw, prep, alpha, B, L, rng):\n",
    "    X_tr_p = apply_preproc(X_tr_raw, prep)\n",
    "    base = Ridge(alpha=alpha, fit_intercept=True)\n",
    "    base.fit(X_tr_p, y_tr.values)\n",
    "    yhat_base = float(base.predict(apply_preproc(x_fore_raw, prep))[0])\n",
    "\n",
    "    preds = []\n",
    "    for _ in range(B):\n",
    "        ix = block_bootstrap_rows(X_tr_raw.index, L, rng)\n",
    "        Xb = X_tr_raw.iloc[ix]\n",
    "        yb = y_tr.iloc[ix]\n",
    "        Xb_p = apply_preproc(Xb, prep)\n",
    "        m = Ridge(alpha=alpha, fit_intercept=True)\n",
    "        m.fit(Xb_p, yb.values)\n",
    "        preds.append(float(m.predict(apply_preproc(x_fore_raw, prep))[0]))\n",
    "    return float(np.mean(preds)), np.array(preds), yhat_base\n",
    "\n",
    "# ---------- Tuning ----------\n",
    "def tune_ridge(X, y):\n",
    "    \"\"\"Recherche du meilleur alpha via GridSearchCV (MAE).\"\"\"\n",
    "    model = Ridge(fit_intercept=True)\n",
    "    grid = GridSearchCV(model, {\"alpha\": alpha_grid},\n",
    "                        scoring=\"neg_mean_absolute_error\",\n",
    "                        cv=5, n_jobs=-1)\n",
    "    grid.fit(X, y)\n",
    "    return grid.best_estimator_.alpha, -grid.best_score_\n",
    "\n",
    "# ---------- Boucle pseudo-OOS ----------\n",
    "rows = []\n",
    "models = []\n",
    "preprocs = []\n",
    "train_ends = []\n",
    "cv_mae_history = []\n",
    "alpha_history = []\n",
    "last_fit_end = None\n",
    "last_t_end = y_all.index.max() - relativedelta(months=h)\n",
    "\n",
    "for t_end in y_all.index:\n",
    "    if t_end > last_t_end:\n",
    "        break\n",
    "    y_tr = y_all.loc[:t_end]\n",
    "    X_tr = X_all.loc[:t_end]\n",
    "    if len(y_tr) < min_train_n:\n",
    "        continue\n",
    "\n",
    "    # PrÃ©proc appris sur TRAIN courant\n",
    "    X_tr_p, prep = fit_preproc(X_tr, wins=winsor_level, do_norm=norm_var)\n",
    "\n",
    "    # Tuning alpha sur TRAIN\n",
    "    alpha, cv_mae = tune_ridge(X_tr_p, y_tr)\n",
    "    cv_mae_history.append(cv_mae)\n",
    "    alpha_history.append(alpha)\n",
    "\n",
    "    # Fit final sur TRAIN\n",
    "    model = Ridge(alpha=alpha, fit_intercept=True)\n",
    "    model.fit(X_tr_p, y_tr.values)\n",
    "    models.append(model)\n",
    "    preprocs.append(prep)\n",
    "    train_ends.append(t_end)\n",
    "    last_fit_end = t_end\n",
    "\n",
    "    # PrÃ©vision h=12\n",
    "    t_fore = t_end + relativedelta(months=h)\n",
    "    if t_fore in y_all.index:\n",
    "        x_fore_raw = X_all.loc[[t_fore]]\n",
    "        if use_bagging:\n",
    "            yhat, dist, yhat_base = bagged_predict_ridge(\n",
    "                X_tr_raw=X_tr, y_tr=y_tr, x_fore_raw=x_fore_raw,\n",
    "                prep=prep, alpha=alpha, B=B_boot, L=L_block, rng=rng\n",
    "            )\n",
    "            y_p05 = float(np.percentile(dist, 5))\n",
    "            y_p95 = float(np.percentile(dist, 95))\n",
    "        else:\n",
    "            yhat = float(model.predict(apply_preproc(x_fore_raw, prep))[0])\n",
    "            yhat_base = yhat\n",
    "            y_p05, y_p95 = (np.nan, np.nan)\n",
    "        rows.append((t_fore, yhat, float(y_all.loc[t_fore]), yhat_base, y_p05, y_p95))\n",
    "\n",
    "# ---------- DataFrame OOS ----------\n",
    "if rows:\n",
    "    df_oos = (\n",
    "        pd.DataFrame(rows, columns=[\"date\",\"y_pred\",\"y_true\",\"y_pred_base\",\"y_pred_p05\",\"y_pred_p95\"])\n",
    "        .assign(date=lambda d: pd.to_datetime(d[\"date\"]).dt.to_period(\"M\").dt.to_timestamp(how=\"start\"))\n",
    "        .set_index(\"date\").sort_index()\n",
    "    )\n",
    "else:\n",
    "    df_oos = pd.DataFrame(columns=[\"y_pred\",\"y_true\",\"y_pred_base\",\"y_pred_p05\",\"y_pred_p95\"])\n",
    "    df_oos.index = pd.to_datetime(pd.Index([]))\n",
    "\n",
    "print(f\"\\nâœ… Pseudo-OOS Ridge terminÃ© â€” n prÃ©visions = {len(df_oos)}\")\n",
    "print(df_oos.head(3))\n",
    "\n",
    "# ---------- Scores ----------\n",
    "def _scores(df):\n",
    "    if len(df) == 0:\n",
    "        return {\"MAE\": np.nan, \"RMSE\": np.nan, \"R2\": np.nan}\n",
    "    mae = mean_absolute_error(df[\"y_true\"], df[\"y_pred\"])\n",
    "    rmse = np.sqrt(mean_squared_error(df[\"y_true\"], df[\"y_pred\"]))\n",
    "    r2 = r2_score(df[\"y_true\"], df[\"y_pred\"]) if len(df) > 1 else np.nan\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
    "\n",
    "df_val  = df_oos.loc[eval_start:eval_end].copy()\n",
    "df_test = df_oos.loc[test_start:test_end].copy()\n",
    "sc_val  = _scores(df_val)\n",
    "sc_test = _scores(df_test)\n",
    "\n",
    "print(f\"\\nðŸ“Š Validation 83â€“89 | n={len(df_val)} | MAE={sc_val['MAE']:.3f} | RMSE={sc_val['RMSE']:.3f} | RÂ²={sc_val['R2']:.3f}\")\n",
    "print(f\"ðŸ“Š Test 90â€“2025     | n={len(df_test)} | MAE={sc_test['MAE']:.3f} | RMSE={sc_test['RMSE']:.3f} | RÂ²={sc_test['R2']:.3f}\")\n",
    "\n",
    "# ---------- Sauvegardes (Ridge) ----------\n",
    "bundle = {\n",
    "    \"oos_predictions\": df_oos.reset_index(),\n",
    "    \"params\": {\n",
    "        \"model\": \"Ridge + Bagging\",\n",
    "        \"horizon\": h,\n",
    "        \"min_train_n\": min_train_n,\n",
    "        \"winsor_level\": winsor_level,\n",
    "        \"norm_var\": norm_var,\n",
    "        \"features\": features,\n",
    "        \"eval_window\": (str(eval_start.date()), str(eval_end.date())),\n",
    "        \"test_window\": (str(test_start.date()), str(test_end.date())),\n",
    "        # plan refit/retune\n",
    "        \"refit_every_months\": refit_every_months,\n",
    "        \"retune_every_months\": retune_every_months,\n",
    "        # search (spÃ©cifique Ridge)\n",
    "        \"hyper_search\": \"GridSearchCV (alpha grid) + hv-block CV (5 folds, gap=12), scoring=MAE\",\n",
    "        \"alpha_grid\": list(alpha_grid) if \"alpha_grid\" in globals() else None,\n",
    "        \"best_alpha_last\": (alpha_history[-1] if len(alpha_history) else None),\n",
    "        # bagging\n",
    "        \"use_bagging\": bool(use_bagging),\n",
    "        \"n_boot\": n_boot,\n",
    "        \"bootstrap_proportion\": bootstrap_proportion,\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"trained_until\": str(last_fit_end.date()) if last_fit_end is not None else None,\n",
    "        \"index_freq\": \"MS\",\n",
    "        \"n_obs_all\": int(len(df_all)),\n",
    "        \"n_forecasts\": int(len(df_oos)),\n",
    "        \"cv_mae_history\": cv_mae_history,     # MAE CV Ã  chaque retune\n",
    "        \"alpha_history\": alpha_history,       # alpha utilisÃ© Ã  chaque refit\n",
    "    },\n",
    "    \"train_fit_dates\": pd.to_datetime(pd.Index(train_ends)) if len(train_ends) else pd.to_datetime(pd.Index([])),\n",
    "\n",
    "    # âœ… Pour permutation_importance_pseudo_oos & SHAP\n",
    "    \"models\":   models,     # liste des modÃ¨les Ridge \"base\" (un par refit)\n",
    "    \"preprocs\": preprocs,   # liste des prÃ©proc (dict) alignÃ©s\n",
    "}\n",
    "\n",
    "with open(RIDGE_BUNDLE, \"wb\") as f:\n",
    "    pickle.dump(bundle, f)\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"model\": \"Ridge+Bagging\",\n",
    "    \"horizon\": h,\n",
    "    \"min_train_n\": min_train_n,\n",
    "    \"winsor_level\": winsor_level,\n",
    "    \"norm_var\": norm_var,\n",
    "    \"refit_every_months\": refit_every_months,\n",
    "    \"retune_every_months\": retune_every_months,\n",
    "    \"use_bagging\": bool(use_bagging),\n",
    "    \"n_boot\": n_boot,\n",
    "    \"bootstrap_proportion\": bootstrap_proportion,\n",
    "    \"trained_until\": bundle[\"meta\"][\"trained_until\"],\n",
    "    \"n_forecasts\": bundle[\"meta\"][\"n_forecasts\"],\n",
    "    \"best_alpha_last\": bundle[\"params\"][\"best_alpha_last\"]\n",
    "}]).to_csv(RIDGE_META, index=False)\n",
    "\n",
    "# Artefact dernier ensemble (optionnel : pour audit)\n",
    "ridge_artifact = {\n",
    "    \"trained_until\": bundle[\"meta\"][\"trained_until\"],\n",
    "    \"horizon\": h,\n",
    "    \"features\": features,\n",
    "    \"n_models_base\": len(models),\n",
    "    \"best_alpha_last\": bundle[\"params\"][\"best_alpha_last\"],\n",
    "}\n",
    "with open(RIDGE_LAST_PKL, \"wb\") as f:\n",
    "    pickle.dump(ridge_artifact, f)\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"trained_until\": ridge_artifact[\"trained_until\"],\n",
    "    \"n_features\": len(features),\n",
    "    \"horizon\": h,\n",
    "    \"n_models_base\": ridge_artifact[\"n_models_base\"],\n",
    "    \"best_alpha_last\": ridge_artifact[\"best_alpha_last\"],\n",
    "}]).to_csv(RIDGE_LAST_META, index=False)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Bundle OOS sauvegardÃ© â†’ {RIDGE_BUNDLE}\")\n",
    "print(f\"ðŸ’¾ MÃ©ta bundle       â†’ {RIDGE_META}\")\n",
    "print(f\"ðŸ’¾ Dernier modÃ¨le    â†’ {RIDGE_LAST_PKL}\")\n",
    "print(f\"ðŸ’¾ MÃ©ta dernier fit  â†’ {RIDGE_LAST_META}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a27eb",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdb0d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# LightGBM + Bagging (pseudo-OOS, h=12) â€” structure \"comme Ridge\"\n",
    "# - Refit annuel, retune hyperparams tous les 36 mois (dÃ¨s 1983)\n",
    "# - hv-block CV (5 folds, gap=12), scoring=MAE\n",
    "# - Winsorisation 1%/99% + normalisation (apprises sur TRAIN)\n",
    "# - Bagging : n_boot bootstrap (proportions/indices), moyenne des prÃ©dictions\n",
    "# - Bundle complet : oos_predictions + models + preprocs + train_fit_dates\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# ---------- ParamÃ¨tres gÃ©nÃ©raux ----------\n",
    "h = 12\n",
    "min_train_n = 36\n",
    "winsor_level = 0.01\n",
    "norm_var = True\n",
    "target_col = \"UNRATE\"\n",
    "\n",
    "# FenÃªtres dâ€™Ã©val / test\n",
    "eval_start = pd.Timestamp(\"1983-01-01\")\n",
    "eval_end   = pd.Timestamp(\"1989-12-31\")\n",
    "test_start = pd.Timestamp(\"1990-01-01\")\n",
    "test_end   = pd.Timestamp(\"2025-12-31\")\n",
    "\n",
    "# ---------- Refit/Retune ----------\n",
    "refit_every_months  = 12     # refit chaque 12 mois\n",
    "retune_every_months = 36     # retune hyperparams chaque 36 mois\n",
    "\n",
    "# ---------- Bagging ----------\n",
    "use_bagging = True\n",
    "n_boot = 30\n",
    "bootstrap_proportion = 1.0\n",
    "rng = np.random.default_rng(12345)\n",
    "\n",
    "# ---------- Fichiers de sortie ----------\n",
    "LGBM_BUNDLE     = \"lightgbm_regression.pkl\"\n",
    "LGBM_META       = \"lightgbm_regression_meta.csv\"\n",
    "LGBM_LAST_PKL   = \"LGBM_last_trained_model.pkl\"\n",
    "LGBM_LAST_META  = \"LGBM_last_trained_model_meta.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93c56304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DonnÃ©es prÃªtes : 1960-01-01 â†’ 2025-08-01 | n=788 | freq=MS\n",
      "Features (10): ['TB3MS', 'RPI', 'INDPRO', 'DPCERA3M086SBEA', 'S&P_500', 'BUSLOANS'] ...\n"
     ]
    }
   ],
   "source": [
    "# ---------- PrÃ©paration df_stationary ----------\n",
    "def _ensure_ms_index(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Index mensuel (MS). Si 'date' existe, on l'utilise comme index.\"\"\"\n",
    "    df = df.copy()\n",
    "    if \"date\" in df.columns:\n",
    "        df = df.set_index(\"date\")\n",
    "    idx = pd.to_datetime(df.index)\n",
    "    df.index = idx.to_period(\"M\").to_timestamp(how=\"start\")\n",
    "    return df.asfreq(\"MS\")\n",
    "\n",
    "# âš ï¸ On suppose df_stationary dispo\n",
    "df_all = _ensure_ms_index(df_stationary).sort_index()\n",
    "\n",
    "if target_col not in df_all.columns:\n",
    "    raise ValueError(f\"La cible '{target_col}' est absente de df_stationary.\")\n",
    "\n",
    "y_all = df_all[target_col].astype(float)\n",
    "X_all = df_all.drop(columns=[target_col]).astype(float)\n",
    "\n",
    "# LightGBM : Ã©viter espaces dans noms de colonnes\n",
    "X_all.columns = [str(c).replace(\" \", \"_\") for c in X_all.columns]\n",
    "features = list(X_all.columns)\n",
    "\n",
    "print(f\"âœ… DonnÃ©es prÃªtes : {df_all.index.min().date()} â†’ {df_all.index.max().date()} | n={len(df_all)} | freq=MS\")\n",
    "print(f\"Features ({len(features)}): {features[:6]}{' ...' if len(features)>6 else ''}\")\n",
    "\n",
    "# ---------- PrÃ©proc ----------\n",
    "def fit_preproc(X: pd.DataFrame, wins=0.01, do_norm=True):\n",
    "    lower = X.quantile(wins)\n",
    "    upper = X.quantile(1 - wins)\n",
    "    Xw = X.clip(lower=lower, upper=upper, axis=1)\n",
    "    if do_norm:\n",
    "        mean = Xw.mean()\n",
    "        std  = Xw.std().replace(0, 1)\n",
    "        Xn   = (Xw - mean) / std\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": mean, \"std\": std, \"norm\": True}\n",
    "        return Xn, prep\n",
    "    else:\n",
    "        prep = {\"lower\": lower, \"upper\": upper, \"mean\": None, \"std\": None, \"norm\": False}\n",
    "        return Xw, prep\n",
    "\n",
    "def apply_preproc(X: pd.DataFrame, prep: dict):\n",
    "    Xp = X.clip(lower=prep[\"lower\"], upper=prep[\"upper\"], axis=1)\n",
    "    if prep[\"norm\"]:\n",
    "        Xp = (Xp - prep[\"mean\"]) / prep[\"std\"].replace(0, 1)\n",
    "    return Xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8a01c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- hv-block CV ----------\n",
    "class HVBlockCV:\n",
    "    def __init__(self, n_splits=5, gap=12):\n",
    "        self.n_splits = n_splits\n",
    "        self.gap = gap\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n = len(X)\n",
    "        fold_sizes = np.full(self.n_splits, n // self.n_splits, dtype=int)\n",
    "        fold_sizes[: n % self.n_splits] += 1\n",
    "        idx = np.arange(n)\n",
    "        cur = 0\n",
    "        for fs in fold_sizes:\n",
    "            start, stop = cur, cur + fs\n",
    "            test_idx = idx[start:stop]\n",
    "            train_mask = np.ones(n, dtype=bool)\n",
    "            left = max(0, start - self.gap)\n",
    "            right = min(n, stop + self.gap)\n",
    "            train_mask[left:right] = False\n",
    "            train_idx = idx[train_mask]\n",
    "            cur = stop\n",
    "            if len(train_idx) == 0 or len(test_idx) == 0:\n",
    "                continue\n",
    "            yield train_idx, test_idx\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21e175d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Grille LightGBM (stabilitÃ©) ----------\n",
    "param_dist = {\n",
    "    \"subsample\":        [0.05,.1,.2,.3,.4,.5,.6,.7,.8,.9,1.0],\n",
    "    \"colsample_bytree\": [.2,.3,.4,.5,.6,.7,1.0],\n",
    "    \"num_leaves\":       [2,3,4,5,8,10,20,40,70,100],\n",
    "    \"n_estimators\":     [5,10,20,30,40,50,75,100],\n",
    "    \"max_depth\":        [1,2,3,5,8,15,-1],          # -1 = illimitÃ©\n",
    "    \"reg_alpha\":        [0, .1, 1, 2, 7, 10, 50, 100],\n",
    "    \"reg_lambda\":       [0, .1, 1, 10, 20, 50, 100],\n",
    "    \"min_child_samples\":[5,10,15],\n",
    "    \"min_split_gain\":   [0.0, 0.01, 0.05],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca6f7f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lgbm(X_tr_p: pd.DataFrame, y_tr: np.ndarray, seed=12345):\n",
    "    cv = HVBlockCV(n_splits=5, gap=12)\n",
    "    base = LGBMRegressor(\n",
    "        boosting_type=\"gbdt\",\n",
    "        objective=\"regression\",\n",
    "        importance_type=\"gain\",\n",
    "        random_state=seed,\n",
    "        n_jobs=1,    # Ã©viter sur-parallÃ©lisation quand RandomizedSearchCV utilise n_jobs=-1\n",
    "        verbose=-1\n",
    "    )\n",
    "    rs = RandomizedSearchCV(\n",
    "        estimator=base,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=100,\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=seed,\n",
    "        refit=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    rs.fit(X_tr_p, y_tr)\n",
    "    return rs.best_params_, float(-rs.best_score_), rs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f79bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Bootstrap utils ----------\n",
    "def bootstrap_indices(n, proportion=1.0, seed=None):\n",
    "    m = int(round(n * proportion))\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    return rng_local.integers(0, n, size=m, endpoint=False)\n",
    "\n",
    "def bagged_predict_lgbm(X_tr_raw, y_tr, x_fore_raw, prep, best_params, B, proportion, seed0):\n",
    "    \"\"\"\n",
    "    Bagging LightGBM :\n",
    "      - prÃ©proc fixe (appris sur TRAIN original)\n",
    "      - bootstrap simple d'indices (proportion)\n",
    "      - fit LGBM(**best_params) et prÃ©diction\n",
    "      - retourne la moyenne + distribution + prÃ©diction base (fit sur tout TRAIN)\n",
    "    \"\"\"\n",
    "    # Base (rÃ©fÃ©rence) : fit sur tout TRAIN prÃ©traitÃ©\n",
    "    X_tr_p = apply_preproc(X_tr_raw, prep)\n",
    "    base = LGBMRegressor(\n",
    "        boosting_type=\"gbdt\",\n",
    "        objective=\"regression\",\n",
    "        importance_type=\"gain\",\n",
    "        n_jobs=1,\n",
    "        random_state=seed0,\n",
    "        verbose=-1,\n",
    "        **best_params\n",
    "    )\n",
    "    base.fit(X_tr_p, y_tr.values)\n",
    "    yhat_base = float(base.predict(apply_preproc(x_fore_raw, prep))[0])\n",
    "\n",
    "    preds = []\n",
    "    n = len(X_tr_raw)\n",
    "    for b in range(B):\n",
    "        ix = bootstrap_indices(n, proportion=proportion, seed=seed0 + b)\n",
    "        Xb = X_tr_raw.iloc[ix]\n",
    "        yb = y_tr.iloc[ix]\n",
    "        Xb_p, _ = fit_preproc(Xb, wins=winsor_level, do_norm=norm_var)  # prÃ©proc appris sur bootstrap\n",
    "        m = LGBMRegressor(\n",
    "            boosting_type=\"gbdt\",\n",
    "            objective=\"regression\",\n",
    "            importance_type=\"gain\",\n",
    "            n_jobs=1,\n",
    "            random_state=seed0 + b,\n",
    "            verbose=-1,\n",
    "            **best_params\n",
    "        )\n",
    "        m.fit(Xb_p, yb.values)\n",
    "        preds.append(float(m.predict(apply_preproc(x_fore_raw, _))[0]))  # appliquer le prep du bootstrap (_)\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    return float(np.mean(preds)), preds, yhat_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8e31984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Validation 83â€“89 â€” n=84 | MAE=0.788 | RMSE=1.040 | RÂ²=-0.385\n",
      "ðŸ“Š Test 90â€“2025 â€” n=428 | MAE=0.777 | RMSE=1.379 | RÂ²=0.183\n"
     ]
    }
   ],
   "source": [
    "# ---------- Boucle pseudo-OOS ----------\n",
    "rows = []                 # (date, y_pred, y_true, y_pred_base, p05, p95)\n",
    "models = []               # modÃ¨les \"base\" LGBM par refit (pour permutation/SHAP)\n",
    "preprocs = []             # prÃ©procs alignÃ©s\n",
    "train_ends = []           # dates de refit\n",
    "cv_mae_history = []       # historique MAE CV lors des retunes\n",
    "best_params_hist = []     # historique des params\n",
    "\n",
    "last_fit_end = None\n",
    "last_t_end   = y_all.index.max() - relativedelta(months=h)\n",
    "last_refit_t = None\n",
    "last_tune_t  = None\n",
    "\n",
    "best_params = {}          # params courants (remplis au 1er retune)\n",
    "base_model = None         # sÃ©curitÃ© si use_bagging=False\n",
    "boot_seed = 12345         # reseed Ã  chaque refit\n",
    "seed0 = 12345\n",
    "\n",
    "for t_end in y_all.index:\n",
    "    if t_end > last_t_end:\n",
    "        break\n",
    "\n",
    "    y_tr = y_all.loc[:t_end]\n",
    "    X_tr = X_all.loc[:t_end]\n",
    "    if len(y_tr) < min_train_n:\n",
    "        continue\n",
    "\n",
    "    # cadence refit\n",
    "    if last_refit_t is None:\n",
    "        need_refit = True\n",
    "    else:\n",
    "        months_since_refit = (t_end.year - last_refit_t.year)*12 + (t_end.month - last_refit_t.month)\n",
    "        need_refit = months_since_refit >= refit_every_months\n",
    "\n",
    "    # cadence retune\n",
    "    need_tune = False\n",
    "    if t_end >= eval_start:\n",
    "        if last_tune_t is None:\n",
    "            need_tune = True\n",
    "        else:\n",
    "            months_since_tune = (t_end.year - last_tune_t.year)*12 + (t_end.month - last_tune_t.month)\n",
    "            need_tune = months_since_tune >= retune_every_months\n",
    "\n",
    "    # PrÃ©proc global (pour tuning/refit)\n",
    "    X_tr_p_global, prep_global = fit_preproc(X_tr, wins=winsor_level, do_norm=norm_var)\n",
    "\n",
    "    if need_tune:\n",
    "        best_params, best_cv_mae, _ = tune_lgbm(X_tr_p_global, y_tr.values, seed=seed0)\n",
    "        last_tune_t = t_end\n",
    "        cv_mae_history.append(best_cv_mae)\n",
    "        best_params_hist.append(best_params.copy())\n",
    "    else:\n",
    "        cv_mae_history.append(np.nan)\n",
    "        best_params_hist.append(best_params.copy() if best_params else {})\n",
    "\n",
    "    # Valeurs sÃ»res si aucun retune n'a encore eu lieu\n",
    "    if not best_params:\n",
    "        best_params = dict(\n",
    "            subsample=0.7, colsample_bytree=0.7, num_leaves=31,\n",
    "            n_estimators=100, max_depth=-1, reg_alpha=0.0, reg_lambda=0.0,\n",
    "            min_child_samples=10, min_split_gain=0.0\n",
    "        )\n",
    "\n",
    "    if need_refit:\n",
    "        # ModÃ¨le \"base\" stockÃ© pour permutation/SHAP\n",
    "        base_model = LGBMRegressor(\n",
    "            boosting_type=\"gbdt\",\n",
    "            objective=\"regression\",\n",
    "            importance_type=\"gain\",\n",
    "            n_jobs=1,\n",
    "            random_state=seed0,\n",
    "            verbose=-1,\n",
    "            **best_params\n",
    "        )\n",
    "        base_model.fit(X_tr_p_global, y_tr.values)\n",
    "        models.append(base_model)\n",
    "        preprocs.append(prep_global)\n",
    "\n",
    "        train_ends.append(t_end)\n",
    "        last_refit_t = t_end\n",
    "        last_fit_end = t_end\n",
    "\n",
    "        # reseed bagging Ã  chaque refit\n",
    "        boot_seed += 9973\n",
    "\n",
    "    # PrÃ©vision h=12\n",
    "    t_fore = t_end + relativedelta(months=h)\n",
    "    if t_fore in y_all.index:\n",
    "        x_fore_raw = X_all.loc[[t_fore]]\n",
    "\n",
    "        if use_bagging:\n",
    "            yhat, dist, yhat_base = bagged_predict_lgbm(\n",
    "                X_tr_raw=X_tr, y_tr=y_tr, x_fore_raw=x_fore_raw,\n",
    "                prep=prep_global, best_params=best_params,\n",
    "                B=n_boot, proportion=bootstrap_proportion, seed0=boot_seed\n",
    "            )\n",
    "            y_p05 = float(np.percentile(dist, 5))\n",
    "            y_p95 = float(np.percentile(dist, 95))\n",
    "        else:\n",
    "            # sans bagging : utiliser / garantir un base_model\n",
    "            if base_model is None:\n",
    "                base_model = LGBMRegressor(\n",
    "                    boosting_type=\"gbdt\",\n",
    "                    objective=\"regression\",\n",
    "                    importance_type=\"gain\",\n",
    "                    n_jobs=1,\n",
    "                    random_state=seed0,\n",
    "                    verbose=-1,\n",
    "                    **best_params\n",
    "                ).fit(X_tr_p_global, y_tr.values)\n",
    "            yhat = float(base_model.predict(apply_preproc(x_fore_raw, prep_global))[0])\n",
    "            yhat_base = yhat\n",
    "            y_p05, y_p95 = (np.nan, np.nan)\n",
    "\n",
    "        rows.append((t_fore, yhat, float(y_all.loc[t_fore]), yhat_base, y_p05, y_p95))\n",
    "\n",
    "# ---------- DataFrame OOS ----------\n",
    "if rows:\n",
    "    df_oos = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\"date\", \"y_pred\", \"y_true\", \"y_pred_base\", \"y_pred_p05\", \"y_pred_p95\"]\n",
    "    )\n",
    "    # âœ… Conversion date â†’ dÃ©but de mois (MS)\n",
    "    df_oos[\"date\"] = pd.to_datetime(df_oos[\"date\"]).dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "    df_oos = df_oos.set_index(\"date\").sort_index()\n",
    "else:\n",
    "    df_oos = pd.DataFrame(columns=[\"y_pred\", \"y_true\", \"y_pred_base\", \"y_pred_p05\", \"y_pred_p95\"])\n",
    "    df_oos.index = pd.to_datetime(pd.Index([]))\n",
    "\n",
    "# ---------- Scores ----------\n",
    "def _scores(df):\n",
    "    if len(df) == 0:\n",
    "        return {\"MAE\": np.nan, \"RMSE\": np.nan, \"R2\": np.nan}\n",
    "    mae  = mean_absolute_error(df[\"y_true\"], df[\"y_pred\"])\n",
    "    rmse = np.sqrt(mean_squared_error(df[\"y_true\"], df[\"y_pred\"]))\n",
    "    ssr  = np.sum((df[\"y_true\"] - df[\"y_pred\"])**2)\n",
    "    sst  = np.sum((df[\"y_true\"] - df[\"y_true\"].mean())**2)\n",
    "    r2   = 1 - ssr/sst if sst > 0 else np.nan\n",
    "    return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2)}\n",
    "\n",
    "df_val  = df_oos.loc[eval_start:eval_end].copy()\n",
    "df_test = df_oos.loc[test_start:test_end].copy()\n",
    "\n",
    "sc_val  = _scores(df_val)\n",
    "sc_test = _scores(df_test)\n",
    "\n",
    "print(f\"\\nðŸ“Š Validation 83â€“89 â€” n={len(df_val)} | MAE={sc_val['MAE']:.3f} | RMSE={sc_val['RMSE']:.3f} | RÂ²={sc_val['R2']:.3f}\")\n",
    "print(f\"ðŸ“Š Test 90â€“{test_end.year} â€” n={len(df_test)} | MAE={sc_test['MAE']:.3f} | RMSE={sc_test['RMSE']:.3f} | RÂ²={sc_test['R2']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90d47e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Bundle OOS sauvegardÃ© â†’ lightgbm_regression.pkl\n",
      "ðŸ’¾ MÃ©ta bundle       â†’ lightgbm_regression_meta.csv\n",
      "ðŸ’¾ Dernier modÃ¨le    â†’ LGBM_last_trained_model.pkl\n",
      "ðŸ’¾ MÃ©ta dernier fit  â†’ LGBM_last_trained_model_meta.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------- Sauvegardes ----------\n",
    "bundle = {\n",
    "    \"oos_predictions\": df_oos.reset_index(),\n",
    "    \"params\": {\n",
    "        \"model\": \"LightGBM + Bagging\",\n",
    "        \"horizon\": h,\n",
    "        \"min_train_n\": min_train_n,\n",
    "        \"winsor_level\": winsor_level,\n",
    "        \"norm_var\": norm_var,\n",
    "        \"features\": features,\n",
    "        \"eval_window\": (str(eval_start.date()), str(eval_end.date())),\n",
    "        \"test_window\": (str(test_start.date()), str(test_end.date())),\n",
    "        # plan refit/retune\n",
    "        \"refit_every_months\": refit_every_months,\n",
    "        \"retune_every_months\": retune_every_months,\n",
    "        # search\n",
    "        \"hyper_search\": \"RandomizedSearchCV (100 iters) + hv-block CV (5 folds, gap=12), scoring=MAE\",\n",
    "        \"best_params_last\": best_params.copy(),\n",
    "        # bagging\n",
    "        \"use_bagging\": bool(use_bagging),\n",
    "        \"n_boot\": int(n_boot),\n",
    "        \"bootstrap_proportion\": float(bootstrap_proportion),\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"trained_until\": str(last_fit_end.date()) if last_fit_end is not None else None,\n",
    "        \"index_freq\": \"MS\",\n",
    "        \"n_obs_all\": int(len(df_all)),\n",
    "        \"n_forecasts\": int(len(df_oos)),\n",
    "        \"cv_mae_history\": cv_mae_history,\n",
    "        \"best_params_history\": best_params_hist\n",
    "    },\n",
    "    \"train_fit_dates\": pd.to_datetime(pd.Index(train_ends)),\n",
    "\n",
    "    # âœ… Pour permutation_importance_pseudo_oos & SHAP\n",
    "    \"models\":   models,     # liste des modÃ¨les \"base\" LGBM (un par refit)\n",
    "    \"preprocs\": preprocs,   # liste des prÃ©proc (dict) alignÃ©s aux modÃ¨les\n",
    "}\n",
    "\n",
    "with open(LGBM_BUNDLE, \"wb\") as f:\n",
    "    pickle.dump(bundle, f)\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"model\": \"LightGBM+Bagging\",\n",
    "    \"horizon\": h,\n",
    "    \"min_train_n\": min_train_n,\n",
    "    \"winsor_level\": winsor_level,\n",
    "    \"norm_var\": norm_var,\n",
    "    \"refit_every_months\": refit_every_months,\n",
    "    \"retune_every_months\": retune_every_months,\n",
    "    \"use_bagging\": bool(use_bagging),\n",
    "    \"n_boot\": int(n_boot),\n",
    "    \"bootstrap_proportion\": float(bootstrap_proportion),\n",
    "    \"trained_until\": bundle[\"meta\"][\"trained_until\"],\n",
    "    \"n_forecasts\": bundle[\"meta\"][\"n_forecasts\"]\n",
    "}]).to_csv(LGBM_META, index=False)\n",
    "\n",
    "# Artefact dernier ensemble (optionnel : pour audit)\n",
    "lgbm_artifact = {\n",
    "    \"trained_until\": bundle[\"meta\"][\"trained_until\"],\n",
    "    \"horizon\": h,\n",
    "    \"features\": features,\n",
    "    \"n_models_base\": len(models),\n",
    "    \"best_params_last\": best_params.copy(),\n",
    "}\n",
    "with open(LGBM_LAST_PKL, \"wb\") as f:\n",
    "    pickle.dump(lgbm_artifact, f)\n",
    "pd.DataFrame([{\n",
    "    \"trained_until\": lgbm_artifact[\"trained_until\"],\n",
    "    \"n_features\": len(features),\n",
    "    \"horizon\": h,\n",
    "    \"n_models_base\": lgbm_artifact[\"n_models_base\"]\n",
    "}]).to_csv(LGBM_LAST_META, index=False)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Bundle OOS sauvegardÃ© â†’ {LGBM_BUNDLE}\")\n",
    "print(f\"ðŸ’¾ MÃ©ta bundle       â†’ {LGBM_META}\")\n",
    "print(f\"ðŸ’¾ Dernier modÃ¨le    â†’ {LGBM_LAST_PKL}\")\n",
    "print(f\"ðŸ’¾ MÃ©ta dernier fit  â†’ {LGBM_LAST_META}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
